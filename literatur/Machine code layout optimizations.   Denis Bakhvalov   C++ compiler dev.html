<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
	<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113154355-1');
</script>

<!--
  <script>
		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		  ga('create', 'UA-38506466-1', 'jmcglone.com');
		  ga('send', 'pageview');

  </script>
-->
	
        <link type="application/atom+xml" rel="alternate" href="https://dendibakh.github.io/feed.xml" title="Denis Bakhvalov">
	<meta charset="utf-8">
	<meta name="viewport" width="device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author" content="Denis Bakhvalov">
	<!-- favicon
	<link rel="shortcut icon" href="/img/ico/favicon.png">
	-->
	<title>Machine code layout optimizations. | Denis Bakhvalov | C++ compiler dev.</title>		
	<!-- styles -->
	<link href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/bootstrap.css" rel="stylesheet">
	<link href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/jumbotron-narrow.css" rel="stylesheet">
	<link href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/style.css" rel="stylesheet">
	
	<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!--[if lt IE 9]>
  <script src="../../assets/js/html5shiv.js"></script>
  <script src="../../assets/js/respond.min.js"></script>
	<![endif]-->
	<!-- google analytics - i will not share this data with google -->
<script type="text/javascript" async="" src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/embed.js"></script><link rel="prefetch" as="style" href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/a_data/lounge.css"><link rel="prefetch" as="script" href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/a_data/common.js"><link rel="prefetch" as="script" href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/a_data/lounge.js"><link rel="prefetch" as="script" href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/a_data/config.js"><script src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/alfie.js" async="" charset="UTF-8"></script></head>

	
	<body>
		<div class="container">
			
			<div class="header">
	<ul class="nav nav-pills pull-right">
          <!--<li class=""><a href="/cv" title="CV">CV</a></li> -->
		<!-- <li class=""><a href="/projects" title="Projects">Projects</a></li> -->
		<li class=""><script src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/lunr.js"></script>

<script>

var documents = [{
    "id": 0,
    "url": "https://dendibakh.github.io/404",
    "title": "404 Error | Sorry, this page does not exist.",
    "body": "    	Whoah, that's a 404 Error    	Hey, sorry, but the page you're trying to access doesn't exist. Try going back to the homepage, or use the navigation menu above to find what you're looking for. "
    }, {
    "id": 1,
    "url": "https://dendibakh.github.io/about_me/",
    "title": "About me",
    "body": "     My bio:   I'm a C++ developer with almost 10 years of experience. I started my journey as a developer of desktop applications, then I moved to embedded and now I work at Intel, doing C++ compiler development. I enjoy writing the fastest-possible code and staring at the assembly.   I am a father of 2, like to play soccer and chess.      List of my talks:      March 2018: embo++ conference in Bochum, Germany. Dealing with performance analysis in C and C++.    March 2016: Code::Dive conference in Wroclaw, Poland. Diving into linker.      Other:      I was recently on CppCast. I was talking about Performance Analysis and Optimizations. Recording is here.    "
    }, {
    "id": 2,
    "url": "https://dendibakh.github.io/contact/",
    "title": "Contact",
    "body": "	You can get in touch with or find me at: 			@LinkedIn 	@GitHub    Twitter: @dendibakh	Or by sending an e-mail to moc. liamg@hkabidned	"
    }, {
    "id": 3,
    "url": "https://dendibakh.github.io/swatches/",
    "title": "Swatches",
    "body": " Swatches Swatches is a color log featuring sets of hex color values found in influential design across the web. In other cases, they are sets I've put together myself. Attribution is given where appropriate. See this note for more info about the project.    21 May 2014:                   #F0552B        #764E2C        #3E2A24        #4C321F        #CDC294        #BB906E        #6F6253        #976C51              via     22 April 2014:                   #FFFFFF        #9D9D9D        #D5D5D5        #128CF9        #181818        #F82809        #72706F        #0E0E0E           17 April 2014:                   #F9F9F9        #163D60        #DABA86        #E11936        #BCBBB0        #39566D        #8D8585        #E05256        via     26 March 2014:                   #F0F4F5        #ACB0B4        #1B2F43        #23956F        #39688F        #1E5667        #609B8A        #588E73        via     25 March 2014:                   #6E336D        #754F72        #F2662C        #FCD612        #C6B632        #A84C4A        #7BA9A0        #718196        via     17 March 2014:                   #0D7043        #0B6D3B        #239E3A        #D0E80F        #3FAF3C        #0D3C37        #D8E8E0        #70AC8E        via       20 Feb 2014:                   #E9E8E5        #555A58        #32333C        #3A4548        #383841        #C0B6A9        #543A37        #5582A4        via       17 Feb 2014:                   #FFFFFF        #F4F3F2        #D4D4D4        #E8E7E6        #E04D32        #666666        #55BBC2        #323232        via                      #EBE4DA        #302219        #D7CBB9        #463930        #6C6157        #AEA49A        #4F443B        #897E75            12 Feb 2014:                   #FEFEE8        #024467        #327288        #B2CBC3        #61959F        #90B5B5        #024668        #034A69              via        28 Jan 2014:                   #D94B41        #F8EBCA        #629EBA        #443238        #B3B1AE        #B03F39        #34272C        #637B90              via        23 Jan 2014:                   #8F9BBB        #E6E8F2        #5C5C73        #707287        #ACB8CB        #3C4154        #817E81        #3BC6E9              via        21 Jan 2014:                   #92C2B5        #8EB299        #ADD4C5        #7E8463        #6B5A35        #625028        #4B453B        #1A110F              via         16 Jan 2014:                   #E8E7D5        #012942        #CABD4D        #D2C76C        #9D9C4A        #34504E        #C7C8A7        #748783              via         10 Jan 2014:                   #A6D5E1        #D3E2E4        #4E4C4C        #5EB7CB        #0196AD        #A5A59F        #E25D3A        #9C8160              via       07 Jan 2014:                   #FEFEFD        #CF967A        #3D3334        #3F4550        #B9B0AF        #957162        #605556        #B58570                     20 Dec 2013:                   #EDF0F7        #A2C2D4        #F8B300        #C44B33        #0CA6E2        #3A3B3D        #277491        #DAB124              via        19 Dec 2013:                   #E8EAEC        #302F32        #A9A8A9        #C8C5B5        #463C38        #5D5B5E        #AF4E41        #8EA564              via                      #F7D0B0        #3D495F        #EEEBEA        #A41A1C        #69050A        #4D4C4C        #C16F6D        #BE8D7B              via        17 Dec 2013:                   #D54133        #D73F33        #E3E2B7        #D86650        #D89071        #DFB390        #E4E6C1              via        16 Dec 2013:                   #F7F2C6        #F8EDB9        #6B0E49        #B33259        #A9B899        #5BB090        #59596A        #9B8C7B              via                      #D95253        #EFE0C8        #3F4550        #CD9579        #985557        #61545B        #ADA8A1        #7B7F87              via      15 Dec 2013:                   #F7F7F7        #EAEAEA        #D9D9D7        #FFFF00        #1C1C1C        #2B2B2B        #595049        #1C1B1A              via       12 Dec 2013:                   #F3DDBE        #0782B0        #EFAA44        #F4CF54        #004469        #64B78C        #058074        #258FAC              via       11 Dec 2013:                   #F6F6F6        #454A66        #5E8FB1        #75A5C3        #D0664F        #BCBCB8        #395474        #79868E              via       10 Dec 2013:                   #FEFEFE        #34474B        #2B3D42        #25363A        #61949E        #A1AAAD        #51666A        #537C83              via      5 Dec 2013:                   #EDF2F1        #343944        #639A91        #8EC2B4        #434852        #927868        #56D5C6        #353B46              via        28 Nov 2013:                   #FFFFFF        #B4BCC2        #5A5A5A        #639A91        #18BC9C        #2C3E50        #202D3B        #1A242F                "
    }, {
    "id": 4,
    "url": "https://dendibakh.github.io/guides/github-pages/",
    "title": "A Guide to Creating and Hosting a Personal Website on GitHub",
    "body": "  Creating and Hosting a Personal Site on GitHub A step-by-step beginner's guide to creating a personal website and blog using Jekyll and hosting it for free using GitHub Pages.   View Demo Site&nbsp;&nbsp;&nbsp;Download Demo Files 	 This guide is meant to help Git and GitHub beginners get up and running with GitHub Pages and Jekyll in an afternoon. It assumes you know very little about version control, Git, and GitHub. It is helpful if you know the basics of HTML and CSS since we'll be working directly with these languages. We'll also be using a little bit of Markdown, but by no means do you need to be an expert with any of these languages. The idea is to learn by doing, so the code we'll be implementing in this tutorial is available in this guide or can be downloaded entirely at this GitHub repo. Feel free to copy and paste or type this code directly into your project's files. For a little background on why I chose GitHub and GitHub Pages for my personal website (and other projects), see this note. Other Resources You Should Know:  In order to make GitHub Pages accessible to a wider audience, this guide focuses on using the web interface on github. com to build your personal website, thereby generalizing and glossing over the standard tools associated with Git and GitHub. To get a lot dirtier with Git and GitHub (ie, the command line and terminal), there are several other great guides you should also know about, probably bookmark, and read after completing this one, or jump over to if that is more your speed: Anna Debenham, Thinkful, and even GitHub itself go above and beyond making the command line or local workflow of GitHub hosting and Jekyll templates accessible to a wider audience. Also, at the end of this document, there is a pretty good list of resources related to Git, GitHub/Pages, Jekyll, and Markdown that can help you dive deeper into these tools. I'll do my best to keep this list updated as I find new ones.  What is Git, GitHub, and GitHub Pages?: Git, GitHub, and GitHub Pages are all very closely related. Imagine Git as the workflow to get things done and GitHub and GitHub Pages as places to store the work you finish. Projects that use Git are stored publicly in GitHub and GitHub Pages, so in a very generalized way, Git is what you do locally on your own computer and GitHub is the place where all this gets stored publicly on a server. Git: Git is a version control system that tracks changes to files in a project over time. It typically records what the changes were (what was added? what was removed from the file?), who made the changes, notes and comments about the changes by the changer, and at what time the changes were made. It is primarily used for software development projects which are often collaborative, so in this sense, it is a tool to help enable and improve collaboration. However, its collaborative nature has led it to gain interest in the publishing community as a tool to help in both authoring and editorial workflows. Git is for people who want to maintain multiple versions of their files in an efficient manner and travel back in time to visit different versions without juggling numerous files along with their confusing names stored at different locations. Think of Git and version control like a magic undo button. In the diagram below, each stage represents a “save”. Without Git, you cannot go back to any of the in between stages from the initial draft and final draft. If you wanted to change the opening paragraph in the final draft, you’d have to delete data that you couldn’t recover. To work around this, we use the “save as” option, name it something different, delete the opening paragraph and start writing a new one. With Git, the flow is multidirectional. Each change that is significant is marked as important in a version, and you proceed. If you need to get back to earlier stages, you can without any loss of data. Presently, Google Docs “revision history” or Wikipedia’s “edit history” work in this sort of fashion. Git is just a lot more detailed and can get a lot more complex if needed. 1 When you have the chance, I highly recommend this 15 minute, hands-on web tutorial on using Git. GitHub: GitHub is a web hosting service for the source code of software and web development projects (or other text based projects) that use Git. In many cases, most of the code is publicly available, enabling developers to easily investigate, collaborate, download, use, improve, and remix that code. The container for the code of a specific project is called a repository. There are thousands of really cool and exciting repositories on GitHub, with new ones added every day. Some examples of popular software development projects that make their code available on GitHub include: 	Twitter Bootstrap, an extremely popular front-end framework for mobile first websites, created by developers at Twitter. 	HTML5 Boilerplate, a front-end template for quickly building websites, 	The JavaScript Visualization Library D3Ruby on Rails, the open-source web framework built on Ruby. Usually, people just host the files that contain their code, so what you see on the end view is the actual code, as in this example from the Ruby on Rails project: GitHub Pages: GitHub Pages are public webpages hosted for free through GitHub. GitHub users can create and host both personal websites (one allowed per user) and websites related to specific GitHub projects. Pages lets you do the same things as GitHub, but if the repository is named a certain way and files inside it are HTML or Markdown, you can view the file like any other website. GitHub Pages is the self-aware version of GitHub. Pages also comes with a powerful static site generator called Jekyll, which we'll learn more about later on. Getting Started with GitHub Pages: Don't worry if some of these concepts are still a little fuzzy to you. The best way to learn this stuff is to just start doing the work, so let's not waste anymore time and dive right in. 1Create your project's repository. Login to your GitHub account and go to https://github. com/new or click the New repository icon from your account homepage. 2 Name your repository username. github. io, replacing username with your GitHub username. Be sure it is public and go ahead and tell GitHub to create a README file upon generating the repo. 3 Create an index. html page by clicking the plus icon next to your repository name and typing the file name directly in the input box that appears. On the resulting page, put this markup inside of the GitHub text editor: &lt;!DOCTYPE html&gt;&lt;html&gt;	&lt;head&gt;		&lt;title&gt;Hank Quinlan, Horrible Cop&lt;/title&gt;	&lt;/head&gt;	&lt;body&gt;		&lt;nav&gt;  		&lt;ul&gt;    		&lt;li&gt;&lt;a href= / &gt;Home&lt;/a&gt;&lt;/li&gt;	    	&lt;li&gt;&lt;a href= /about &gt;About&lt;/a&gt;&lt;/li&gt;    		&lt;li&gt;&lt;a href= /cv &gt;CV&lt;/a&gt;&lt;/li&gt;    		&lt;li&gt;&lt;a href= /blog &gt;Blog&lt;/a&gt;&lt;/li&gt;  		&lt;/ul&gt;		&lt;/nav&gt;		&lt;div class= container &gt;  		&lt;div class= blurb &gt;    		&lt;h1&gt;Hi there, I'm Hank Quinlan!&lt;/h1&gt;				&lt;p&gt;I'm best known as the horrible cop from &lt;em&gt;A Touch of Evil&lt;/em&gt; Don't trust me. &lt;a href= /about &gt;Read more about my life. . . &lt;/a&gt;&lt;/p&gt;  		&lt;/div&gt;&lt;!-- /. blurb --&gt;		&lt;/div&gt;&lt;!-- /. container --&gt;		&lt;footer&gt;  		&lt;ul&gt;    		&lt;li&gt;&lt;a href= mailto:hankquinlanhub@gmail. com &gt;email&lt;/a&gt;&lt;/li&gt;    		&lt;li&gt;&lt;a href= https://github. com/hankquinlan &gt;github. com/hankquinlan&lt;/a&gt;&lt;/li&gt;			&lt;/ul&gt;		&lt;/footer&gt;	&lt;/body&gt;&lt;/html&gt;4 Commit index. html. At the bottom of the page, there is a text input area to add a description of your changes and a button to commit the file. Congrats! You just built your first GitHub Pages site. View it at http://username. github. io. Usually the first time your GitHub Pages site is created it takes 5-10 minutes to go live, so while we wait for that to happen, let's style your otherwise plain HTML site. 5 To style the content go back to your repository home and create a new file named css/main. css. The css/ before the filename will automatically create a subdirectory called css. Pretty neat. Place the following inside main. css: body &lbrace;  margin: 60px auto;  width: 70%;&rbrace;nav ul, footer ul &lbrace;  font-family:'Helvetica', 'Arial', 'Sans-Serif';  padding: 0px;  list-style: none;  font-weight: bold;&rbrace;nav ul li, footer ul li &lbrace;  display: inline;  margin-right: 20px;&rbrace;a &lbrace;  text-decoration: none;  color: #999;&rbrace;a:hover &lbrace;  text-decoration: underline;&rbrace;h1 &lbrace;  font-size: 3em;  font-family:'Helvetica', 'Arial', 'Sans-Serif';&rbrace;p &lbrace;  font-size: 1. 5em;  line-height: 1. 4em;  color: #333;&rbrace;footer &lbrace;  border-top: 1px solid #d5d5d5;  font-size: . 8em;&rbrace;ul. posts &lbrace;   margin: 20px auto 40px;   font-size: 1. 5em;&rbrace;ul. posts li &lbrace;  list-style: none;&rbrace;Don't forget to commit the new CSS file! 6 Link to your CSS file inside your HTML document's &lt;head&gt;. Go back to index. html and select the  Edit  button. 		Add a link to main. css (new markup is in bold): &lt;!DOCTYPE html&gt;&lt;html&gt;	&lt;head&gt;		&lt;title&gt;Hank Quinlan, Horrible Cop&lt;/title&gt;		&lt;!-- link to main stylesheet --&gt;		&lt;link rel= stylesheet  type= text/css  href= /css/main. css &gt;	&lt;/head&gt;	&lt;body&gt;		&lt;nav&gt;  		&lt;ul&gt;    		&lt;li&gt;&lt;a href= / &gt;Home&lt;/a&gt;&lt;/li&gt;	    	&lt;li&gt;&lt;a href= /about &gt;About&lt;/a&gt;&lt;/li&gt;    		&lt;li&gt;&lt;a href= /cv &gt;CV&lt;/a&gt;&lt;/li&gt;    		&lt;li&gt;&lt;a href= /blog &gt;Blog&lt;/a&gt;&lt;/li&gt;  		&lt;/ul&gt;		&lt;/nav&gt;		&lt;div class= container &gt;  		&lt;div class= blurb &gt;    		&lt;h1&gt;Hi there, I'm Hank Quinlan!&lt;/h1&gt;				&lt;p&gt;I'm best known as the horrible cop from &lt;em&gt;A Touch of Evil&lt;/em&gt; Don't trust me. &lt;a href= /about &gt;Read more about my life. . . &lt;/a&gt;&lt;/p&gt;  		&lt;/div&gt;&lt;!-- /. blurb --&gt;		&lt;/div&gt;&lt;!-- /. container --&gt;		&lt;footer&gt;  		&lt;ul&gt;    		&lt;li&gt;&lt;a href= mailto:hankquinlanhub@gmail. com &gt;email&lt;/a&gt;&lt;/li&gt;    		&lt;li&gt;&lt;a href= https://github. com/hankquinlan &gt;github. com/hankquinlan&lt;/a&gt;&lt;/li&gt;			&lt;/ul&gt;		&lt;/footer&gt;	&lt;/body&gt;&lt;/html&gt;Visit http://username. github. io to see your styled website. It should look like the page at http://hankquinlan. github. io. Using Jekyll with GitHub Pages: Like GitHub Pages, Jekyll is self-aware, so if you add folders and files following specific naming conventions, when you commit to GitHub, Jekyll will magically build your website. While I recommend setting up Jekyll on your own computer so you can edit and preview your site locally, and when ready, push those changes to your GitHub repo, we're not going to do that. Instead, to quickly get a handle on how Jekyll works, we're going to build it into our GitHub repo using the GitHub web interface. What is Jekyll?: Jekyll is a very powerful static site generator. In some senses, it is a throwback to the days of static HTML before databases were used to store website content. For simple sites without complex architectures, like a personal website, this is a huge plus. When used alongside GitHub, Jekyll will automatically re-generate all the HTML pages for your website each time you commit a file.  Jekyll makes managing your website easier because it depends on templates. Templates (or layouts in Jekyll nomenclature) are your best friend when using a static site generator. Instead of repeating the same navigation markup on every page I create, which I'd have to edit on every page if I add, remove, or change the location of navigation item, I can create what Jekyll calls a layout that gets used on all my pages. In this tutorial, we're going to create two Jekyll templates to help power your website. Setting Up Jekyll on github. com: In order for Jekyll to work with your site, you need to follow Jekyll's directory structure. To learn about this structure, we're going to build it right into our GitHub repo.  7 Create a . gitignore file. This file tells Git to ignore the _site directory that Jekyll automatically generates each time you commit. Because this directory and all the files inside are written each time you commit, you do not want this directory under version control. Add this simple line to the file: 	_site/8 Create a _config. yml file that tells Jekyll some basics about your project. In this example, we're telling Jekyll the name of our site and what version of Markdown we'd like to use: 	name: Hank Quinlan, Horrible Cop	markdown: kramdownAt this point, I'm hopeful that you've got the hang of creating files and directories using the GitHub web interface, so I'm going stop using screenshots to illustrate those actions. 9Make a _layouts directory, and create file inside it called default. html. (Remember, you can make directories while making new files. See the main. css step if you forgot. ) This is our main layout that will contain repeated elements like our &lt;head&gt; and &lt;footer&gt;. Now we won't have to repeat that markup on every single page we create, making maintenance of our site much easier. So let's move those elements from index. html into default. html to get something that looks like this in the end: &lt;!DOCTYPE html&gt;	&lt;html&gt;		&lt;head&gt;			&lt;title&gt;&lbrace;&lbrace; page. title &rbrace;&rbrace;&lt;/title&gt;			&lt;!-- link to main stylesheet --&gt;			&lt;link rel= stylesheet  type= text/css  href= /css/main. css &gt;		&lt;/head&gt;		&lt;body&gt;			&lt;nav&gt;	  		&lt;ul&gt;	    		&lt;li&gt;&lt;a href= / &gt;Home&lt;/a&gt;&lt;/li&gt;		    	&lt;li&gt;&lt;a href= /about &gt;About&lt;/a&gt;&lt;/li&gt;	    		&lt;li&gt;&lt;a href= /cv &gt;CV&lt;/a&gt;&lt;/li&gt;	    		&lt;li&gt;&lt;a href= /blog &gt;Blog&lt;/a&gt;&lt;/li&gt;	  		&lt;/ul&gt;			&lt;/nav&gt;			&lt;div class= container &gt;						&lbrace;&lbrace; content &rbrace;&rbrace;						&lt;/div&gt;&lt;!-- /. container --&gt;			&lt;footer&gt;	  		&lt;ul&gt;	    		&lt;li&gt;&lt;a href= mailto:hankquinlanhub@gmail. com &gt;email&lt;/a&gt;&lt;/li&gt;	    		&lt;li&gt;&lt;a href= https://github. com/hankquinlan &gt;github. com/hankquinlan&lt;/a&gt;&lt;/li&gt;				&lt;/ul&gt;			&lt;/footer&gt;		&lt;/body&gt;	&lt;/html&gt;Take note of the &lbrace;&lbrace; page. title &rbrace;&rbrace; and &lbrace;&lbrace; content &rbrace;&rbrace; tags in there. They're what Jekyll calls liquid tags, and these are used to inject content into the final web page. More on this in a bit.  10 Now update your index. html to use your default layout. Replace the entire contents of that file with this: ---layout: defaulttitle: Hank Quinlan, Horrible Cop---&lt;div class= blurb &gt;	&lt;h1&gt;Hi there, I'm Hank Quinlan!&lt;/h1&gt;	&lt;p&gt;I'm best known as the horrible cop from &lt;em&gt;A Touch of Evil&lt;/em&gt; Don't trust me. &lt;a href= /about &gt;Read more about my life. . . &lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;!-- /. blurb --&gt;Notice the plain text at the top of the file. Jekyll calls this the Front-matter. Any file on your site that contains this will be processed by Jekyll. Every time you commit a file that specifies layout: default at the top, Jekyll will magically generate the full HTML document by replacing &lbrace;&lbrace; content &rbrace;&rbrace; in _layouts/default. html with the contents of the committed file. Awesome! Setting up a Blog: A Jekyll-based blog uses the same conventions that we've familiarized ourselves with in the previous steps, but takes things further by adding a few more for us to follow. Jekyll is very flexible allowing you to extend your site as you wish, but in this guide we're only going to cover the basics: creating a post, making a page to list our posts, creating a custom permalink for posts, and creating an RSS feed for the blog.  We'll want to create a new layout for our blog posts called post. html and a folder to store each individual post called _posts/. 11 Start by creating the layout. Create a file named post. html in your _layouts folder. Notice the post layout uses the default layout as it's base, and adds a couple new liquid tags to print the title of the post and date: ---layout: default---&lt;h1&gt;&lbrace;&lbrace; page. title &rbrace;&rbrace;&lt;/h1&gt;&lt;p class= meta &gt;&lbrace;&lbrace; page. date | date_to_string &rbrace;&rbrace;&lt;/p&gt;&lt;div class= post &gt; &lbrace;&lbrace; content &rbrace;&rbrace;&lt;/div&gt;12 Make a _posts/ directory where we'll store our blog posts. Inside that folder will be our first post. Jekyll is very strict with how these files are named, so pay attention. It must follow the convention YYYY-MM-DD-title-of-my-post. md. This file name gets translated into the permalink for the blog post. So in this example, we'll create a file named 2014-04-30-hank-quinlan-site-launched. md:---layout: posttitle:  Hank Quinlan, Horrible Cop, Launches Site date: 2014-04-30---Well. Finally got around to putting this old website together. Neat thing about it - powered by &lbrack;Jekyll&rbrack;&lpar;http://jekyllrb. com&rpar; and I can use Markdown to author my posts. It actually is a lot easier than I thought it was going to be. 	Note the file extension . md stands for Markdown, and the Markdown syntax used inside the file gets converted to HTML by Jekyll. Like Wikitext, Markdown is a markup language with a syntax closer to plain text. The idea of Markdown is to get out of the author's way so they can write their HTML content quickly, making Markdown very suitable as a blog authoring syntax. If you aren't already, you'll want to get familiar with Markdown syntax, and this printable cheatsheet (PDF) will be your best friend. 		After committing the new post, navigate to http://username. github. io/YYYY/MM/DD/name-of-your-post to view it.  	All this is great, but your readers won't always know the exact URLs of your posts. So next we need to create a page on our site that lists each post's title and hyperlink. You could create this list on your homepage or alternatively, create a blog subpage that collects all of your posts. We're going to do the latter. 		13 Create a blog directory and create a file named index. html inside it. To list each post, we'll use a foreach loop to create an unordered list of our blog posts: ---layout: defaulttitle: Hank Quinlan's Blog---	&lt;h1&gt;&lbrace;&lbrace; page. title &rbrace;&rbrace;&lt;/h1&gt;	&lt;ul class= posts &gt;	 &lbrace;&percnt; for post in site. posts &percnt;&rbrace;	  &lt;li&gt;&lt;span&gt;&lbrace;&lbrace; post. date | date_to_string &rbrace;&rbrace;&lt;/span&gt; &raquo; &lt;a href= &lbrace;&lbrace; post. url &rbrace;&rbrace;  title= &lbrace;&lbrace; post. title &rbrace;&rbrace; &gt;&lbrace;&lbrace; post. title &rbrace;&rbrace;&lt;/a&gt;&lt;/li&gt;	 &lbrace;&percnt; endfor &percnt;&rbrace;	&lt;/ul&gt;		Now checkout http://username. github. io/blog/. You should see your first post listed and linked there. Nice job! 		Customizing Your Blog: 	We've only begun to scratch the surface with Jekyll's blog aware functionality. This guide is only going to cover a couple more steps you might want to take for your blog. 		You may have noticed that the URL of your blog post does not include the blog directory in it. In Jekyll we can control the structure of our permalinks by editing the _config. yml file we created in Step 8. So let's change our permalink structure to include the blog directory. 		14 Edit the _config. yml file. Add the following line at the end of the file: 				permalink: /blog/:year/:month/:day/:title		Now your blog posts will live at http://username. github. io/blog/YYYY/MM/DD/name-of-your-post. 		It is also very easy to setup an RSS feed for your blog. Every time you publish a new post, it will get added to this RSS file. &lt;/a&gt;	15 Inside your blog/ directory create a file and name it atom. xml. Add this to file:---layout: feed---&lt;?xml version= 1. 0  encoding= utf-8 ?&gt;&lt;feed xmlns= http://www. w3. org/2005/Atom &gt;	&lt;title&gt;Hank Quinlan's Blog&lt;/title&gt;	&lt;link href= http://hankquinlan. github. io/blog/atom. xml  rel= self /&gt;	&lt;link href= http://hankquinlan. github. io/blog /&gt;	&lt;updated&gt;&lbrace;&lbrace; site. time | date_to_xmlschema &rbrace;&rbrace;&lt;/updated&gt;	&lt;id&gt;http://hankquinlan. github. io/blog&lt;/id&gt;	&lt;author&gt;		&lt;name&gt;Hank Quinlan&lt;/name&gt;		&lt;email&gt;hankquinlanhub@gmail. com&lt;/email&gt;	&lt;/author&gt;	&lbrace;&percnt; for post in site. posts &percnt;&rbrace;		&lt;entry&gt;			&lt;title&gt;&lbrace;&lbrace; post. title &rbrace;&rbrace;&lt;/title&gt;			&lt;link href= http://hankquinlan. github. io&lbrace;&lbrace; post. url &rbrace;&rbrace; /&gt;			&lt;updated&gt;&lbrace;&lbrace; post. date | date_to_xmlschema &rbrace;&rbrace;&lt;/updated&gt;			&lt;id&gt;http://hankquinlan. github. io&lbrace;&lbrace; post. id &rbrace;&rbrace;&lt;/id&gt;			&lt;content type= html &gt;&lbrace;&lbrace; post. content | xml_escape &rbrace;&rbrace;&lt;/content&gt;		&lt;/entry&gt;	&lbrace;&percnt; endfor &percnt;&rbrace;&lt;/feed&gt;Now you can include a link to your RSS feed somewhere on your site for users to subscribe to your blog in their feed aggregator of choice. Navigate to http://username. github. io/blog/atom. xml to view your feed. Note: In Chrome, your feed might look like an error, but it isn't. Chrome doesn't know how to display XML. Wrapping Up: 		16 You're almost done! Don't forget to create and commit your about/index. html and cv/index. html pages. Since I'm sure you've got the hang of things now, I'll back off and let you get these pages finished on your own.  17 Before going any further, take the time to setup Git and Jekyll on your own computer. This tutorial is all about Git in the web browser, so really it's only the half way point. You're going to have to do this if you want to be able to upload image or PDF files to your GitHub repo. GitHub's tutorials and desktop application make local setup easy, and now that you know many of Git and GitHub's basic concepts, you should be able to get this going. Go do it! Next Steps: Hopefully this guide has given you the confidence to do many other things with Git, GitHub, Jekyll, and your website or blog. You could go in many different directions at this point, as I'm sure you've already started thinking about, but here are a few other things I think would be worth your time: 	Create _includes. They're a lot like _layouts, only smaller snippets of markup and can be injected into your _layouts and pages. 		Try creating an _include file that inserts Google Analytics tracking code into your &lt;head&gt; so you can get stats on the visitors to your website. Here's an example. 			Want commenting for your blog? Create a DISQUS _include and call it in your post. html layout. 	Don't want github. io in your URL? Set up a custom domain. 	Add blogging pagination	Create a sitemap. xml file for better SEO. You can have one automatically generated by GitHub Pages. See this one I use for my site. 	Become a coding pro and create a development branch of your site. Each time you want to fix a bug or add a new feature you can create a copy of your master branch, make your changes, and then merge that branch to the master branch. The idea is to keep the master branch clean. 	Need more inspiration? Check out how the sites from the developers of Jekyll are setup or browse this huge list of sites using Jekyll. Resources: I'll try to keep this list current and up to date. If you know of a great resource you'd like to share or notice a broken link, please get in touch. Git, GitHub, and GitHub Pages: Git DocumentationLearn Git and GitHub in 15 minutesGitHub Pages HelpGitHub HelpGitHub Cheat SheetGitHub GlossaryGitHub For AcademicsJekyll: Sites Using JekyllBlog Migrations to JekyllMarkdown: 	Official Markdown Spec	Printable Markdown Cheatsheet	Markdown Cheatsheet	GitHub Flavored MarkdownNotes: 1. Somasundaram, R. (2013). Git: Version Control for Everyone (pp. 9-17). Birmingham, UK: Packt Publishing. &lt;/div&gt;&lt;/div&gt;   "
    }, {
    "id": 5,
    "url": "https://dendibakh.github.io/about/",
    "title": "About This Site",
    "body": "    dendibakh. github. io is:          Licensed under a Creative Commons Attribution 3. 0 Unported License.     Marked up with W3C validated HTML5    WCAG 2. 0 compliant    Responsive and compatible with a variety of devices    Built with Bootstrap 3. 0    Using typefaces Merriweather by Eben Sorkin and Montserrat by Julieta Ulanovsky    Powered by Jekyll    Hosted on GitHub thanks to GitHub Pages    A work in progress          "
    }, {
    "id": 6,
    "url": "https://dendibakh.github.io/notes/",
    "title": "Notes",
    "body": "&lt;div class= notes &gt; 			{% include post-list. html %}		&lt;/div&gt; &lt;!-- . notes --&gt;"
    }, {
    "id": 7,
    "url": "https://dendibakh.github.io/",
    "title": "dendibakh.github.io",
    "body": "			    Hi, I'm Denis Bakhvalov and I'm working at Intel doing C/C++ compiler development. This is a blog about performance analysis and tuning, and C/C++ compilers. I love fast code, compiler optimizations and cool algorithms.  I live in the Bay Area, California, USA. All opinions, presented in this blog, are my own and not my employer's. "
    }, {
    "id": 8,
    "url": "https://dendibakh.github.io/blog/2019/04/03/Precise-timing-of-machine-code-with-Linux-perf",
    "title": "Precise timing of machine code with Linux perf.",
    "body": "2019/04/03 - Contents:  Recap on LBR Getting cycles count with linux perf Application: estimating prefetch windowI feel like writing these days, so powered by this feeling I decided to share another quite useful technique that I sometimes use. Today I will show how you can utilize Intel LBR (Last Branch Record) feature to do cycle-based timing of the code blocks. Knowing precisely how much cycles it took to execute certain number of assembly instructions, how great that would be? Want to know how? Keep on reading and you will learn. Just to tease you, look at this desired report:  0000000000400618  movb $0x0, (%rbp,%rdx,1)  000000000040061d  add $0x1, %rdx  0000000000400621  cmp $0xc800000, %rdx  0000000000400628  jnz 0x400618   # 5 cyclesHow cool is that! It’s just an arbitrary code snippet to give you a taste of what you’ll be able to see in this article. This shows precise number of cycles for a given basic block: 4 instructions in a block were executed in 5 cycles. I find it very educational to look at those numbers and try to understand why you get them. Again, great for improving the mental model of how CPU works. I want to thank Andi Kleen for showing me this technique. Recap on LBR: The underlying CPU feature that allows this to happen is called LBR(Last Branch Record). I previously wrote an article about LBR, so I encourage you to visit this blog post if you want to know what it is. LBR feature is used to track control flow of the program. This feature uses MSRs (Model Specific Registers) to store history of last executed branches. Why we are so interested in branches? Well, because this is how we are able to determine the control flow of our program. Since we are interested in branches which are always the last instructions in a basic blocks and all instructions in the basic block are guaranteed to be executed once, we can only focus on branches. Using this control flow statistics we can determine which path of our program (chain of basic blocks) is the hottest. This is sometimes called Hyper Blocks. And there are other applications of LBR feature, see here. Traditionally LBR entry1 has two important components: FROM_IP and TO_IP, which are basically source address of the branch and destination address. If we collect long enough history of source-destination pairs, we will be able to unwind the control flow of our program. Just like a call stack! Sounds nice and simple, right? Starting from Haswell we already could get the information if the branch was predicted or not. There was a dedicated bit for it in the LBR entry. But since Skylake additional LBR_INFO2 component was added to LBR entry which received additional Cycle Count field: Cycle Count - Elapsed core clocks since last update to the LBR stack. With this new field we are able not only to get the branch history, but also to get precise timing in cycles between two taken branches. Awesome! But be aware that it only works starting from Skylake. Additionally you need to have not too old version of perf (mine is 4. 15. 18). Here is the commit which added this functionality in perf. Getting cycles count with linux perf: First of all, to use this functionality with perf, LBR must be enabled: $ dmesg | grep -i lbr[  0. 228149] Performance Events: PEBS fmt3+, 32-deep LBR, Skylake events, full-width counters, Intel PMU driver. To demonstrate usefulness of this technique I took the example from one my previous articles about Top-Down Analysis methodology. The code has a loop with a random load that typically will miss in L3-cache and go to main memory: #include &lt;random&gt;extern  C  { void foo(char* a, int n); }const int _200MB = 1024*1024*200;int main() { char* a = (char*)malloc(_200MB); // 200 MB buffer for (int i = 0; i &lt; _200MB; i++) {  a[i] = 0; } const int min = 1; const int max = _200MB; std::default_random_engine generator; std::uniform_int_distribution&lt;int&gt; distribution(min,max); for (int i = 0; i &lt; 10000000; i++) {  int random_int = distribution(generator);  foo(a, random_int); } free(a); return 0;}Function foo is implemented in assembly like this: foo:# start some irrelevant workOne_KB_of_NOPs# finish some irrelevant work# load that goes to DRAMmov   rax, QWORD [rdi + rsi]# introduce dependency chainmov   rax, QWORD [rdi + rax]xor rax, raxretYou can find complete code sample on my github. Let’s collect LBR data on this application: $ ~/perf record -b -e cycles . /a. out[ perf record: Woken up 13 times to write data ][ perf record: Captured and wrote 3. 024 MB perf. data (3864 samples) ]Now let’s decode the data we collected. You need xed (Intel X86 Encoder Decoder) to see the instructions not just the raw bytes. $ ~/perf script -F +brstackinsn | . . /xed -F insn: -A -64 &gt; dump. txtNow in dump. txt we have all the branch history records, but we are only interested in those which end with return from foo instruction:  . . . &lt;lots of code&gt; 400df0:    0f 1f 84 00 00 00 00  nop  DWORD PTR [rax+rax*1+0x0] 400df7:    00  400df8:    0f 1f 84 00 00 00 00  nop  DWORD PTR [rax+rax*1+0x0] 400dff:    00  400e00:    48 8b 04 37       mov  rax,QWORD PTR [rdi+rsi*1] 400e04:    48 8b 04 07       mov  rax,QWORD PTR [rdi+rax*1] 400e08:    48 31 c0        xor  rax,rax 400e0b:    c3           ret                &lt;== This is the branch of our interestLet’s find something interesting in the dump using the address of our ret instruction:  . . . &lt;lots of code&gt; 0000000000400df0  nopl %eax, (%rax,%rax,1) 0000000000400df8  nopl %eax, (%rax,%rax,1) 0000000000400e00  movq (%rdi,%rsi,1), %rax 0000000000400e04  movq (%rdi,%rax,1), %rax 0000000000400e08  xor %rax, %rax 0000000000400e0b  retq              # PRED 266 cycles 0. 49 IPCCool! But this is just one snippet out of many. With every sample we also capture entire LBR stack which might have multiple branch records for the block that we are interested in: $ grep  0000000000400e0b  dump. txt | grep  cycles  -c20536Notice we have 3864 samples, but 20536 LBR entries for our branch. On the average for every sample we had roughly 5 LBR entries that we are interested in. Application: estimating prefetch window: Let’s see what we can do with this timing information. Let’s collect all the timings for this RET instruction. Here is the script that creates csv file from the dump: $ grep  0000000000400e0b  dump. txt | grep  cycles  | sort &gt; cycle_lines. txt$ sed 's/. *PRED \(. *\) cycles. */\1/' cycle_lines. txt &gt; cycles. txt$ uniq cycles. txt uniq. txt$ cat uniq. txt | while read line ; do echo -n $line ,  &gt;&gt; cycles. csv &amp;&amp; grep $line cycles. txt -w -c &gt;&gt; cycles. csv ; doneNow, let’s plot it: How to read this chart: This chart shows us the number of times we got the certain latency for the basic block. On one hand we don’t want to have high latency, but on the other hand we want to have as higher amount of samples with low latency. Something like that: To estimate prefetch window I removed both loads and collected LBR samples once again. I found that 99% of the time function foo executes in 32 cycles3. It is easy to prove since execution is bound by Retiring. On Skylake we can retire 4 instructions per cycle. In 1 KB of 8-byte NOPs we have 2^10 / 8 = 2^7 instructions. Thus it executes in 2^7 / 4 = 32 cycles. So, this tells us that we have prefetch window of 32 cycles. In the presented case it’s constant and doesn’t vary, but in the code that you might be dealing with it likely won’t be so. Let’s insert prefetch hint and plot the latencies for this case:  for (int i = 0; i &lt; 100000000; i++) {  int random_int = distribution(generator);+  __builtin_prefetch ( a + random_int, 0, 1);  foo(a, random_int); }Here is combined plot with original (baseline) and improved (prefetched) cases: You see, we lowered the spike around 300 cycles and shifted both spikes to the left which is good (towards lower latencies). Also notice the orange dot for 32 cycles latency which has frequency around 3000 times. That means we now have much less cycles that are wasted due to demanding load that misses in caches. See more details about cache misses statistics for this exact case in my previous article about Top-Down Analysis methodology. That’s all. Hope you enjoyed and found it useful! Good luck in using this powerful feature!       See https://software. intel. com/sites/default/files/managed/7c/f1/253669-sdm-vol-3b. pdf, chapter “17. 4. 8 LBR Stack” &#8617;        See https://software. intel. com/sites/default/files/managed/7c/f1/253669-sdm-vol-3b. pdf, chapter “17. 12. 1 MSR_LBR_INFO_x MSR” &#8617;        Function foo is a single basic block function, so it doesn’t matter. Same timing applies to the basic block and the whole function itself.  &#8617;    "
    }, {
    "id": 9,
    "url": "https://dendibakh.github.io/blog/2019/03/27/Machine-code-layout-optimizatoins",
    "title": "Machine code layout optimizations.",
    "body": "2019/03/27 - Contents:  What is machine code layout? Machine code layout optimizations     Basic block placement   Basic block alignment   Function splitting   Function grouping    Profile guided optimizations (PGO) SummaryI spent a large amount of time in 2018 working on optimizations that try to improve layout of machine code. I decided to share what it is and show some basic types of such transformations. I think usually those improvements are underestimated and usually end up being omitted and forgotten. I agree that you might want to start with “fruits that hang lower” like loop unrolling and vectorization opportunities. But knowing that you might get extra 5-10% just from better laying out the machine code is still useful. Before actually going to the core of the article I should say that CPU architects put a lot of efforts in hiding those kind of problems that we will talk about today. There are different structures in the CPU front-end that mitigate code layout inefficiencies, however there is still no free lunch there. Everything that we will discuss in this article applies whenever you see a big amount of execution time wasted due to Front-End issues. See my previous article about Top-Down performance analysis methodology for examples. Compilers? Is that what you think? Right, compilers are very smart nowadays and are getting smarter each day. They do the most part of the job of generating the best layout for your binary. In combination with profile guided optimization (see at the end of the article) they will do most of the things that we will talk about today. And I doubt you can do it better than PGO, however there are still some limitations. Keep on reading and you will know. If you just want to refresh knowledge in your head, you can jump straight to summary. What is machine code layout?: When compiler translates your source code into zeros and ones (machine code) it should generate serial byte sequence. For example, it should convert this C code: if (a &lt;= b) c = 1;Into something like: ; a is in rax; b is in rdx; c is in rcxcmp rax, rdxja . labelmov rcx, 1. label:Assembly instructions will be encoded with some amount of bytes and will be laid out consequently in memory. This is what is called machine code layout. Next I will show some typical optimizations of code layout in the order of biggest impact it can make1. Machine code layout optimizations: Basic block placement: If you’re unfamiliar with what is Basic block, it is a sequence of instructions with single entry and single exit: Compilers like to operate on a basic block level, because it is guaranteed that every instruction in the basic block will be executed exactly once. Thus for some problems we can treat all instructions in the basic block as one instruction. This greatly reduces the problem of CFG (control flow graph) analysis and transformations. All right, enough theory. Why one sequence of blocks might be better than the other? Here is the answer. For the code like this: // hot pathif (cond) coldFunc();// hot path againHere are two different physical layouts we may come up with:  What we did on the right was just invert the condition from if (cond) into if (!cond). Arrow suggests that the one on the right is better than the one on the left. But why? Main reason is because we maintain fall through between hot pieces of the code. Not taken branches are fundamentally cheaper that taken. Additionally second case better utilizes L1 I-cache and uop-cache (DSB). See one of my previous posts for further details. You can make a hint to compiler using __builtin_expect construct2: // hot pathif (__builtin_expect(cond, 0)) // NOT likely to be taken coldFunc();// hot path againWhen you dump assembly you will see the layout that looks like on the right. In LLVM this functionality is implemented in lib/CodeGen/MachineBlockPlacement. cpp based on lib/Analysis/BranchProbabilityInfo. h and lib/Analysis/BlockFrequencyInfoImpl. cpp. It is very educational to browse through the code and see what heuristics are implemented there. There are also hidden gems implemented in MachineBlockPlacement. cpp like, for example, rotating blocks of the loop. It’s not obvious at a glance why it is done and there is no explanation in the comments either. I learned why we need it the hard way after disabling it and look at the result. I might write a separate article on that topic. Facebook in the mid 2018 open-sourced their great peace of work called BOLT (github). This tool works on already compiled binary. It uses profile information to reorder basic blocks within the function3. I think it shouldn’t be too hard to integrate it in the build system and enjoy the optimized code layout! The only thing you need to worry about is to have representative and meaningful workload for collecting profiling information, but that a topic for PGO which we will touch later. Basic block alignment: I already wrote a complete article on this topic some time ago: Code alignment issues. This is purely microarchitectural optimization which is usually applied to loops. Figure below is the best brief explanation of the matter:  Idea here is that shift the hot code (in yellow) down using NOPs (in blue) so that the whole loop will reside in one cache line. On the picture below cache line start from c0 and ends at ff. This transformation usually improves I-cache and DSB utilization. In LLVM it is implemented in the same file as basic block placement algorithms: lib/CodeGen/MachineBlockPlacement. cpp, look at MachineBlockPlacement::alignBlocks(). This topic was so popular that I wrote also article that describes compiler different options in LLVM to manually control alignment of basic blocks: Code alignment options in llvm. For experimental purposes it is also possible to emit assembly listing and then insert NOP instructions or ALIGN4 assembler directives: ; will place the . loop at the beginning of 256 byte boundaryALIGN 256. loop dec rdi jnz rdiFunction splitting: The idea of function splitting is to separate hot from cold code. This usually improves the memory locality of the hot code. Example: void foo(bool cond1, bool cond2) { // hot path if (cond1)  // cold code 1 //hot code if (cond2)  // cold code 2}We might want to cut cold part of the function into it’s own new function and put a call to it instead. Something like this: void foo(bool cond1, bool cond2) { // hot path if (cond1)  cold1();  //hot code if (cond2)  cold2(); }void cold1() __attribute__((noinline)) { // cold code 1 }void cold2() __attribute__((noinline)) { // cold code 2 }This is how it looks in the physical layout:  Because we just left the call instruction inside the hot path it’s likely that next hot instruction will reside in the same cache line. This improves utilization of CPU Front-End data structures like I-cache and DSB-cache. This transformation contains another important idea which is disable inlining of cold functions. You see how we forbid inlining of cold1 and cold2 functions above? The same applies when you see a lot of cold code that appeared after inlining some function. You don’t need to split it into new function, just disable inlining of this function. Typically we also would like to put cold code into subsection of . text or even into a separate section. This optimization is beneficial for relatively big functions with complex CFG where there are big pieces of cold code inside hot path, for example, switch statement inside the loop. There is implementation of this functionality in LLVM that resides in lib/Transforms/IPO/HotColdSplitting. cpp. Last time5 I analyzed it wasn’t able to do much with really complicated CFGs, but the work is under development, so I hope it will be better soon. Function grouping: Usually we want to place hot functions together such that they touch each other in the same cache line.   In the figure above you can see how we grouped foo, bar and zoo in such a way that now their code fits in only 3 cache lines. Additionally when we call zoo from foo, beginning of zoo is already in the I-cache, since we fetched that cache line already. Similar to previous optimizations here we try to improve utilization of I-cache and DSB-cache. This is rather the job for the linker, because it has a power of placing functions in executable. In gold linker it can be done using –section-ordering-file option. This optimization works best when there are many small hot functions. There is also very cool tool for doing this automatically: hfsort. It uses linux perf for getting profile information, then it does it’s ordering magic and gives the text file with optimized function order which you can then pass to the linker. Here is the whitepaper if you want to read more about the underlying algorithms. Profile guided optimizations (PGO): It is usually the best option to use PGO if you can come up with a typical scenario for your application. Why that’s important? Well, I will explain shortly. Compiling a program and generating assembly is all about heuristics. You will be surprised how much uncertainty there is inside every good optimizing compiler, like LLVM. For a lot of decisions compiler makes it tries to guess the best solution based on some typical cases. For example, should I inline this function? But what if it is called a lot of times? In this case I probably should do it, but how do I know that beforehand? Here is when profiling information becomes handy. Given profiling information compiler doesn’t need to question what should be the decision. And often times you will see in compiler implementation the pattern like: if (profiling information available) make decision based on profiling dataelse make decision based on heuristicsKeep in mind that compiler “blindly” uses the profile data you provided. What do I mean by that is compiler assumes that all the workloads will behave the same, so it optimizes your app just for that single workload. So be careful with choosing the workload to profile, because you can make things even worse. I must say that it shouldn’t be exactly single workload since you can merge multiple profile data into single file. I’ve seen real workloads that were improved up to 15% from profile guided optimizations. PGO does not only improve code placement, but also improve register allocation, because with PGO compiler can put all the hot variables into registers, etc. The guide for using PGO in clang is described here. Summary:           How transformed?   Why helps?   Works best for   Done by         Basic block placement   maintain fall through hot code   not taken branches are cheaper that taken; better caches utilization   any code, especially with a lot of branches   compiler       Basic block alignment   shift the hot code using NOPS   better caches utilization   hot loops   compiler       Function splitting   split cold blocks of code and place them in separate functions   better caches utilization   functions with complex CFG when there are big blocks of cold code between  hot parts   compiler       Function grouping   group hot functions together   better caches utilization   many small hot functions   linker         This is solely based on my experience and typical gains I saw.  &#8617;        You can read more about builtin-expect here: https://llvm. org/docs/BranchWeightMetadata. html#builtin-expect.  &#8617;        It also able to do function splitting and grouping.  &#8617;        This example use MASM. Otherwise you will see . align directive.  &#8617;        I did it on December 2018.  &#8617;    "
    }, {
    "id": 10,
    "url": "https://dendibakh.github.io/blog/2019/02/23/How-to-collect-performance-counters-on-Windows",
    "title": "How to collect CPU performance counters on Windows?",
    "body": "2019/02/23 - Contents:  What tools you will need? Using tracelog and xperf for collecting traces Parsing traces with python script What other counters we can collect? ConclusionI was asked a couple of times by my subscribers how to do microarchitectural analysis on Windows? To be honest I never had that problem before. Guess why? Because I work at Intel and of course I have the license to use Intel® VTune™ Amplifier. I can’t fully feel the pain of the people who are doing performance related work on windows and don’t have access to Vtune or AMD CodeAnalyst. Since it wasn’t my problem I didn’t make any efforts towards it. Finally I was browsing through Bartek’s coding blog and found the article Curious case of branch performance. To me that seemed like a case that can be easily proven just by running perf stat if we were on Linux. But since we are on Windows… it’s not that simple. In this article I want to present one way how you can collect PMU counters without Intel® VTune™ Amplifier. I took almost all the info from Bruce Dawson’s blog. He wrote and article which I want to expand and make it more of a step-by-step process. So, all the credit goes to Bruce here, because i didn’t invent this. If you want to try it yourself, I suggest you first reproduce the example described in Bruce’s article (link to github with sources and scripts). Take all that is written in my article with a grain of salt though. I’m not a Windows developer and I’m not spending my time doing performance analysis on Windows. This is just one way to collect PMU counters, but there might be others, more simple and robust. In the end you can purchase Intel® VTune™ Amplifier which by the way can be quite expensive. But I want to say upfront, that there are no real alternatives to Vtune if you are going to do serious performance analysis and tuning on Windows (this is not an advertisement). What tools you will need?:    xperf. You need to install Windows Performance Toolkit which is a part of Windows Assessment and Deployment Kit (Windows ADK). For me xperf was automatically added to PATH.     tracelog. Follow instructions to get this tool. You need the following components to be installed:      Windows Driver Kit   Visual Studio   Windows SDK   Tracelog wasn’t added to my PATH, but I was able to find it under the following path:  C:\Program Files (x86)\Windows Kits\10\bin\10. 0. 17763. 0\x64\ . It might differ for you. Installing all those kits require some time, so please be patient. Using tracelog and xperf for collecting traces: I will use the example that Bruce created, partially repeating his actions. Here is how you can obtain traces (with branch mispediction information) from your application using the tools mentioned above (should be run as Administrator): tracelog. exe -start counters -f counters. etl -eflag CSWITCH+PROC_THREAD+LOADER -PMC BranchMispredictions,BranchInstructions:CSWITCH&lt;your app&gt;xperf -stop countersxperf -merge counters. etl pmc_counters_merged. etlxperf -i pmc_counters_merged. etl -o pmc_counters. txtIf we will look inside pmc_counters. txt we can observe the whole trace in the text format. There is a lot of interesting can be extracted from them, but lets concentrate on two things:  Pmc (performance monitoring counter) event:          Pmc, TimeStamp,  ThreadID, BranchMispredictions, BranchInstructions CSwitch (context switch) event:        CSwitch, TimeStamp, New Process Name ( PID),  New TID, NPri, NQnt, TmSinceLast, WaitTime, Old Process Name ( PID),  Old TID, OPri, OQnt,    OldState,   Wait Reason, Swapable, InSwitchTime, CPU, IdealProc, OldRemQnt, NewPriDecr, PrevCState, OldThrdBamQosLevel, NewThrdBamQosLevelHere is some piece of the actual trace:           Pmc,   214810,    5956, 1101534, 44324578        CSwitch,   214810, ConditionalCount. exe (14224),    5956,  9,  -1,      6,    0,      System (  4),    560,  12,  -1,     Waiting,     WrQueue, NonSwap,   6,  1,  3,  84017152,  0,  0,  Important,  Important          Pmc,   214821,   14460, 1101713, 44326484        CSwitch,   214821,    csrss. exe ( 888),   14460,  14,  -1,    73556,    5, ConditionalCount. exe (14224),    5956,  9,  -1,     Waiting,    WrLpcReply, Swapable,   11,  1,  3,  77701120,  0,  0,  Important,  ImportantNote, that for each CSwitch event there is corresponding Pmc event. We can see that they come with the same timestamp. In this snippet of trace, there was a context switch from our process (which is ConditionalCount. exe) to another process (csrss. exe). We can see this by looking at Old Process Name ( PID) of CSwitch event with timestamp 214821. So, there was some period of time in which ConditionalCount. exe has been executing on CPU (between timestamps 214821 and 214810). The value for the BranchMispredictions counter is constantly growing. We can calculate how much branch mispredictions were there for this period of time, by substracting values from the 2 Pmc event. For this snippet there were 1101713 - 1101534 = 179 branch mispredictions. By summing up together all the deltas we can calculate total number of branch mispredictions for the whole runtime of the app. Pro tip: if you see the numbers that are different from what you expected, I suggest you try to run the same benchmark on Linux using ‘perf stat ' command. You can find lots of articles how to do that on my blog. The other way to is to dump the assembly and check that there is the code you expect. It might be the case that compiler did something smart and eliminated the code that you want to benchmark. Parsing traces with python script: To parse the trace and extract the information Bruce wrote the script. This script actually extracts the PMC values for the processes that we are interested in (2 argument): python. exe etwpmc_parser. py pmc_counters. txt &lt;your app&gt;Here is the output that I received on my machine (Win 10, Intel(R) Core(TM) i5-7300U).          Process name: branch misp rate, [br_misp, total branc] ConditionalCount. exe (14224):      21. 91%, [109184040, 498250335], 3690 context switches, time: 1093072 ConditionalCount. exe (10964):       0. 07%, [369677, 496453009],  761 context switches, time: 257492Vtune shows similar results. What other counters we can collect?: &gt; tracelog. exe -profilesources HelpId Name            Interval Min   Max-------------------------------------------------------------- 0 Timer             10000 1221  1000000 2 TotalIssues          65536 4096 2147483647 6 BranchInstructions       65536 4096 2147483647 10 CacheMisses          65536 4096 2147483647 11 BranchMispredictions      65536 4096 2147483647 19 TotalCycles          65536 4096 2147483647 25 UnhaltedCoreCycles       65536 4096 2147483647 26 InstructionRetired       65536 4096 2147483647 27 UnhaltedReferenceCycles    65536 4096 2147483647 28 LLCReference          65536 4096 2147483647 29 LLCMisses           65536 4096 2147483647 30 BranchInstructionRetired    65536 4096 2147483647 31 BranchMispredictsRetired    65536 4096 2147483647Conclusion: This goes anywhere near to what Linux perf or Vtune are able to do. The number of counters is limited and this is only does counting, no sampling (see the difference between counting and sampling here ). That’s all true, but at least you can do some initial performance analysis. Second thing is that if you want to collect different PMC than branch misprediction you need to modify not just the tracelog command but also the python script that parses traces. If you know any other/better way to do that, let me know. I would definitely want to hear that. I hope that also helps people that are on Windows and do want to participate in my contest. If so, make sure to subscribe using the form at the bottom of the page. "
    }, {
    "id": 11,
    "url": "https://dendibakh.github.io/blog/2019/02/16/Performance-optimization-contest-1",
    "title": "Performance optimization contest &#35&#49.",
    "body": "2019/02/16 - Contents:  Quickstart Couple of hints What’s NOT allowed Validation Submissions Updated 2nd March 2019     Scores   Optimizations found   Second round    Updated 7th April 2019     Total score table of all my attempts   Recently I announced performance optimization contest in my recent article. If you see this post and haven’t read my initial post about the contest, I recommend that you first read it. Now it’s time to start our first edition. I’m glad to say “Welcome” to every participant! I’m collecting all your submissions until 25th February 2019 and will announce results on 1st March 2019. The benchmark for the contest is:https://github. com/llvm-mirror/test-suite/blob/master/SingleSource/Benchmarks/Shootout/sieve. c 12345678910111213141516171819202122232425262728#include &lt;stdio. h&gt;#include &lt;stdlib. h&gt;int main(int argc, char *argv[]) {#define LENGTH 170000  int NUM = ((argc == 2) ? atoi(argv[1]) : LENGTH);  static char flags[8192 + 1];  long i, k;  int count = 0;  while (NUM--) {	count = 0; 	for (i=2; i &lt;= 8192; i++) {	  flags[i] = 1;	}	for (i=2; i &lt;= 8192; i++) {	  if (flags[i]) {        /* remove all multiples of prime: i */		for (k=i+i; k &lt;= 8192; k+=i) {		  flags[k] = 0;		}		count++;	  }	}  }  printf( Count: %d\n , count);  return(0);}That’s it. Yes, it’s really small. I decided to pick single source benchmark for a first contest in order to offer a quick start for everybody. Quickstart: Because the benchmark has no external dependancies you can build it as simple as: $ wget https://raw. githubusercontent. com/llvm-mirror/test-suite/master/SingleSource/Benchmarks/Shootout/sieve. c$ gcc sieve. c -O3 -o sieve$ time -p . /sieveTarget machine for this edition of the contest is Haswell CPU with 64-bit Linux. Couple of hints:  Collect the baseline (use time or analogs).  Find the hotspot (use perf record).  Find performance headroom     Take a look at the assembly and try to guess how you can do better.    Run through TMAM process.     Build the benchmark, run it and compare against baseline. I also have a few general advises:  Do not try to understand the whole benchmark. For some people (including me) it’s crucial to understand how every peace of code works. For the purposes of optimizing it will be wasted effort. There are CPU benchmarks with thousands LOC (like SPEC2017) it’s absoultely impossible to understand them in a reasonable time. What you need to familiarize yourself with are hotspots. That’s it. You most likely need to understand one function/loop which is not more than 100 LOC.  You have specific workload for which you optimize the benchmark. You don’t need to optimize it for any other input/workload. The main principle behind Data-oriented design is that you know the data of your application. Information presented in llvm documentation: Benchmarking tips migth also be helpful. What’s NOT allowed:  Do not rewrite the benchmark completely or introduce major changes in algorithms.  Do not manually parallelize the benchmark, e. g converting it from single- to multi-threaded or offload computations to the GPU. I mean, I’m glad that you can do it and I will be happy to take a look what you did, but it’s not the intent of the contest.  Using PGO is allowed, however you can use it only for driving you optimizations, not for the submission. So, you can check how the benchmark gets better with PGO and understand why. And then make this optimization manually. Again, the purposes is practicing and learning. Validation: Your benchmark should output the same result as reference: Count: 1028Submissions:  Disclaimer: Again I think it’s worth to say that I will not use your submissions in any commercial purposes. The baseline that I will be measuring against is ‘gcc -O3’. If you’re willing to submit your work please subscribe to my mailing list and then send all you have via email. Rules and guidelines for submissions I described earlier in my initial post in “Q6: How should the submission look like?”. I’m collecting all your submissions until 25th February 2019 and will announce results on 1st March 2019. Good luck and have fun! P. S. I have no automation for my contest yet, so if anyone knows any good service or a way to automate it using web interface, please let me know. P. P. S. I’m also open to your comments and suggestions. If you have any suggestions for the benchmarks for the next edition of contest, please tell me. Updated 2nd March 2019: Scores: I received 7 submissions for the contest which is quite good. I wrote the script to automate measuring process (you can find it here). My setup: xeon E3-1240 v3 + gcc 7. 4 + Red Hat 7. 4. The baseline code showed this results: time(s)  submission  timings for 10 consecutive runs (s)([2. 31,  'baseline', [2. 31, 2. 32, 2. 32, 2. 32, 2. 32, 2. 32, 2. 32, 2. 32, 2. 32, 2. 32]])Here are the best 3 submissions:  time(s)  submission     timings for 10 consecutive runs (s)              speedup1. ([0. 34,  'Nathan Kurz'  , [0. 34, 0. 34, 0. 34, 0. 34, 0. 34, 0. 34, 0. 34, 0. 34, 0. 34, 0. 34]], ' + 6. 79x')2. ([0. 46,  'Hector Grebbell', [0. 46, 0. 46, 0. 46, 0. 46, 0. 46, 0. 46, 0. 46, 0. 46, 0. 47, 0. 47]], ' + 5. 02x')3. ([1. 06,  'Hadi Brais'   , [1. 06, 1. 07, 1. 07, 1. 07, 1. 07, 1. 07, 1. 07, 1. 07, 1. 07, 1. 07]], ' + 2. 18x')Congratulations! Optimizations found: There were some amount of complaints about the benchmark and I admit it has high-level inefficiencies. So most of the optimizations I will present have algoritmic nature and are not necessary related to CPU microarchitecture. Indeed, it is not smart to look for low-level optimizations when there are high-level ones. However, bear with me, I do have something to offer. Because things that I will present can be easily found on the web, I will not explain it in very details. Yo can read more about Sieve of Eratosthenes on the wikipedia.  Limit loop16 (loop on the line# 16) to run until sqrt(8192) - 2. 6x speedup. That requires to move counting it in a separate loop.  Do not mark even numbers and iterate only through the odd numbers - 2. 3x speedup. Source code with both optimizations combined can be found on my github. And of course there was a submission that utilized C++ constexpr feature and just printed correct answer in the runtime. Other observations made by participants:  converting char flags[] to bitfields showed negative effect.  all the code and all the data fits into L1I and L1D caches. No prefetching opportunities. Second round: Now when all the low-hanging fruits are found it’s not that easy to optimize it further. But it doesn’t mean there is absolutely no performance headroom. I briefly tried to optimize the version from my github even more. By adding proper loop unrolling hints to the compiler and adjusting alignment of the loops I was able to reduce the execution time from 0. 32s down to 0. 27s. If I’ll find time I’ll write more about it. If you wish you can take this as your homework. Take the the code from my github as a baseline. To make results more stable I suggest to increase the number of repetitions from 170000 to 1700000. Let me know what you can find. Updated 7th April 2019: I spent some time on solving the contest myself. Here is the baseline that I started with: 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;stdio. h&gt;#include &lt;stdlib. h&gt;int main(int argc, char *argv[]) {#define LENGTH 1700000  int NUM = ((argc == 2) ? atoi(argv[1]) : LENGTH);  static char flags[8192 + 1];  long i, k;  long sqrt_max = sqrt(8192);  int count;  while (NUM--) {    for (i=2; i &lt; 8192; i+=2) {      flags[i] = 0;   // evens are not prime      flags[i+1] = 1;  // odd numbers might be    }	// flags[8192] doesn't need to be set, because it's even    // since even numbers already crossed out we can:    // - start from i=3     // - iterate over odd numbers (i+=2)	for (i=3; i &lt;= sqrt_max; i+=2) { 	  if (flags[i]) {        /* remove all multiples of prime: i */		// 1. less than i*i already marked		// 2. only mark odd multiples (i*i+i will		//  produce even number, which is already marked)		for (k=i*i; k &lt;= 8192; k+=2*i) {		  flags[k] = 0;		}	  }	}    count = 1; // accounting for 2 is prime    for (long i = 2; i &lt;= 8192; i++) {      if (flags[i])        count++;    }  }  printf( Count: %d\n , count);  return(0);}Here is what we can be done: 1) User on my mailing list Nan Xiao suggested that since only odd number can be prime, there is no need to initialize even number (line 14). Additionally there is no need to check even number when counting. Full version can be found here. Interestingly that has negative effect of 20% performance drop. But the reason for that is that now it’s harder for compiler to vectorize the memset (line 13) and counting (line 34) loops since it can’t touch even locations. 2) I profiled the baseline and found that we have 2 hotspots in the benchmark. First is the loop where we mark odd multiples (line 28). Second is the counting loop (line 34). I also applied Top-Down Analysis on it and found that we are 80% bound by Retirement, which means we already are doing quite good. Let’s look at the first hotspot: for (k=i*i; k &lt;= 8192; k+=2*i) { flags[k] = 0;}Here is the assembly generated by gcc 7. 4: . loop:mov BYTE PTR [rax+0x601080], 0x0add rax,rcxcmp rax,0x2000jle . loopIn this loop rax corresponds to k and rcx corresponds to 2*i. 3) First idea I tried was unrolling this loop using compiler hint. For that to work I bumped GCC version to 8. 2. #pragma GCC unroll 2for (k=i*i; k &lt;= 8192; k+=2*i) { flags[k] = 0;}Unrolling this loop gave negative effects for all the factors I tried: 2,4,8,16,32. We didn’t increase the amount of instructions executed on each iteration since we need to check if we are inbounds for every write. But unrolling the loop reulted in code bloat which is bad for CPU front-end. 4) I decided to check if this is the best what we can do in the first hotspot. I applied the technique I showed here (yes, I did that part on Skylake). And found that 99% of the time each iteration of the loop runs in one cycle. This loop is bound by stores and since we have one execution port for doing stores on Haswell and Skylake, we can’t do better than that. There is another (not accurate) way to prove this. It is very simple, but you’ve better not use it on any kernels that are more complicated than the one in this contest. So, first I got the total number of cycles: $ perf stat . /sieve    16299730506   cycles          #  3,784 GHz We have 1700000 repetitions of the outermost loop. Dividing total number of cycles by the number of repetitions we get roughly 10000 cycles per repetition. Knowing that we have 50% of the time spent in this loop we can say that we have 5000 for this loop. After that I manually instrumented the code to count the trip count (number of iterations) of the loop where we mark the odd multiples. It turned out that we have 4823 iterations in total. Dividing number of cycles for the loop by the number of iterations of the loop we get ~1 cycle per iteration. This is very inaccurate way, but it works for small kernels. 5) I tried aligning different loops at different boundaries (16 bytes, 32, 64, 128) but that yielded negative effect in all the cases. The reason for this is that NOPs are injected in the execution path. 6) I looked at the other hotspot which was the counting loop: for (long i = 2; i &lt;= 8192; i++) { if (flags[i])  count++;}This code was vectorized, which is good. But it was done using XMM vectors, which is not optimal. I tried to use AVX instructions: gcc sieve. c -O3 -march=core-avx2 -o sieveThat gave 20% speedup. 7) I profiled improved version again and now we can see three hot places:  marking loop (line 28) ~70% counting loop (line 34) ~25% memset loop (line 13) ~5%I also tried unrolling the first loop, but it didn’t make any improvement. Running TMAM on improved version showed this result: $ ~/pmu-tools/toplev. py --core S0-C0 -l1 -v --no-desc taskset -c 0 . /sieveS0-C0  FE       Frontend_Bound:      7. 38 +-   0. 00 % Slots belowS0-C0  BAD      Bad_Speculation:     5. 76 +-   0. 00 % Slots belowS0-C0  BE       Backend_Bound:      24. 80 +-   0. 00 % Slots    &lt;==S0-C0  RET      Retiring:        62. 06 +-   0. 00 % Slots below$ ~/pmu-tools/toplev. py --core S0-C0 --nodes Memory_Bound,Core_Bound -v --no-desc taskset -c 0 . /sieveS0-C0  BE/Mem     Backend_Bound. Memory_Bound:     1. 91 +-   0. 00 % Slots belowS0-C0  BE/Core    Backend_Bound. Core_Bound:      22. 85 +-   0. 00 % Slots    &lt;==$ ~/pmu-tools/toplev. py --core S0-C0 --nodes Divider,Ports_Utilization -v --no-desc taskset -c 0 . /sieveS0-C0-T0 BE/Core    Backend_Bound. Core_Bound. Divider:          0. 00 +-   0. 00 % Clocks belowS0-C0-T0 BE/Core    Backend_Bound. Core_Bound. Ports_Utilization:     24. 43 +-   0. 00 % Clocks below &lt;==Most likely that points us to the problem we analyzed before, which is marking loop (line 28) is bound by the stores. Total score table of all my attempts: time(s) submission  timings for 10 consecutive runs (s)               speedup([3. 56,  'AVX2',    [3. 56, 3. 56, 3. 56, 3. 56, 3. 56, 3. 56, 3. 56, 3. 56, 3. 56, 3. 56]], ' + 1. 21x')([4. 29,  'baseline',  [4. 29, 4. 29, 4. 29, 4. 29, 4. 3, 4. 3, 4. 3, 4. 3, 4. 3, 4. 33]],   ' + 1. 0x' )([4. 35,  'align loop', [4. 35, 4. 35, 4. 35, 4. 35, 4. 35, 4. 35, 4. 35, 4. 35, 4. 35, 4. 35]], ' + 0. 99x')([4. 4,  'unroll',   [4. 4, 4. 4, 4. 4, 4. 41, 4. 41, 4. 41, 4. 41, 4. 41, 4. 41, 4. 41]],  ' + 0. 97x')([5. 28,  'elim_evens', [5. 28, 5. 28, 5. 29, 5. 29, 5. 29, 5. 29, 5. 29, 5. 29, 5. 29, 5. 29]], ' + 0. 81x')"
    }, {
    "id": 12,
    "url": "https://dendibakh.github.io/blog/2019/02/09/Top-Down-performance-analysis-methodology",
    "title": "Top-Down performance analysis methodology.",
    "body": "2019/02/09 - Contents:  TMAM concept Step 1 Step 2 Fixing the issue Additional resources and links:This post aims to help people that want to better understand performance bottlenecks in their application. There are many existing methodolgies to do performance anlysis, but not so many of them are robust and formal. When I was just starting with performance work I usually just profiled the app and tried to grasp through the hotspots of the benchmark hoping to find something there. This often lead to random experiments with unrolling, vectorization, inlining, you name it. I’m not saying it’s always a loosing strategy. Sometimes you can be lucky to get big performance boost from random experiments. But usually you need to have very good intuition and luck :). In this post I show more formal way to do performance analysis. It’s called Top-down Microarchitecture Analysis Method (TMAM) (Intel® 64 and IA-32 Architectures Optimization Reference Manual, Appendix B. 1). In this metodology we try to detect what was stalling our execution starting from the high-level components (like Front End, Back End, Retiring, Branch predictor) and narrowing down the source of performance inefficiencies. It’s an iterative process with 2 steps:  Identify the type of the performance problem.  Locate the exact place in the code using PEBS (precise event). After fixing performance issue you repeat the process again. If it doesn’t make sense to you yet, don’t worry, it’ll become clear with the example. TMAM concept: TMAM conceptually works in a “black box” manner, with assumption that we don’t know nothing about the benchmark. Let’s imagine we have the binary (a. out) and it runs for 8. 5 sec: $ time -p . /a. outreal 8. 53TMAM methodology is implemented in toplev tool that is a part of pmu-tools written by Andi Kleen. Step 1: This is the two most important pictures for TMAM (taken from Intel manual, see link above). First is the breakdown of metric levels in TMAM and second shows reasoning for a single uop:  We will run our app and collect specific metrics that will help us to characterize our application. We will try to detect in which category our application will fall to. Let’s run it, collecting level-1 metrics on our binary: $ ~/pmu-tools/toplev. py --core S0-C0 -l1 -v --no-desc taskset -c 0 . /a. out. . . S0-C0  FE       Frontend_Bound:     13. 81 +-   0. 00 % Slots belowS0-C0  BAD      Bad_Speculation:     0. 22 +-   0. 00 % Slots belowS0-C0  BE       Backend_Bound:      53. 43 +-   0. 00 % Slots    &lt;==S0-C0  RET      Retiring:        32. 53 +-   0. 00 % Slots belowS0-C0-T0        MUX:          100. 00 +-   0. 00 %      S0-C0-T1        MUX:          100. 00 +-   0. 00 %      All right, now we know that we are bounded by backend. Let’s drill one level down: $ ~/pmu-tools/toplev. py --core S0-C0 -l2 -v --no-desc taskset -c 0 . /a. out. . . S0-C0  FE       Frontend_Bound:               13. 92 +-   0. 00 % Slots belowS0-C0  BAD      Bad_Speculation:               0. 23 +-   0. 00 % Slots belowS0-C0  BE       Backend_Bound:               53. 39 +-   0. 00 % Slots   S0-C0  RET      Retiring:                  32. 49 +-   0. 00 % Slots   S0-C0  FE       Frontend_Bound. Frontend_Latency:      12. 11 +-   0. 00 % Slots belowS0-C0  FE       Frontend_Bound. Frontend_Bandwidth:      1. 84 +-   0. 00 % Slots belowS0-C0  BAD      Bad_Speculation. Branch_Mispredicts:     0. 22 +-   0. 00 % Slots belowS0-C0  BAD      Bad_Speculation. Machine_Clears:       0. 01 +-   0. 00 % Slots belowS0-C0  BE/Mem     Backend_Bound. Memory_Bound:         44. 59 +-   0. 00 % Slots    &lt;==S0-C0  BE/Core    Backend_Bound. Core_Bound:          8. 80 +-   0. 00 % Slots belowS0-C0  RET      Retiring. Base:               24. 83 +-   0. 00 % Slots belowS0-C0  RET      Retiring. Microcode_Sequencer:        7. 65 +-   0. 00 % Slots        Okay, we see that we are actually bounded by memory. Almost half of the execution time CPU was stalled waiting for memory requests to arrive. Let’s try one level deeper: $ ~/pmu-tools/toplev. py --core S0-C0 -l3 -v --no-desc taskset -c 0 . /a. out. . . S0-C0  FE       Frontend_Bound:                 13. 91 +-   0. 00 % Slots below   S0-C0  BAD      Bad_Speculation:                 0. 24 +-   0. 00 % Slots below   S0-C0  BE       Backend_Bound:                 53. 36 +-   0. 00 % Slots      S0-C0  RET      Retiring:                    32. 41 +-   0. 00 % Slots      S0-C0  FE       Frontend_Bound. Frontend_Latency:        12. 10 +-   0. 00 % Slots below   S0-C0  FE       Frontend_Bound. Frontend_Bandwidth:        1. 85 +-   0. 00 % Slots below   S0-C0  BAD      Bad_Speculation. Branch_Mispredicts:       0. 23 +-   0. 00 % Slots below   S0-C0  BAD      Bad_Speculation. Machine_Clears:         0. 01 +-   0. 00 % Slots below   S0-C0  BE/Mem     Backend_Bound. Memory_Bound:           44. 58 +-   0. 00 % Slots      S0-C0  BE/Core    Backend_Bound. Core_Bound:            8. 78 +-   0. 00 % Slots below   S0-C0  RET      Retiring. Base:                 24. 77 +-   0. 00 % Slots below   S0-C0  RET      Retiring. Microcode_Sequencer:          7. 63 +-   0. 00 % Slots      S0-C0  FE       Frontend_Bound. Frontend_Bandwidth. MITE:     7. 33 +-   0. 00 % CoreClocks belowS0-C0  FE       Frontend_Bound. Frontend_Bandwidth. DSB:      2. 19 +-   0. 00 % CoreClocks belowS0-C0  FE       Frontend_Bound. Frontend_Bandwidth. LSD:      0. 00 +-   0. 00 % CoreClocks belowS0-C0-T0 FE       Frontend_Bound. Frontend_Latency. ICache_Misses:      0. 05 +-   0. 00 % Clocks below     S0-C0-T0 FE       Frontend_Bound. Frontend_Latency. ITLB_Misses:       0. 00 +-   0. 00 % Clocks below     S0-C0-T0 FE       Frontend_Bound. Frontend_Latency. Branch_Resteers:     0. 10 +-   0. 00 % Clocks_Estimated belowS0-C0-T0 FE       Frontend_Bound. Frontend_Latency. DSB_Switches:       0. 00 +-   0. 00 % Clocks below     S0-C0-T0 FE       Frontend_Bound. Frontend_Latency. LCP:           0. 00 +-   0. 00 % Clocks below     S0-C0-T0 FE       Frontend_Bound. Frontend_Latency. MS_Switches:       3. 86 +-   0. 00 % Clocks        S0-C0-T0 BE/Mem     Backend_Bound. Memory_Bound. L1_Bound:           4. 39 +-   0. 00 % Stalls below     S0-C0-T0 BE/Mem     Backend_Bound. Memory_Bound. L2_Bound:           2. 42 +-   0. 00 % Stalls below     S0-C0-T0 BE/Mem     Backend_Bound. Memory_Bound. L3_Bound:           5. 75 +-   0. 00 % Stalls        S0-C0-T0 BE/Mem     Backend_Bound. Memory_Bound. DRAM_Bound:          47. 11 +-   0. 00 % Stalls         &lt;==S0-C0-T0 BE/Mem     Backend_Bound. Memory_Bound. Store_Bound:          0. 69 +-   0. 00 % Stalls below     S0-C0-T0 BE/Core    Backend_Bound. Core_Bound. Divider:             8. 56 +-   0. 00 % Clocks below     S0-C0-T0 BE/Core    Backend_Bound. Core_Bound. Ports_Utilization:       11. 31 +-   0. 00 % Clocks below     S0-C0-T0 RET      Retiring. Base. FP_Arith:                  1. 45 +-   0. 00 % Uops below      S0-C0-T0 RET      Retiring. Base. Other:                   98. 55 +-   0. 00 % Uops below      S0-C0-T0 RET      Retiring. Microcode_Sequencer. Assists:           0. 00 +-   0. 00 % Slots_Estimated below S0-C0-T0        MUX:                           3. 45 +-   0. 00 %            Cool! We found the bottleneck. Step #1 is completed. Let’s now go to the step #2. Step 2: To locate the place in the code where this is happening, we need to refer to the TMA metrics table. I know, I know, it looks scary and big. Don’t worry. It’s complex only when you see it the first time. For Skylake architecture DRAM_Bound metric is calculated using CYCLE_ACTIVITY. STALLS_L3_MISS performance event. Let’s collect it: $ perf stat -e cycles,cpu/event=0xa3,umask=0x6,cmask=0x6,name=CYCLE_ACTIVITY. STALLS_L3_MISS/ . /a. out    32226253316   cycles                               19764641315   CYCLE_ACTIVITY. STALLS_L3_MISS                  According to the definition of CYCLE_ACTIVITY. STALLS_L3_MISS it counts cycles when execution stalls while L3 cache miss demand load is outstanding. We can see that there are ~60% of such cycles which is pretty bad. In the Locate-with column there is performance event that we can use to locate exact place in the code where the issue occurs. For DRAM_Bound metric we should use MEM_LOAD_RETIRED. L3_MISS_PS precise event. Let’s sample on it: $ perf record -e cpu/event=0xd1,umask=0x20,name=MEM_LOAD_RETIRED. L3_MISS/ppp . /a. outIf you don’t understand the underlying mechanics of what we just did, I encourage you to read one of my previous posts: Basics of profiling with perf and Understanding performance events skid. Let’s look into the profile: $ perf report -n --stdio. . . # Samples: 33K of event 'MEM_LOAD_RETIRED. L3_MISS'# Event count (approx. ): 71363893## Overhead    Samples Command Shared Object   Symbol             # . . . . . . . .  . . . . . . . . . . . .  . . . . . . .  . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . #  99. 95%     33811 a. out  a. out       [. ] foo           &lt;==   0. 03%      52 a. out  [kernel. kallsyms] [k] get_page_from_freelist   0. 01%       3 a. out  [kernel. kallsyms] [k] free_pages_prepare   0. 00%       1 a. out  [kernel. kallsyms] [k] free_pcppages_bulkAll L3 misses are caused by our code. Let’s drill down to assembly: $ perf annotate --stdio -M intel fooPercent |   Source code &amp; Disassembly of a. out for MEM_LOAD_RETIRED. L3_MISS-------------------------------------------------------------------------------     :   Disassembly of section . text:     :     :   0000000000400a00 &lt;foo&gt;:     :   foo():  0. 00 :    400a00:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400a08:    nop  DWORD PTR [rax+rax*1+0x0]         . . .   0. 00 :    400df0:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400df8:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400e00:    mov  rax,QWORD PTR [rdi]  0. 00 :    400e03:    mov  rax,QWORD PTR [rdi+0xa] 100. 00 :    400e07:    mov  rax,QWORD PTR [rdi+rsi*1]  &lt;==  0. 00 :    400e0b:    mov  rax,QWORD PTR [rdi+rax*1]  0. 00 :    400e0f:    mov  rax,QWORD PTR [rdi+0x14]  0. 00 :    400e13:    xor  rax,rax  0. 00 :    400e16:    ret Just out of curiosity I collected the number of L3 misses: $ perf stat -e cpu/event=0xd1,umask=0x20,name=MEM_LOAD_RETIRED. L3_MISS/ . /a. out     71370594   MEM_LOAD_RETIRED. L3_MISS                  It shows that 7 out of each 10 iterations had loads that missed in L3. Now that we know what instructions caused so many L3 misses let’s fix it. Fixing the issue: Let’s look at the code: extern  C  { void foo(char* a, int n); }const int _200MB = 1024*1024*200;int main() { char* a = (char*)malloc(_200MB); // 200 MB buffer . . .  for (int i = 0; i &lt; 100000000; i++) {  int random_int = distribution(generator);  foo(a, random_int); } . . . }I allocate a big enough array to make it not fit in the L3 cache (L3 cache on the machine I was using is 38,5 MB - Intel(R) Xeon(R) Platinum 8180 CPU). Inside foo function (written in assembly in order to avoid compiler optimizations) I’m reading random memory location: foo:One_KB_of_nops		# emulate some irrelevant workmov   rax, QWORD [rdi + 0]  # constant loadmov   rax, QWORD [rdi + 10] # constant loadmov   rax, QWORD [rdi + rsi] # load that goes to DRAMmov   rax, QWORD [rdi + rax] # introduce dependency chainmov   rax, QWORD [rdi + 20] # constant loadYou probably already guessed what we should do. Yes, it’s prefetching. Because there is some significant time between the moment we get the next address we will read and actual load instruction, we can add prefetch hint (more details about __builtin_prefetch here):  for (int i = 0; i &lt; 100000000; i++) {  int random_int = distribution(generator);+  __builtin_prefetch ( a + random_int, 0, 1);  foo(a, random_int); }Code samples can be found on my github. This hint improved execution time by 2 seconds (+30% speedup):    24621931288   cycles                               2069238765   CYCLE_ACTIVITY. STALLS_L3_MISS                       8889566   MEM_LOAD_RETIRED. L3_MISS                      6,498080824 seconds time elapsedNotice 10x less values for CYCLE_ACTIVITY. STALLS_L3_MISS and MEM_LOAD_RETIRED. L3_MISS. However, it didn’t fully go away. There is one technique based on using LBR that can help us to estimate our prefetch window. If there will be interest in it I can write additional post about it. Just leave a comment if you interested. UPD 3rd April 2019 I wrote this article: Precise timing of machine code with Linux perf. Remember that TMAM is an iterative process, so we now need to repeat the process from the step #1. Likely it will move the bottleneck into some other bucket, probably Retiring. Ideally we want to be 100% bound by Retirement. Most of the time that means good thing, however not always. There are situations when you have very high retirement, but still app performs slow. This usually happens when Microcode sequencer starts feeding uops to the pipeline, like shown here. Additional resources and links::  Recording of the presentation made by the author of TMAM.  Andi Kleen’s blog - pmu-tools, part II: toplev.  Toplev manual - very nice description of TMAM.  Intel® VTune™ Amplifier has built-in support ofr TMAM - Understanding How General Exploration Works in Intel® VTune™ Amplifier"
    }, {
    "id": 13,
    "url": "https://dendibakh.github.io/blog/2019/02/02/Performance-optimization-contest",
    "title": "How good of a performance optimizer you are? Contest!",
    "body": "2019/02/02 - Contents:  Q&amp;A How to get started? Hints What’s NOT allowed List of contest editionsHello everybody! I decided to try new thing at the beginning of 2019. You probably heard about competitive programming and about popular web-sites like Topcoder, Codility and others. The idea is that you can try yourself in solving programming puzzles. Usually it’s not enough to just solve the puzzle, your solution must qualify for certain criterias such as algorithmic complexity and memory usage. Also on a real challenges it is improtant how fast you solved the problem. Those sort of challenges usually test your problem solving skills as well as your knowledge of algorithms and data structures. It is a kind of tasks we usually see on programming job interviews. You need to write the code from scratch to a complete solution. What I am starting is also a form of contest but aims at developing different set of skills. We will train to optimize already optimized code. We will learn hardware optimizations. This is briefly how it will look like: I will pick one existing benchmark of my choice and will send it to all my subscribers. You will have the time to play with the benchmark and find all performance headrooms you can find. You then send all your findings to me (modified sources/assemblies/compiler patches?/whatever speeds up the benchmark) and I check it and run on my hardware. At the end I will anounce the winners. Before I explain it in more details I feel a need of writing disclaimer:  Disclaimer: This is absolutely non-profit effort. I’m not looking for make any money from it or using someone’s knowledge in my own purposes. I will not use submissions to extract any intellectual property they might have. Also this is not aiming to advertise any particular software product including benchmarks/compilers. That’s why all the benchmarks and compilers I will use require to be open sourced. Neither it aims to advertise any particular HW. In this contest I try to emulate the situation in performance critical projects on the final stages. When all the functionality is delivered and tested. But before shipping the binary to the customers you were asked to tune the app to it’s peak performance. You know the hardware it will be deployed to. You have the sources in your hands and freedom to modify it (without introducing any bugs :) ). You are the build master, so you can add any compiler flags you want. For the challenges that I will send I’m not looking for optimizations that fall into the category: “Oh, I just used quicksort instead of bubblesort”. Expect the benchmark to be already optimized to some degree. Your task is to tune it for particular hardware to the peak performance. Q&amp;A: Q0: Why the hell on earth I should participate in this contest? A0: You will learn/practice how to do optimizations for HW. This might include eliminating cache misses by inserting prefetch instructions, getting rid of Code alignment issues, improving performance by helping compiler to vectorize/unroll the loop better. You will learn different techniques as you go. Q1: What benchmarks are taken into the contest? A1: It will be open sourced benchmark written in C/C++. Usually several source files. It should be easy to build, require minimal dependencies. Preferably it should have some form of validation. Q2: What is the machine/environment we will be optimizing for? A2: Most likely it will be 64 bit Linux with Intel x86 CPU (probably Haswell architecture). For a start I will not bother with disabling CPU dynamic frequency scaling features or setting thread affinity. I might do this in future. Q3: What if I don’t have the environment you are using? A3: It doesn’t matter much. If you don’t have Intel CPU or you are on Windows/Mac, just optimize for whatever you have. I would be happy to know about optimizations that help other CPUs, operating systems, etc. I don’t have real prizes to give, so it’s mostly practicing and learning. Q4: How I should find performance headrooms? A4: Use all of your knowledge. Start with profiling the benchmark. You can browse through posts on this blog. Additionally I will write a separate post that might help begginers. Q5: What optimizations are allowed? A5: Good news! All the dirty tricks allowed! The goal of this contest is to learn how to squeeze as much performance as possible from the hardware using any means available. You can modify sources and insert any compiler hints like pragmas, builtins, function attributes, etc. Also you can generate assembly listing (-S compiler option) and modify it. Finally, you can add some compiler option that might speed up the benchmark. Q6: How should the submission look like? A6: I will not accept binaries for security reasonsYou can send patch files that I can apply to the sources of the benchmark or just assembly listing. If you send assembly listing files please do also include diff file from the baseline (what you changed in the assembly). If you provide modified assembly listings it should be genearted only with open-sourced C/C++ compilers like gcc and clang. You can of course cheat and generate assembly with some other compiler that is better for the benchmark, but I will probably easily detect that. And it’s not about tricking, it’s about learning. If you are capable of hacking compiler that’s also acceptable. You can send me patches for gcc/llvm compilers which I can apply and use for building the benchmark. Please use the top of tree revisions because it will be easier for me to apply them. We can then use it for improving our open source compilers. Also it would be very nice if you can send me textual description of all the optimizations you made. Q7: How I will test your solutions? A7: I will take your sources, build them on my machine and run the benchmark. I will run your binary multiple times (depends on the running time) and take the minimum. I’m thinking about testing all the solutions on some cloud machine, but that’s not settled yet. Q8: How I will select the winner? A8: I will calculate your score as a ratio between execution time of the binary with your optimizations and the baseline. I know there is a lot more concerns you might have. This is just a first attempt with focus on learning how to do HW optimizations. In the end I don’t have real prizes to give out. :) After each contest I will share all the findings people did, so there is a big opportunity to learn from others! I know that a number of really experienced guys read this blog, so I encourage everyone to participate. Everyone is welcome! All communication (including sending benchmarks and score submissions) will happen through emails, so make sure to subscribe using the form at the bottom of the page! I am planning to start first contest in the end of February 2019. Let me know what you think about it or if you have any ideas or comments. You can also vote if you like it using the buttons below. How to get started?:  Collect the baseline (use time or analogs).  Find the hotspot (use perf record).  Find performance headroom     Take a look at the assembly and try to guess how you can do better.    Run through TMAM process.     Build the benchmark, run it and compare against baseline. Hints:  Do not try to understand the whole benchmark. For some people (including me) it’s crucial to understand how every peace of code works. For the purposes of optimizing it will be wasted effort. There are CPU benchmarks with thousands LOC (like SPEC2017) it’s absoultely impossible to understand them in a reasonable time. What you need to familiarize yourself with are hotspots. That’s it. You most likely need to understand one function/loop which is not more than 100 LOC.  You have specific workload for which you optimize the benchmark. You don’t need to optimize it for any other input/workload. The main principle behind Data-oriented design is that you know the data of your application. Information presented in llvm documentation: Benchmarking tips migth also be helpful. What’s NOT allowed:  Do not rewrite the benchmark completely or introduce major changes in algorithms.  Do not manually parallelize the benchmark, e. g converting it from single- to multi-threaded or offload computations to the GPU. I mean, I’m glad that you can do it and I will be happy to take a look what you did, but it’s not the intent of the contest.  Using PGO is allowed, however you can use it only for driving you optimizations, not for the submission. So, you can check how the benchmark gets better with PGO and understand why. And then make this optimization manually. Again, the purposes is practicing and learning. List of contest editions:  Contest #1 - 16 Feb 2019. "
    }, {
    "id": 14,
    "url": "https://dendibakh.github.io/blog/2018/12/29/Understanding-IDQ_UOPS_NOT_DELIVERED",
    "title": "Understanding IDQ_UOPS_NOT_DELIVERED performance counter.",
    "body": "2018/12/29 - Contents:  Mental model Example 1 Example 2 ConclusionIt is very important to be able to characterize the application to understand it’s bottlenecks and how to improve it. By characterizing I mean tell whether we are bound by CPU front-end (we can’t fetch and decode instructions efficiently), memory (we have lots of data cache misses) or we are compute bound (say, we have a lot of expensive divisions). Modern tools are able to do this automatically thanks to particular performance monitoring counters (PMC). PMC that we will discuss today plays very important role in top-down analysis (Intel® 64 and IA-32 Architectures Optimization Reference Manual, Appendix B. 1). In this metodology we try to detect what was stalling our execution starting from the high-level components (like Front End, Back End, Retiring, Branch predictor) and narrowing down the source of performance inefficiencies. If you’ll take a look at TMA metrics which is the heart of this metodology, first of all you’ll find that it looks very scary. :) But if you’ll look at the very first metric Frontend_Bound you’ll find formula which is used for calculating it: Frontend_Bound = IDQ_UOPS_NOT_DELIVERED. CORE / SLOTS,where SLOTS = Pipeline_Width * CPU_CLK_UNHALTED. THREAD,where Pipeline_Width = 4 // for most of modern Intel architerturesOfficial description in this document says that Frontend_Bound “represents slots fraction where the processor’s Frontend undersupplies its Backend”. A little bit mouthful, but in simple words we can describe it as “how good we are at fetching, decoding instructions and feeding them to the rest of the pipeline”. Also mouthful, but anyway… Mental model: In order to understand what this PMC counts let’s look at a simplified diagram: Here is the workflow:  We fetch assembly instruction and feed it into the decoders.  Decoders decode assembly instruction into the sequence of uops. It can be one or many uops depending on the instruction.  Then scheduler sends them into the back-end for execution. Because we have multiple execution units we can execute multiple uops in parallel. Most of modern Intel CPUs are 4-wide, meaning that we can schedule 4 uops each cycle. But it usually happens that due to some hazards (data dependency/execution unit occupied/lack of uops to schedule) we can’t fully utilize all available slots and, say, issue only 3 uops. We can have a counter that will keep track of how many slots we failed to utilize. So if, let’s say, we deliver only 1 uop this counter will be increased by 3 (it’s basically 4 – 1 is 3). If we deliver 4 uops in one cycle it will not be increased because we did a good job of filling all available slots. As you already guessed the lower the value for this counter, the better. In reality CPU is much more complicated than this and there is IDQ (Instruction Decode Queue) and RAT (Resource Allocation Table). Our PMC IDQ_UOPS_NOT_DELIVERED. CORE counts the number of uops not delivered from IDQ to RAT and RAT is not stalled. I hope it will become more clear once you’ll go through examples (see below). I will not describe how IDQ and RAT interoperate because this goes beyond the topic of this article. For us it’s just important to understand that there is a “bridge” from CPUs front end to the back end and we have means to monitor it. Example 1: I made a simple example with “ideal” loop where in each cycle we utilize all 4 available slots: mov rdx, 1000000000. loop:inc rcxinc rsiinc rdidec rdxjnz . loopAll measurements were done on Skylake CPU. $ perf stat -e instructions,cycles,cpu/event=0x9c,umask=0x1,name=IDQ_UOPS_NOT_DELIVERED. CORE/ -- . /a. out Performance counter stats for '. /a. out':    5001750626   instructions       #  4,96 insn per cycle         1009211538   cycles                                 1429415   IDQ_UOPS_NOT_DELIVERED. CORE Code and build script are available on my github. Notice, our loop is 5 instructions/4 uops per iteration, because dec+jnz pair was MacroFused into a single uop. Also notice, amount of IDQ_UOPS_NOT_DELIVERED. CORE is negligible comparing to the number of cycles executed. Our Front-End boundness metric will be: Frontend_Bound = IDQ_UOPS_NOT_DELIVERED. CORE / (4 * cycles) = 1429415 / (4 * 1009211538) = 0. 03 %We can say that this case is ideal from the Front End bound point of view. Everything fits nicely in the pipeline and we are able to issue 4 uops each cycle. I also confirmed that by collecting LSD. CYCLES_ACTIVE and LSD. CYCLES_4_UOPS: $ perf stat -e cycles,cpu/event=0xA8,umask=0x01,cmask=0x1,name=LSD. CYCLES_ACTIVE/,cpu/event=0xA8,umask=0x01,cmask=0x4,name=LSD. CYCLES_4_UOPS/ -- . /a. out Performance counter stats for '. /a. out':    1018747105   cycles                                994491346   LSD. CYCLES_ACTIVE                          994490536   LSD. CYCLES_4_UOPS This tells us that LSD delivered 4 uops every cycle. It may look easy when things get ideal but it becomes quite complicated to analyze when there are stalls in the pipeline. Let’s look at such example. Example 2: The dummy loop of 2 instructions/1 uop per iteration (again, dec+jnz pair was MacroFused into a single uop): mov rdx, 1000000000. loop:dec rdxjnz . loop$ perf stat -e instructions,cycles,cpu/event=0x9c,umask=0x1,name=IDQ_UOPS_NOT_DELIVERED. CORE/ -- . /a. out Performance counter stats for '. /a. out':    2001858013   instructions       #  2,00 insn per cycle         1001933752   cycles                               1012451532   IDQ_UOPS_NOT_DELIVERED. CORE According to our mental model we should be wasting 3 slots to deliver uops to RAT each cycle. We have 10^9 cycles, so the number for IDQ_UOPS_NOT_DELIVERED. CORE should be somewhere around 3 * 10^9, but it’s not. It’s much less in this case because the RAT itself was stalled (remember, IDQ_UOPS_NOT_DELIVERED. CORE only gets increased when the backend is requesting uops) during some amount of cycles and was not able to take uops: Performance counter stats for '. /a. out':    1007893869   cycles                                505674623   UOPS_ISSUED. STALL_CYCLES                       503135956   LSD. CYCLES_ACTIVE                            232128   LSD. CYCLES_4_UOPSWe can see that half of the time RAT was stalled (likely because it was full) and another half of the time LSD was active and delivered some amount of uops. We can already guess that number because we know how much total slots we waisted (IDQ_UOPS_NOT_DELIVERED. CORE). Each second cycle LSD delivered 2 uops which also means that each second cycle IDQ_UOPS_NOT_DELIVERED. CORE was increased by 2. Given that the number of cycles the backend was requesting uops is 5 * 10^8 and we waisted 2 slots on each of them, we confirmed the number for IDQ_UOPS_NOT_DELIVERED. CORE (10^9). I must say that I expected to see one uop delivered each cycle instead of 2 uops each second cycle. I’m not entirely sure why that’s the case. It proves that real CPU design is much more complicated than my mental model. :) UPD: Travis Downs in the comments provided his measurements when LSD is disabled. There he shows that if the loops is served out of DSB we have 3 “uops not delivered” per cycle. See comments for more details. Another way to do this is to use subcounters of IDQ_UOPS_NOT_DELIVERED:  IDQ_UOPS_NOT_DELIVERED. CYCLES_0_UOP_DELIV. CORE - Cycles which 4 issue pipeline slots had no uop delivered from the front end to the back end when there is no back-end stall.  IDQ_UOPS_NOT_DELIVERED. CYCLES_LE_n_UOP_DELIV. CORE - Cycles which “4-n” issue pipeline slots had no uop delivered from the front end to the back end when there is no back-end stall.  IDQ_UOPS_NOT_DELIVERED. CYCLES_FE_WAS_OK - Cycles which front end delivered 4 uops or the RAT was stalling FE Performance counter stats for '. /a. out':    1002271977   cycles                            (83,33%)      286803   IDQ_UOPS_NOT_DELIVERED. CYCLES_0_UOP_DELIV. CORE                   (83,33%)      6248629   IDQ_UOPS_NOT_DELIVERED. CYCLES_LE_1_UOP_DELIV. CORE                   (83,33%)     503382522   IDQ_UOPS_NOT_DELIVERED. CYCLES_LE_2_UOP_DELIV. CORE                   (83,33%)     503531042   IDQ_UOPS_NOT_DELIVERED. CYCLES_LE_3_UOP_DELIV. CORE                   (83,33%)     500685038   IDQ_UOPS_NOT_DELIVERED. CYCLES_FE_WAS_OK                   (66,77%)Looking at those numbers we can say how much cycles we were delivering specific number of uops to the RAT (and there is no back-end stall): Cycles with 0 uop delivered = IDQ_UOPS_NOT_DELIVERED. CYCLES_0_UOP_DELIV. CORECycles with 1 uop delivered = IDQ_UOPS_NOT_DELIVERED. CYCLES_LE_1_UOP_DELIV. CORE - IDQ_UOPS_NOT_DELIVERED. CYCLES_0_UOP_DELIV. CORECycles with 2 uop delivered = IDQ_UOPS_NOT_DELIVERED. CYCLES_LE_2_UOP_DELIV. CORE - IDQ_UOPS_NOT_DELIVERED. CYCLES_1_UOP_DELIV. CORECycles with 3 uop delivered = IDQ_UOPS_NOT_DELIVERED. CYCLES_LE_3_UOP_DELIV. CORE - IDQ_UOPS_NOT_DELIVERED. CYCLES_2_UOP_DELIV. CORECycles with 4 uop delivered = IDQ_UOPS_NOT_DELIVERED. CYCLES_FE_WAS_OKIn our case it’s: Cycles with 0 uop delivered = 286803Cycles with 1 uop delivered = 5961826Cycles with 2 uop delivered = 497133893Cycles with 3 uop delivered = 148520Cycles with 4 uop delivered = 500685038If we sum up all those numbers we will receive the number of cycles spent. I’m pretty sure we haven’t delivered 4 uops not a single cycle, it’s just that the backend was stalled during that time, that’s why we have such a big number for IDQ_UOPS_NOT_DELIVERED. CYCLES_FE_WAS_OK. Conclusion: Here you have usefull tool which you can use to better understand performance issues you might have. It’s very interesting to see sometimes cases where just making small changes makes a huge impact on IDQ_UOPS_NOT_DELIVERED counters. For big applications it’s impossible to reason about the execution pattern of the benchmark just by looking at IDQ_UOPS_NOT_DELIVERED counters, but it might give you some interesting insights and further direction. When you made some changes in your application and want to analyze the effect, it’s sometimes fruitful to compare the absolute numbers for IDQ_UOPS_NOT_DELIVERED counters. However, there should be a strong difference, couple of percents do not count. For particular application of using this counter take a look at my post Code alignment issues. Nowadays, when I’m doing benchmarking I run the binaries by default under perf stat -e IDQ_UOPS_NOT_DELIVERED. CORE. Because it not only gives you the execution time, it also collects performance counters and it’s almost free (when there is no multiplexing between counters). "
    }, {
    "id": 15,
    "url": "https://dendibakh.github.io/blog/2018/11/08/Using-denormal-floats-is-slow-how-to-detect-it",
    "title": "Using denormal values is slow. How to detect it?",
    "body": "2018/11/08 - Contents:  Example Measurements ExplanationIn this short post I want to show the example how denormal values that you can use (unintentionally) in your calculations might slow down your code. And especially how to detect it using performance counters. If you’re not familiar with what denormal floats it’s now the good time to read it. Disclaimer: In this post I don’t touch the topic of how to disable denormal floats at the code/compiler level. There is lots of information in the web. Example: I put a division of two floats in a tight loop: int bench(volatile float x, volatile float y){ float sum = 0. 0f; for (int i = 0; i &lt; 100000000; i++) {  sum = x / y;  DoNotOptimize(sum);  sum = 0. 0f; } return (int)sum;}In first case I pass 2 normal floats and in second 2 denormal floats as arguments. Example of a denormal float would be 0xF and 0x7. Example of normal float would be 0. 1f and 0. 2f. You can check their binary representations and compare. Full code can be found on my github. I built everything with gcc -O1 and checked that we have a loop with division inside:  4009df:	c5 fa 10 4c 24 fc  	vmovss xmm1,DWORD PTR [rsp-0x4] 4009e5:	c5 fa 10 44 24 f8  	vmovss xmm0,DWORD PTR [rsp-0x8] 4009eb:	c5 f2 5e d0     	vdivss xmm2,xmm1,xmm0 4009ef:	c5 f9 7e d2     	vmovd edx,xmm2 4009f3:	83 e8 01       	sub  eax,0x1 4009f6:	75 e7        	jne  4009df &lt;_Z5benchff+0x11&gt;Measurements: Normal floats: $ perf stat -e cycles,cpu/event=0xc2,umask=0x2,name=UOPS_RETIRED. RETIRE_SLOTS/,cpu/event=0xca,umask=0x1e,cmask=0x1,name=FP_ASSIST. ANY/,cpu/event=0x79,umask=0x30,name=IDQ. MS_UOPS/ . /a. out normx isnormal: yesy isnormal: yes Performance counter stats for '. /a. out norm':     303078534   cycles                                502937703   UOPS_RETIRED. RETIRE_SLOTS                          0   FP_ASSIST. ANY                              808676   IDQ. MS_UOPS                            0,081426690 seconds time elapsedDenormal floats: $ perf stat -e cycles,cpu/event=0xc2,umask=0x2,name=UOPS_RETIRED. RETIRE_SLOTS/,cpu/event=0xca,umask=0x1e,cmask=0x1,name=FP_ASSIST. ANY/,cpu/event=0x79,umask=0x30,name=IDQ. MS_UOPS/ . /a. out denormx isnormal: noy isnormal: no Performance counter stats for '. /a. out denorm':    15720344436   cycles                               4721230495   UOPS_RETIRED. RETIRE_SLOTS                      100000000   FP_ASSIST. ANY                            4307771477   IDQ. MS_UOPS                            4,154192419 seconds time elapsedExplanation: First observation is that divisions on denormal values is 50 times slower. No surprise, but lets understand why that happens. Whenever CPU see that it’s processing denormal value it asks for a microcode assist. Microcode Sequencer (MS) then will feed the pipeline with lots of UOPs for handling that scenario. We can see that in the slow case we have exactly 100000000 fp assits from MS and in normal case it’s zero. Also we can spot that in the slow case major part of UOPs comes from MS. So, here are your tools in detecting situations when your programm starts doing calculation with denormal values. "
    }, {
    "id": 16,
    "url": "https://dendibakh.github.io/blog/2018/10/12/I-am-relocating-to-the-US",
    "title": "I am relocating to the US.",
    "body": "2018/10/12 - I’m about to start a new chapter in my career and life. I’m relocating to the US in a few days. Right now I want to say Thank you to Poland and say Hello to US. For those who know me: don’t worry, I’m not changing job. I’m still staying with Intel and will be doing basically the same things I do now. But there was a business need for a company to move my project from Poland to US. Thank you Poland!: I spent in Poland 4 amazing years and lived in 2 cities: Wroclaw and Gdansk. Wroclaw has very strong IT community and in general is a very beautiful modern city. Gdansk is located close to the sea, so that’s definitely an advantage! Polish people are very polite and kind, especially they like children. My younger daughter was born in Poland, so I know what I’m talking about :) . In general I can say that my adaptation in Poland was easy. Also Russian (my native) language is very similar to Polish, so I started to understand what people on street are talking about very quickly. I’ve been in many different countries, and I swear that for living I would prefer Poland to many other European countries. I hope to be in Poland again! It’s needless to say that I learned a lot!: I’ve spent first 2,5 years at Nokia working on embedded SW for radio frequency modules. Although it was embedded SW we used all the modern tools like git, cmake, gerrit and latest C++ standard (C++14 at that time, gcc 5. 2). But I always loved writing fast code and benchmarking it. I learned some of the stuff related to that on my own. For example, compiler optimizations and cpu architecture. Regardless it was required or not, I tried to benchmark the code that I was writing. So one opportunity came to me in this regard. It was a position in compiler development team at Intel which I later accepted. While being at Intel I really mastered my skills of performance analysis and compiler development. There are a lot of very smart people, which you can learn from.  Special thanks goes to code::dive team and the whole Nokia. For me it was an amazing opportunity to meet with world class experts and have a chat with them while drinking another glass of beer. :)  I was planning to give a talk this year and even submitted one, but my relocation broke those plans. On one of such code::dive’s I met cool guys from Bochum, which organize embo++ conference. Thank you crew, for accepting my talk and letting me to give it. Speaking of my accomplishments: I would like to mention my blog (which you are reading now). I started it in 2016 and since then I have 40'000 pageviews and 20'000 unique users, which I consider quite a good achievement. Every day I have around 30-40 pageviews and around 15 new users on my blog. All this gives me the energy to keep on writing new articles. Another thing I’m proud of is that I learned Polish language. I don’t feel uncomfortable speaking or writing Polish. I’m still very far from people who know 8-9 different languages, but I still consider this as an advantage that shows your divers knowledge. I’m at the beginning of new journey and I’m very excited about that! I can’t wait to say Hello! to the US, but for now I keep saying Thank you! to Poland! "
    }, {
    "id": 17,
    "url": "https://dendibakh.github.io/blog/2018/09/04/Performance-Analysis-Vocabulary",
    "title": "Performance analysis vocabulary.",
    "body": "2018/09/04 - Contents:  What is retired instruction? What’s an UOP(micro-op)? What is reference cycle? What is mispredicted branch? What is CPI &amp; IPC?For a beginer it can be a very hard time looking in a profile generated by the tool like perf or Intel Vtune Amplifier. It immediately throws at you lots of strange terms, which you might not know. Whenever I’m presenting to the audience or speaking with somebody who is not very much involved into performance analysis activities, they ask the same basic questions. Like: “What is instruction retired?” or “What is reference cycles?”. So, I decided to write an article describing some of unobvious terms connected with performance analysis. What is retired instruction?:  Modern processors execute much more instructions that the program flow needs. This is called a speculative execution. Instructions that were “proven” as indeed needed by the program execution flow are “retired”. Source: https://software. intel. com/en-us/vtune-amplifier-help-instructions-retired-event. So, instruction processed by the CPU can be executed but not necessary retired. And retired instruction is usually executed, except those times when it does not require an execution unit. An example of it can be mov elimination (see my post What optimizations you can expect from CPU?). Taking this into account we can usually expect the number of executed instructions to be higher than the number of retired instructions. There is a fixed performance counter (PMC) that is collecting this metric. See one of my previous articles for more information on this topic: PMU counters and profiling basics. To collect this basic metric, you can use perf: $ perf stat -e instructions . /a. outor just simply$ perf stat . /a. outWhat’s an UOP(micro-op)?: From Agner’s Fog microarchitecture manual, chapter 2. 1 “Instructions are split into µops”:  The microprocessors with out-of-order execution are translating all instructions into microoperations - abbreviated µops or uops. A simple instruction such as ADD EAX,EBX generates only one µop, while an instruction like ADD EAX,[MEM1] may generate two: one for reading from memory into a temporary (unnamed) register, and one for adding the contents of the temporary register to EAX. The instruction ADD [MEM1],EAX may generate three µops: one for reading from memory, one for adding, and one for writing the result back to memory. The advantage of this is that the µops can be executed out of order. In the chapter about micro-ops Agner has more examples, so you may want to read them as well. Modern Intel architectures are capable of collecting the number of issued, executed and retired uops. The difference between executed and retired uop is mostly the same as for instruction. $ perf stat -e cpu/event=0xe,umask=0x1,name=UOPS_ISSUED. ANY/,cpu/event=0xb1,umask=0x1,name=UOPS_EXECUTED. THREAD/,cpu/event=0xc2,umask=0x1,name=UOPS_RETIRED. ALL/ ls Performance counter stats for 'ls':      2856278   UOPS_ISSUED. ANY                            2720241   UOPS_EXECUTED. THREAD                          2557884   UOPS_RETIRED. ALLUops also can be MacroFused and MicroFused. What is reference cycle?: Majority of modern CPUs including Intel’s and AMD’s ones don’t have fixed frequency on which they operate. Instead, they have dynamic frequency scaling. In Intel’s CPUs this technology is called Turbo Boost, in AMD’s processors it’s called Turbo Core. There is nice explanation of the term “reference cycles” on this stackoverflow thread:  Having a snippet A to run in 100 core clocks and a snippet B in 200 core clocks means that B is slower in general (it takes double the work), but not necessarily that B took more time than A since the units are different. That’s where the reference clock comes into play - it is uniform. If snippet A runs in 100 ref clocks and snippet B runs in 200 ref clocks then B really took more time than A. $ perf stat -e cycles,ref-cycles . /a. out Performance counter stats for '. /a. out':    43340884632   cycles		# 3. 97 GHz    37028245322   ref-cycles	# 3. 39 GHz   10,899462364 seconds time elapsedI did this experiment on Skylake i7-6000 process, which base frequency is 3. 4 GHz. So, ref-cycles event counts cycles as if there were no frequency scaling. This also matches with clock multiplier for that processor, which can find in the specs (it’s equal to 34). Usually system clock has frequency of 100 MHz, and if we multiply it by clock multiplier we will receive the base frequency of the processor. You also might be interested to read about Overclocking. One interesting experiment which I suggest to do on your own looks like this: open 3 terminals and run corresponding commands: 1. perf stat -e cycles -a -I 10002. perf stat -e ref-cycles -a -I 10003. perf stat -e bus-cycles -a -I 1000Place them in such a way that all 3 will be visible. Then open another terminal in which start executing some workload. You will notice how collected values will increase and decrease over time. This experiment will also give you an idea how the state of the CPU is changing. For advanced information about reference cycles please check this thread on Intel forum. What is mispredicted branch?: Modern CPUs try to predict the outcome of a branch instruction (taken or not taken). For example, when processor see a code like that: dec eaxjz . zero# eax is not 0. . . zero:# eax is 0Instruction jz is a branch instruction and in order to increase performance modern CPU architectures try to predict the result of such branch. This is also called speculative execution. Processor will speculate that, for example, branch will not be taken and will execute the code that corresponds to the situation when eax is not 0. However, if the guess was wrong, this is called “branch misprediction” and CPU is required to undo all the speculative work that it has done lately. This may involve something between 10 and 20 clock cycles. You can check how much branch mispredictions there were in the workload by using perf: $ perf stat -e branches,branch-misses ls Performance counter stats for 'ls':      358209   branches                                 14026   branch-misses       #  3,92% of all branches        0,009579852 seconds time elapsedor simply$ perf stat lsMore information like history, possible and real world implementations and more can be found on wikipedia and in Agner’s Fog microarchitecture manual, chapter 3 “Branch prediction”. What is CPI &amp; IPC?: Those two are derivative metrics that stand for:  CPI - Cycles Per Instruction (how much cycles it took to execute one instruction on the average?) IPC - Instructions Per Cycle (how much instructions were retired per one cycle on the average?)There are lots of other analysis that can be done based on those metrics. But in the nutshell, you want to have low CPI and high IPC. Formulas: IPC = INST_RETIRED. ANY / CPU_CLK_UNHALTED. THREADCPI = 1 / IPCLet’s look at the example: $ perf stat -e cycles,instructions ls Performance counter stats for 'ls':      2369632   cycles                                 1725916   instructions       #  0,73 insn per cycle        0,001014339 seconds time elapsedNotice, perf tool automatically calculates IPC metric for us. "
    }, {
    "id": 18,
    "url": "https://dendibakh.github.io/blog/2018/08/29/Understanding-performance-events-skid",
    "title": "Understanding performance events skid.",
    "body": "2018/08/29 - Contents:  Example of skid What we can do about it?In my previous article I promised to write about retired instructions, reference cycles and other stuff. But one of my readers asked me why he sometimes sees an instruction tagged by an event which was not caused by this instruction? So, today I want to discuss this very important concept, which I think is crucial to understand. Performance analysis is a hard thing, no question about it. But it becomes even harder when the profile data that you are looking at misleads you. Imagine you have application with big amount of L1D-cache misses and the hot assembly code that look like this: ; load1 ; load2; load3 &lt;-- here profile shows you lots of L1D-cache missesThis is great, but in reality load1 is the instruction that causes L1D-cache misses. Ugghf! The skid is defined as the distance between the IP(s) that caused the issue to the IP(s) where the event is tagged Example of skid: To demonstrate the thing I wrote a small test in assembly that is available on my github. Inside it I have a simple loop: ; there will be 100'000'000 iterations. loop:; cache_line of 8-byte NOPs; cache_line of 8-byte NOPs; cache_line of 8-byte NOPs; cache_line of 8-byte NOPsdec rdijnz . loopFor the purpose of the experiment we don’t need to have real assembly instructions. We will emulate the workload with NOPs. In my experiment we will sample on the event branches (BR_INST_RETIRED. ALL_BRANCHES) and expect all such events to correspond to jnz . loop instruction. I made all experiments on Haswell CPU: $ perf stat -e cpu/event=0xc4,umask=0x4,name=BR_INST_RETIRED. ALL_BRANCHES/ . /a. out Performance counter stats for '. /a. out':     100338645   BR_INST_RETIRED. ALL_BRANCHES                     0,301300877 seconds time elapsedTotal number of branches retired is close to 100'000'000 (1 branch per iteration). Now let’s sample on it: $ perf record -e cpu/event=0xc4,umask=0x4,name=BR_INST_RETIRED. ALL_BRANCHES/ . /a. out[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0. 062 MB perf. data (1176 samples) ]$ perf annotate --stdio -M intel main. loop Percent |   Source code &amp; Disassembly of a. out for BR_INST_RETIRED. ALL_BRANCHES (1170 samples)--------------------------------------------------------------------------------------------------     :     :     :     :   Disassembly of section . text:     :     :   000000000040057e &lt;main. loop&gt;:     :   main. loop():  0. 00 :    40057e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400586:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40058e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400596:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40059e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005a6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005ae:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005b6:    nop  DWORD PTR [rax+rax*1+0x0] 100. 00 :    4005be:    nop  DWORD PTR [rax+rax*1+0x0]	&lt;-- OOOPS, we have skid of ~10 instructions!  0. 00 :    4005c6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005ce:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005d6:    nop  DWORD PTR [rax+rax*1+0x0]	  0. 00 :    4005de:    nop  DWORD PTR [rax+rax*1+0x0]	&lt;-- This insruction is tagged on Ivy Bridge CPU.   0. 00 :    4005e6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005ee:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005f6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005fe:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400606:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40060e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400616:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40061e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400626:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40062e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400636:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40063e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400646:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40064e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400656:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40065e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400666:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40066e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400676:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40067e:    dec  rdi  0. 00 :    400681:    jne  40057e &lt;main. loop&gt;		&lt;-- This instruction should be tagged.   0. 00 :    400687:    add  rsp,0x4  0. 00 :    40068b:    pop  rdi  0. 00 :    40068c:    pop  rax  0. 00 :    40068d:    ret    0. 00 :    40068e:    ud2 Notice, that all collected samples correspond to the wrong instruction! To understand why that happens you might want to check one of my previous articles: Advanced profiling topics PEBS and LBR. But if you want to know the short answer, there is a delay between performance monitoring interrupt issued and capture of instruction pointer (IP). What we can do about it?: Skid makes it difficult to discover the instruction which is actually causing the performance issue. But fortunately, there is a special mechanism called PEBS (Precise Event-Based Sampling) which is dedicated to solve the problem. More on this topic I wrote in already mentioned blog post. Here is how the things changed when using it (notice pp suffix in the event declaration): $ perf record -e cpu/event=0xc4,umask=0x4,name=BR_INST_RETIRED. ALL_BRANCHES/pp . /a. out[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0. 064 MB perf. data (1245 samples) ]$ perf annotate --stdio -M intel main. loop Percent |   Source code &amp; Disassembly of a. out for BR_INST_RETIRED. ALL_BRANCHES (1237 samples)--------------------------------------------------------------------------------------------------     :     :     :     :   Disassembly of section . text:     :     :   000000000040057e &lt;main. loop&gt;:     :   main. loop():  0. 00 :    40057e:    nop  DWORD PTR [rax+rax*1+0x0] &lt;-- This instruction is tagged for SNB families.   0. 00 :    400586:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40058e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400596:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40059e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005a6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005ae:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005b6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005be:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005c6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005ce:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005d6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005de:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005e6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005ee:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005f6:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    4005fe:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400606:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40060e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400616:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40061e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400626:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40062e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400636:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40063e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400646:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40064e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400656:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40065e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400666:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40066e:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    400676:    nop  DWORD PTR [rax+rax*1+0x0]  0. 00 :    40067e:    dec  rdi 100. 00 :    400681:    jne  40057e &lt;main. loop&gt;	 &lt;-- Indeed, this is the instruction, we were looking for.   0. 00 :    400687:    add  rsp,0x4  0. 00 :    40068b:    pop  rdi  0. 00 :    40068c:    pop  rax  0. 00 :    40068d:    ret    0. 00 :    40068e:    ud2Now we have clear picture of the event that we sample on and the instruction that caused it. However, keep in mind, that only since Haswell precise events tag the same instruction, but on SandyBridge processor family they skid to the next IP (see the listing above). Precise events is a very important tool for top-down analysis (look at Appendix B. 1). It’s idea that you first identify the source of the performance problems. You basically do this by counting a lot of events at once (not necessary precise). You understand what is causing problems and than you locate the exact place in the code using precise event. For more details and examples take a look at TMA metrics. "
    }, {
    "id": 19,
    "url": "https://dendibakh.github.io/blog/2018/08/26/Basics-of-profiling-with-perf",
    "title": "Basics of profiling with perf.",
    "body": "2018/08/26 - In this post I want to go back to the basic things of profiling with perf. I want to show what’s happening when you type perf record. We all know that it somehow shows us the hotspots and where our application spend most of the time. That’s great, but how it’s doing it? Let’s find out. Before reading this post I suggest you to familiarize yourself with my two previous posts about PMU counters and profiling basics and Advanced profiling topics. PEBS and LBR. Especially with what is counting and sampling. Suppose we have our application “a. out” which runs for approximately 2,5 seconds: $ time -p . /a. outreal 2. 67user 2. 48Let’s run perf record on it: $ perf record . /a. out[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0. 110 MB perf. data (2451 samples) ] We have 2451 samples, that’s 1 sample per millisecond. And that’s a default behaviour: the perf tool defaults the frequency to 1000Hz, or 1000 samples/sec. It’s also equivalent to run perf record -F 1000. Perf will stop our program 1000 times per second and see where the IP (instruction pointer) is. So, if we don’t want that accuracy, we can choose a lower frequency: $ perf record -F 100 . /a_out[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0. 026 MB perf. data (247 samples) ] // 1 sample per 10 millisecondsBut the interesting thing is that perf doesn’t just stop your application after equal time intervals. If it would be so, there will be no difference for sampling on various events. Would we sample on cycles or instructions, there will be no difference, as perf will still stop the app after equal time intervals. To understand what is it doing, we’ll take a look inside perf. data. If we do so we will be able to see raw samples: $ perf report -D. . . 9253562614198937 0x4d18 [0x28]: PERF_RECORD_SAMPLE(IP, 0x2): 20531/20531: 0x40090b period: 32287405 addr: 0 . . . thread: a_out:20531 . . . . . . dso: . /a_out0x4d40 [0x28]: event: 9. . . . . raw event: size 40 bytes.  0000: 09 00 00 00 02 00 28 00 15 0a 40 00 00 00 00 00 . . . . . . (. . . @. . . . . .  0010: 33 50 00 00 33 50 00 00 21 99 1e f1 10 e0 20 00 3P. . 3P. . !. . . . . . .  0020: 3c 65 ee 01 00 00 00 00             &lt;e. . . . . .     This is just one out of many samples collected during the whole runtime. First thing we’ll take a look at is 0x40090b. It is the instruction address on which this sample was collected. At the time when sample was captured, IP (instruction pointer) was set to this instruction. If we grep all the samples by this address: $ perf report -D | grep 0x40090b -c16Which matches with what we see in perf report:    │   0000000000400906 &lt;foo. loop&gt;:    │   foo. loop():  136 │400906:  mov  $0x0,%eax  16 │40090b:  dec  %rsi    │40090e: ↑ jne  400906 &lt;foo. loop&gt;  22 │400910: ↓ jmpq  400a15 &lt;foo. merge&gt;The second interesting thing is period: 32287405 the number of occurrences of the event between two samples. Here things start to get interesting. So, between sample N-1 and N (that’s presented) there were 32287405 cycles executed. Perf, when preparing for capturing next sample, set the value of one of the PMU counters to -3228740, then start incrementing it with every cycle (because we sample on cycles) and wait until it overflows (from -1 to 0). You can read more about this in my article about PMU counters and profiling basics. Now, remember that by default we sample on cycles (equivalent to perf record -e cycles). With latest run we collected 247 samples. For simplicity let’s assume average period for all samples is 32300000 events. Based on that, the number of cycles it took to execute this workload is: 247 * 32300000 = 7978100000 cycles. If we compare this number with the number of counted cycles: $ perf stat -e cycles . /a_out                                                      Performance counter stats for '. /a_out':    7805574851   cycles                               2,398101184 seconds time elapsedWe see that our calculated number 7978100000 is not that far off from the measured 7805574851. We can do the same experiment with branch-misses: $ perf record -F 1000 -e branch-misses . /a_out                                                    [ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0. 109 MB perf. data (2417 samples) ]$ perfRep -D | grep period9254712117721275 0x1a758 [0x28]: PERF_RECORD_SAMPLE(IP, 0x2): 21133/21133: 0x40051c period: 55754 addr: 09254712118718533 0x1a780 [0x28]: PERF_RECORD_SAMPLE(IP, 0x2): 21133/21133: 0x40051c period: 55804 addr: 0$ perf stat -e branch-misses . /a_out Performance counter stats for '. /a_out':     133366825   branch-misses                           2,406486488 seconds time elapsedHere we have 2417 (samples collected) * 55804 (period for each sample) = 134757418 (total branch-misses). Which again is not that far off from the measured value. The opposite of setting frequency of collecting samples is to configure period: $ perf record -e instructions -c 1000000 . /a_out[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0. 436 MB perf. data (13731 samples) ]$ perf stat -e instructions . /a_out                                                            Performance counter stats for '. /a_out':    13706955042   instructions                            2,443039456 seconds time elapsedHere we have 13731 (samples collected) * 1000000 (fixed number of retired instructions between samples) = 13731000000 (total number of instructions). Again the diviation from measured number of instructions retired (13706955042) is pretty small. That’s all for today. I’m preparing another beginner’s post about basic terms in performance analysis, such as what’s a retired instruction and how it’s different from executed instruction. What is the difference between cycles and reference cycle. What is uop (micro-op), CPI/IPC, instruction latency and throughput. So, stay tuned! "
    }, {
    "id": 20,
    "url": "https://dendibakh.github.io/blog/2018/07/09/Improving-performance-by-better-code-locality",
    "title": "Improving performance by better code locality.",
    "body": "2018/07/09 - Contents:  Keep the cold code as far as you can Why latter case is better? Enough theory, show me the benchmark Compiler heuristics Built-in expect and attributes for inlining PGO (profile-guided optimizations)Data locality is a known problem and there are lots of information written on that topic. Most of modern CPUs have caches, so it’s best to keep the data that we access most frequently in one place (spatial locality). The other side of this problem is not to work on a huge chunk of memory in a given time interval, but work on a small pieces (temporal locality). The most known example of this kind is matrix traversal. And I hope that by now there are no developers who do matrix traversal by columns. Similar rules apply to the machine code: if we will do frequent long jumps - it won’t be very I-cache efficient. Today I will show one typical example of when it can make a difference. Without further ado let’s jump to the core of the article. Keep the cold code as far as you can: Let’s suppose we have a function like that: void foo(bool criticalFailure, int iter){ if (criticalFailure) {  // error handling code } else {  // hot code }}Let’s suppose and error handling function is quite big (several I-cache lines) and it was inlined, which brought the code from it to the body of foo. As we know, the code is always layed out sequentially in memory. If we disassemble our foo function we might see something like this: ; I stripped all the offsets and other not important stuff&lt;foo&gt;:cmp rdi, 0jz . hot; error handling codejmp . end; hot code. end:retIf we vizualize it we will see the picture how our hot code is layed out in memory: On the picture above I highlithed typical hot path over foo function with yellow and cold path with blue. You can clearly see, that we make one long jump from the block “if (criticalFailure)” to “hot path”. Without justification (for now) let’s take a look at another way of placing the blocks inside the function: Why latter case is better?: (updated, thanks to comments by Travis) There are a number of reasons for the second case to perform better:    If we would have sequential hot path, it will be better for our I-cache. The code that will be executed will be prefetched before CPU will start executing it. It is not always the case for the original block placement. In the presented example it doesn’t make significant impact.     It makes better use of the instruction and uop-cache: with all hot code contiguous, you don’t get cache line fragmentation: all the cache lines in the i-cache are used by hot code. This same is true for the uop-cache since it cached based on the underlying code layout as well.     Taken branches are fundamentally more expensive that untaken: recent Intel CPUs can handle 2 untaken branches per cycle, but only 0. 5 taken branches per cycle in the general case (there is a special small loop optimization that allows very small loops to have 1 taken branch/cycle). They are also more expensive for the fetch unit: the fetch unit fetches contiguous chunks of 16 bytes, so every taken jump means the bytes after the jump are useless and reduces the maximum effective fetch throughput. Same for the uop cache: cache lines from only one 32B region (64B on Skylake, I think) are accessed per cycle, so jumps reduce the max delivery rate from the uop cache.  So, how we can make the second case happen? Well, there are 2 issues to fix: error handling function was inlined into the body of foo, hot code was not placed in a fall through position. Enough theory, show me the benchmark: I wrote a small benchmark to demonstrate the thing and show you the numbers. Below you can find two assembly functions that I benchmarked (written in pure assembly). My hot code only consists of NOPs (instead of real assembly instructoins), but it doesn’t affect the measurements. Benchmark is not doing any useful work, just simulates the real workload. But again, it’s enough to show what I wanted to show. Complete code of the benchmark can be found on my github. Scripts for building and running the benchmark included. Note, that in order to build it you need to build nasm assembler. // a_jmp (not efficient code placement)		 // a_fall (improved code placement)foo:                       foo:                         ; some hot code (4 I-cache lines)         ; some hot code (4 I-cache lines)                         cmp rdi, 0                    cmp rdi, 0jz . hot                      jnz . cold                         ; error handling code (4 I-cache lines)      . hot:                         dec rsijmp . end                     jnz . hot                         . hot:                       . end:dec rsi                      retjnz . hot                                              . cold:. end:                       call err_handler                         jmp . endret                                                 err_handler:                         ; error handling code (4 I-cache lines)                         retAnd here is how I’m calling them: extern  C  { void foo(int, int); }int main(){ for (int i = 0; i &lt; 1000000000; i++)  foo(0, 32); return 0;}Now let’s run them. My measurements are for Skylake, but I think it holds for most modern architectures. I measured hardware events in several separated runs. Here are the results: $ perf stat -e &lt;events&gt; -- . /a_jmp Performance counter stats for '. /a_jmp':   124623459202   r53019c			 # IDQ_UOPS_NOT_DELIVERED. CORE   105451915136   instructions       # 1,62 insn per cycle    64987538427   cycles      1293787   L1-icache-load-misses     1000146958   branch-misses           1259211   DSB2MITE_SWITCHES. PENALTY_CYCLES                     38001539159   IDQ. DSB_CYCLES    68002930233   IDQ. DSB_UOPS   16,346708137 seconds time elapsed$ perf stat -e &lt;events&gt; -- . /a_fall Performance counter stats for '. /a_fall':   109388366740   r53019c			 # IDQ_UOPS_NOT_DELIVERED. CORE   105443845060   instructions       # 1,92 insn per cycle    55019003815   cycles      825560   L1-icache-load-misses     33546707   branch-misses         648816   DSB2MITE_SWITCHES. PENALTY_CYCLES                     41742394288   IDQ. DSB_CYCLES                           71971516976   IDQ. DSB_UOPS                       13,821951438 seconds time elapsedOverall performance improved by ~15%, which is pretty attractive. UPD: Travis (in the comments) showed me that it was the edge case for branch misprediction. You can clearly see that in the good case we have 30 times (!) less branch mispredictions. Additionally, we see that by reordering basic blocks we have 36% less I-cache misses (but it’s impact is miscroscopic), and DSB coverage is 6% better (calculated from IDQ. DSB_UOPS, but not precisly). Overall, “Front-end bound” metric decreased by 12% (calculated from IDQ_UOPS_NOT_DELIVERED. CORE).  Disclaimer: From my experience, this doesn’t usually give impressive boost in performance. I usually see around 1-2%%, so don’t expect miracles from this optimization. See more information in PGO section. So, we can make two improtant points from this benchmark:  Don’t inline the cold functions.  Put hot code in a fall through position. Compiler heuristics: Compilers also try to make use of this and thus introduced heuristics for better block placement. I’m not sure they are documented anywhere, so the best way is to dig into the source code. Those heuristics try to calculate cost of inlining the function call and probabilities of branch being taken. For example, gcc treats function calls guarded under condition as an error handling code (cold). Both gcc and llvm when they see a check for a pointer against null pointer: if (ptr == nullptr), they decide that pointer unlikely to be null, and put “else” branch as a fall through. It’s quite frequent that compilers do different inlining decisions for the same code because they have different heuristics and cost models. But in general, I think when compilers can’t decide which branch has bigger probability, they will leave the original order as they appear in the source code. I haven’t thoroughly tested that though. So, I think it’s a good idea to put your hot branch (most frequent) in a fall through position by default. Built-in expect and attributes for inlining: You can influence compiler decisions by making hints to it. When using clang you can use this attributes: void bar() __attribute__((noinline)) // won't be inlined{ if (__builtin_expect(criticalFailure, 0)) {  // error handling code  // this branch is NOT likely to be taken } else {  // hot code  // this branch is likely to be taken }}Here is documentation for inline attributes and Built-in expect. With those hints compiler will not make any guesses and will do what you asked for. Another disclaimer I want to make is that I’m not advocating for inserting those hints for every branch in your source code. It reduces readability of the code. Only put them in the places where it’s proven to improve performance. PGO (profile-guided optimizations): If it’s possible to use PGO in your case, it’s the best option you can choose. PGO will help compiler tune the generated code exactly for your workload. The problem here is that some applications do not have single workload or set of workloads, so it makes impossible to tune for the general case. But if you have such a single hot workload, you’ve better compile your code with PGO. The guide for using PGO in clang is described here. In short, compiler will first instrument your application with the code for profiling. Then you need to run it, it will generate profile information that you will then feed back to the compiler. Compiler will use this profile information to make better code generation decisions, because now it knows which code is hot and which is cold. I’ve seen real workloads that were improved up to 15%, which is quite attractive. PGO is not only improve code placement, but also improves register allocation, because with PGO compiler can put all the hot variables into registers, etc. "
    }, {
    "id": 21,
    "url": "https://dendibakh.github.io/blog/2018/06/08/Advanced-profiling-topics-PEBS-and-LBR",
    "title": "Advanced profiling topics. PEBS and LBR.",
    "body": "2018/06/08 - Contents:  Multiplexing and scaling events Runtime overhead of characterizing and profiling Interrupt- vs. event-based sampling Processor Event-Based Sampling (PEBS) Last Branch Record (LBR) Other resources:In my previous post I made an overview of what PMU (Performance Monitoring Unit) is and what is PMU counter (PMC). We learned that there are fixed and programmable PMCs inside each PMU. We explored basics of counting and sampling mechanisms and left off on the advanced techniques and features for sampling. To recap, previously I showed the number of steps which profiling tool does in order to collect statictics for your application. We initialize the counter with some number and wait until it overflows. On counter overflow, the kernel records information, i. e. , a sample, about the execution of the program. What gets recorded depends on the type of measurement, but the key information that is common in all samples is the instruction pointer, i. e. where was the program when it was interrupted. Multiplexing and scaling events: The topic of multiplexing between different events in runtime is covered pretty well here, so I decided to take most of the explanation from it. If there are more events than counters, the kernel uses time multiplexing to give each event a chance to access the monitoring hardware. With multiplexing, an event is not measured all the time. At the end of the run, the tool scales the count based on total time enabled vs time running. The actual formula is: final_count = raw_count * ( time_running / time_enabled )For example, say during profiling we were able to measure counter that we are interested 5 times, each measurement interval lasted 100ms (time_enabled). The program executed time is 1s(time_running). Total number of events for this counter is 10000 (raw_count). So, the final_count will be equal to 20000. This provides an estimate of what the count would have been, had the event been measured during the entire run. It is very important to understand that this is an estimate not an actual count. Depending on the workload, there will be blind spots which can introduce errors during scaling. This pretty much explains how “general-exploration” analysis in Intel VTune Amplifier is able to collect near 100 different events just in single execution of the programm. For callibrating purposes, profiling tools usually have thresholds for different counters to decide if we can trust the measured number of events, or it is too low to rely on (see MUX reliability). The easiest algorithm for multiplexing events is to manage it in round-robin fashion. Therefore each event will eventually get a chance to run. If there are N counters, then up to the first N events on the round-robin list are programmed into the PMU. In certain situations it may be less than that because some events may not be measured together or they compete for the same counter. To avoid scaling, one can try to reduce the number of events to be not bigger than the amount of physical PMCs available. Runtime overhead of characterizing and profiling: On the topic of runtime overhead in counting and sampling modes there is a very good paper written by A. Nowak and G. Bitzes. They measured profiling overhead on a Xeon-based machine with 48 logical cores in different configurations: with disabled/enabled Hyper Threading, running tasks on all/several/one cores and collecting 1/4/8/16 different metrics. In my interpretation there is almost no runtime overhead (1-2%%) in counting mode. In sampling mode it’s cheap unless you don’t multiplex between different counters (and keep sampling frequency not too high). However, if you’ll try to collect more counters than the physical PMU counters available, you’ll get performance hit of about 5-15% depending on the number of counters you want to collect. Finally, the higher is the sampling frequency the bigger is the overhead of profiling, as more interrupts need to be processed. Interrupt- vs. event-based sampling: Interrupt-based sampling can be described as a simple process in which when the counter overflows, processor triggers the performance interrupt. Interrupt-based sampling introduces skids on modern processors. That means that the instruction pointer stored in each sample designates the place where the program was interrupted to process the PMU interrupt, not the place where the counter actually overflows, i. e. , where it was at the end of the sampling period. In some case, the distance between those two points may be several dozen instructions or more if there were taken branches. Let’s take a look at the example:Let’s assume that on retirement of instr1 we have an overflow of the counter that samples “instruction retired” events. Because of latency in the microarchitecture between the generation of events and the generation of interrupts on overflow, it is sometimes difficult to generate an interrupt close to an event that caused it. So by the time the interrupt is generated our IP has gone further by a number of instructions. When we reconstruct register state in interrupt service routine, we have slightly inaccurate data. Processor Event-Based Sampling (PEBS): The problem with the skids is possible to mitigate by having the processor itself store the instruction pointer (along with other information) in a designated buffer in memory – no interrupts are issued for each sample and the instruction pointer is off only by a single instruction, at most. This needs to be supported by the hardware, and is typically available only for a subset of supported events – this capability is called Processor Event-Based Sampling (PEBS) on Intel processors. You can also see people call it Precise Event-Based Sampling, but according to Intel manuals, first word is “Processor” not “Precise”. But it basically means the same thing.  When a counter is enabled to capture machine state, the processor will write machine state information to a memory buffer specified by software. When the counter overflows from maximum count to zero, the PEBS hardware is armed. Upon occurrence of the next PEBS event, the PEBS hardware triggers an assist and causes a PEBS record to be written into the PEBS buffer. This record contains the architectural state of the processor (state of the general purpose registers, EIP register, and EFLAGS register). With PEBS, the format of the samples is mandated by the processor, so the best way to know it is to look into the Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3B, Chapter 18. Not all events support PEBS. For example, on Sandy Bridge there are 7 PEBS events supported:  INST_RETIRED UOPS_RETIRED BR_INST_RETIRED BR_MISP_RETIRED MEM_UOPS_RETIRED MEM_LOAD_UOPS_RETIRED MEM_LOAD_UOPS_LLC_HIT_RETIREDPEBS events for patricular architecture can be checked in Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3B, Chapter 18. PEBS buffer consists of records.  Each sample contains the machine state of the processor at the time the counter overflowed. Here is the example of PEBS record (picture below is taken from Intel manual): You can check if PEBS are enabled by executing dmesg right after startup: $ dmesg | grep PEBS[  0. 061116] Performance Events: PEBS fmt1+, IvyBridge events, 16-deep LBR, full-width counters, Intel PMU driver. You can use PEBS with perf by adding :p and :pp suffix to the event specifier: perf record -e event:ppBenefits of using PEBS:  The skid is mimized compared to regular interrupted instruction pointer.  Reduce the overhead because the Linux kernel is only involved when the PEBS buffer fills up, i. e. , there is no interrupt until a lot of samples are available. Last Branch Record (LBR): There is a great series on the topic of LBR and it’s applications on lwm. net by Andi Kleen:  Intel CPUs have a feature called last branch records (LBR) where the CPU can continuously log branches to a set of model-specific registers (MSRs). The CPU hardware can do this in parallel while executing the program without causing any slowdown. There is some performance penalty for reading these registers, however.  The LBRs log the “from” and “to” address of each branch along with some additional metadata. The registers act like a ring buffer that is continuously overwritten and provides only the most recent entries. There is also a TOS (top of stack) register to provide a pointer to the most recent branch. With LBRs we can sample branches, but during each sample look at the previous 8-32 branches that were executed. This gives reasonable coverage of the control flow in the hot code paths, but does not overwhelm us with too much information, as only a smaller number of the total branches are examined.  Once we are able to sample LBRs it is possible to set up sampling of branch events at a frequency that does not slow down the workload unduly, and still create an useful histogram of hot branches. It is important to keep in mind that this is still sampling, so not every executed branch can be examined. CPUs generally execute too fast for that to be feasible. The last branch recording mechanism tracks not only branch instructions (like JMP, Jcc, LOOP and CALL instructions), but also other operations that cause a change in the instruction pointer (like external interrupts, traps and faults). The branch recording mechanisms generally employs a set of MSRs (Model Specific Registers), referred to as last branch record (LBR) stack. The size and exact locations of the LBR stack are generally model-specific. The picture below is taken from Intel® 64 and IA-32 Architectures Optimization Reference Manual, Chapter B. 3. 3. 4: Last Branch Record (LBR) Stack — The LBR consists of N pairs of MSRs (N is, again, model specific) that store source and destination address of recent branches. Last Branch Record Top-of-Stack (TOS) Pointer — contains a pointer to the MSR in the LBR stack that contains the most recent branch, interrupt, or exception recorded. There are two important usages for LBR as mentioned in Intel® 64 and IA-32 Architectures Optimization Reference Manual, Chapter B. 3. 3. 4:  Collecting Call Counts and Function Arguments. If the LBRs are captured for PMIs triggered by the BR_INST_RETIRED. NEAR_CALL event, then the call count per calling function can be determined by simply using the last entry in LBR. As the PEBS IP will equal the last target IP in the LBR, it is the entry point of the calling function. Similarly, the last source in the LBR buffer was the call site from within the calling function. If the full PEBS record is captured as well, then for functions with limited numbers of arguments on 64-bit OS’s, you can sample both the call counts and the function arguments.  Basic Block Execution Counts. This is rather complicated to explain, so I refer a reader for the manual to read more about this. From a user perspective LBR can be used for collecting call-graph information even if you compiled your app without frame pointers (controlled by compiler option ‘-fomit-frame-pointer’, ON by default): $ perf record --call-graph lbrUsing LBR in perf also allows you to see where were the most amount of branches: $ perf record -bFor some more applications of LBR, including debugging support and hot-path branch history, you can take a look at Andi Kleen’s articles on lwn. net: part1, part2. Other resources::  Presentation on Black Hat 2015 conference by N. Herath and A. Fogh “These are Not Your Grand Daddy’s CPU Performance Counters”: video, slides Brendan Gregg’s article about perf Linux kernel profiling with perf tutorial Somewhat old, but still actual guide on how to program PMU: Nehalem Performance Monitoring Unit Programming Guide Description of instructions: RDPMC, RDTSC, RDTSCP, RDMSR, WRMSR in Intel SDM v2, Instruction Set Reference"
    }, {
    "id": 22,
    "url": "https://dendibakh.github.io/blog/2018/06/01/PMU-counters-and-profiling-basics",
    "title": "PMU counters and profiling basics.",
    "body": "2018/06/01 - Contents:  CPU mental model and simplest PMU counter So, how about counting more? Fixed and programmable counters MSRs - model specific registers Counting vs. Sampling Sampling and profilingOne way of analyzing performance of an application is instrumenting and then running it. Instrumenting means source code modification in such a way that allows us to grab some information about how our app executes. For example, we can measure number of calls for particular function, number of loop interations, etc. Theoretically, with this approach we can profile our application right within the application itself. However, it is not really the desired way to do it, because it is very time consuming, require recompilation each time we want to collect new metrics, and brings runtime overhead and noise in the measurements. We all know that, no surprise. The other way is to use profiling tools like perf and Vtune that will collect statistics without instrumenting the binary. I’m using those profiling tools very extensively but understanding how they work came to me not so long time ago. In this article I will try to uncover some basic principles of how those tools work. CPU mental model and simplest PMU counter: In a really simplified view our processor looks like this: Just for a moment let’s imagine that it is how things physically lay out on a die. I also omitted lots of things on this diagram, but it’s not really much important right now, just bear with me. There is a clock generator that sends pulses to every piece of the system to make everything moving to the next stage. This is called a cycle. If we add just a little bit of silicon and connect it to the pulse generator we can count a number of cycles, yay! This is the simplest possible counter. It is called a counter for a reason of course, it’s purpose is to count certain events. Every time a new pulse comes out our counter is incremented by 1. In reality counter is just yet another HW register. You can sample it from time to time to know how many clockticks passed. Counting cycles is great, however, that’s not super helpful if we want to collect statistics about, say, L1 cache or our execution units. So, how about counting more?: We can connect our counter to other units just by laying out the wires from every element we interested in to our counter.  Notice, that I added one more element to the figure, it’s a configuration register. Because now we need a way to tell “now I want to sneak into L1” and “now I want to return back to counting cycles”. I should also point out that this is not everything that is needed for our beautiful counters to work. We also need special assembly instructions to read the value from the counter and write to the config register. In order for those instructions to work we need physical paths from execution units to all the counters to be able to pull the values from it. With only one counter it’s possible to count only one thing at a time. Ough! You maybe already guessed where I’m going with this. Each additional counter increases complexity and amount of wires quite significantly. And of course we have a limited amount of physical paths on a die. In practice, architects don’t try to connect every component with every counter, because it increases amount of wires. Instead they try to put counters in a different places on a die to be as closer as possible to the components they are intended to observe. And also they connect each component to at least 2 different counters, so that it’s guaranteed to be able to count two different events at the same time. Taking in consideration our example, it will look something like this: Usually there is also one global register that controls all the other counters. For example, with it you can turn all the counters on and off. And that also requires physical paths from the global configuration registers to all the counters in the processor. Fixed and programmable counters: In practice most of the CPUs have PMU (Performance Monitoring Unit) with fixed and programmable counters. Fixed PMC (Performance Monitoring Counter) always measures the same thing inside the core. With programmable counter it’s up to user to choose what he wants to measure. I believe for the most Intel Core processors, number of fully programmable counters is 4 (per logical core) and usually 3 fixed function counters (per logical core). Fixed counters usually are set to count core clocks, reference clocks, instructions retired. More details can be found in Intel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 3B, Part2, Chapter 18. 2. 2. For my IvyBridge processor here is the output of cpuid command: $ cpuid. . .  Architecture Performance Monitoring Features (0xa/eax):   version ID                = 0x3 (3)   number of counters per logical processor = 0x4 (4)   bit width of counter           = 0x30 (48). . .  Architecture Performance Monitoring Features (0xa/edx):   number of fixed counters  = 0x3 (3)   bit width of fixed counters = 0x30 (48). . . Similar information can be grepped out from the kernel message buffer right after the system is booted: $ dmesg. . . [  0. 061530] Performance Events: PEBS fmt1+, IvyBridge events, 16-deep LBR, full-width counters, Intel PMU driver. [  0. 061550] . . . version:        3[  0. 061550] . . . bit width:       48[  0. 061551] . . . generic registers:   4[  0. 061551] . . . value mask:       0000ffffffffffff[  0. 061552] . . . max period:       00007fffffffffff[  0. 061552] . . . fixed-purpose events:  3[  0. 061553] . . . event mask:       000000070000000f. . . MSRs - model specific registers: PMU counters and configuration registers are implemented as MSR (Model Specific Registers) registers. What that means is that number of counters and their width can vary from model to model and you can’t rely on the same number of counters in your CPU, you should always query that first, using cpuid. MSRs are accessed via the RDMSR and WRMSR instruction. Certain counter registers can be accessed via the RDPMC instruction. More information and details are available in Volume 2B of the Programmer’s Reference Manual. Counting vs. Sampling: Typical use case for such a counter would be: - disable counting- set all the counters to 0- configure evenst that we want to measure- enable counting- run the application- disable counting- read the values of the countersThis process is also called characterizing, and this is what such tools do! This method allows you to collect overall statistics about execution of the application. This is, for example, what perf stat will output if I will run it on ls and try to additionally collect 3 advanced counters: $ perf stat -e r5301b1,r53010e,r5301c2,instructions,cycles,ref-cycles -- ls&lt;output of ls command&gt; Performance counter stats for 'ls':      2142223   r5301b1          # UOPS_EXECUTED. CORE                       2217291   r53010e          # UOPS_ISSUED. ANY                       2084935   r5301c2          # UOPS_RETIRED. ALL                       1553280   instructions       #  0,75 insn per cycle                            2078230   cycles                                 3062668   ref-cycles                             0,001497400 seconds time elapsedNote, that some data was collected “for free”, based on fixed PMCs (see above). Also perf is not showing the name of the counter in the output, it was added by me. Codes for the counters (that I put in parameters to perf) can be obtained with the method described here. As you can see, there are no details about the hottest functions or the line of code which caused the biggest amount of cache misses, etc. It just raw statistics for the whole runtime. Basically, during the whole runtime each counter measured only one thing. There were no multiplexing between them. In Intel Vtune Amplifier there is analysis which is called “general-exploration” and it’s capable of collecting lots of counters during the runtime, but it’s obviously does that by multiplexing between them in the runtime. Multiplexing adds more overhead (because we need to switch counters in the runtime) and decreases the precision. There is one caveat to this. In particular: what happens when the counter overflows? In this situation you can handle OS exception, and inside this handler you can: - stop counting- increment the number of overflows in some variable in SW- clear the counter to zero- start counting againThere is for sure more things that we need to care about, but before going into more sophisticated things lets consider another fundamental concept called “sampling”. Sampling and profiling: If you take an OS exception anyway, there is a lot of information you can get from it. For example, you can capture IP (instruction pointer). So if you will dump your IP at the time when the counter overflows, (ta-dam!) you know the place in your program where the event occurred. Say we have our PMU counter of 32-bit width. If you start to count clockticks with such a counter and capture overflows, suddenly you will stop your application approximately each 1 second (2^32 cycles, and if the CPU frequency 4. 3 GHz) and know what your application executes in this particular moment. And this is more detailed but still quite simplified process of profiling: - set counter to 0- enable counting- wait for the overflow and disable counting when it happens- inside the interrupt handler capture IP, registers state, etc. - repeat the processThe problem in this case is that we only sample every 1 second, which might not be the good precision. To have a finer granularity we can start not at a counter being zeroed out, but at a maximum value minus some threshold. So, if we want to count every 100 ms, we set initial value to 0xFFFFFFFF - 0x19999999, where 0x19999999 represent the number of clockticks in 100ms. And in this case we will receive an interrupt after 0x19999999 clocks. Typically very few amount of work is done by the profiling tool during collection of samples. Then there is a separate post-processing stage sometimes called “finalization” which parses all the raw samples, organizes them and convert them into human readable form. In linux perf profiling associates with perf record command. More examples about using perf you can find in the great article by Brendan Gregg. The similar capabilities for Intel Vtune Amplifier are “hotspot” and “advanced-hotspot” analysis. Sampling at a smaller intervals increases the overhead (because of more interrupts) and makes the size of collected data bigger. On the other side, counting involves almost no runtime overhead. I will write more about the topic of overhead and how profilers deal with that in my next articles. That’s all for today, in my next post I will dig more into profiling and write about two advanced sampling techniques: PEBS and LBR. "
    }, {
    "id": 23,
    "url": "https://dendibakh.github.io/blog/2018/05/03/My-learning-resources",
    "title": "My learning resources in 2018",
    "body": "2018/05/03 - Contents:  Online courses Podcasts Blogs and sites BooksAbout a year ago I joined C/C++ compiler team at Intel. And some people asked me how to become a compiler developer, because I assume that for many C++ developers compiler is just a piece of magic. Now, I don’t consider myself an expert in this field, but still I decided to share with others what learning resources do I use. And it’s not limited to compilers, but in general. I hope that someone will find something useful in this post. I don’t want to advocate for any particular form of learning. It works differently for different people, so I will just place them in the order which works the best for me. Also I completely accept the fact that the resources I use in 2018 are somewhat different than they were a year ago. I’m starting reading different blogs and listen to different podcasts as my focus moves in different direction. Online courses: Coursera - this is my main source of gaining knowledge for years. I finished ~10 online courses so far on this site. I think that it is just an amazing opportunity to study courses from the best universities around the world. Absolutely for free! Some of the courses that I finished:  Computer Architecture by Princeton University.  Heterogeneous Parallel Programming (link is not active anymore) by University of Illinois at Urbana-Champaign.  Cryptography by Stanford University Algorithms part1 and part2 by Princeton University. And it’s not limited to Computer Science disciplines. There are really a lot of interesting courses out there. Usually the course lasts for ~3 months, but it takes me twice as more time to finish it. Anyway, I try to take a least one course every year. I don’t have experience on other online platforms like Udacity, so can’t really compare them. Also some universities have their own online learning systems. One such course that I took fairly recently was Compiler Class by Stanford University. I would recommend this course for all C++ developers just to have a feeling of what’s going on under the cover of C++ compiler. Also I should include here videos from conferences like CppCon, CppNow and LLVM dev meeting - I learned a lot from watching recordings of the talks. Podcasts: CppCast - podcast for C++ developers by C++ developers. I’m their regular listener from the very first episodes. Although I’m not that focused on modern features of C++ than I used to be in the past, but I still listen to this podcast during my way to the office each Friday morning. Even though I’m now not spending my free time on playing with C++17 and such, but it’s really good source of information of keeping up-to-date with the new stuff that’s coming in C++ world. I still consider myself as a part of this community. Rob and Jason are doing really great work! Simple Programmer podcast - podcast mostly about soft skills. I was so upset when John Sonmez said he will stop doing this podcast. This was a truly gem for me, I really learned a lot from his podcasts. I would recommend his stuff to everybody who want to boost his/her career. After John’s course on making your own blog I restarted my blog, because he really showed me the value of doing it. John usually speaks about simple things, like commit yourself and being consistent on those commitments. Those are simple things, but it’s not easy to follow them. John has his own techniques to deal with such kind of things and I try to apply some of them in my life too. Programming and Performance with Cliff Click - this is a podcast mostly about Java, but I like to listen to it, because Cliff is really the expert on performance and compilers. He often brings topics about compilers, and that’s quite interesting to me. Twich (This week in computer hardware), PC perspective - as I’m now working closely with the HW, I need to stay tuned with the latest news and trends that coming up in CPU space. And those two podcasts help me whit that. SE radio, SE daily - I don’t listen to those two podcasts regularly, but sometimes on those podcasts appear interesting topics that I want to listen. Usually episodes are about general things in SW development, management, new services and tools. Blogs and sites: Here is just a list of blogs and sites that I try to follow quite frequently:  Daniel Lemire - performance and optimizations, language agnostic John Sonmez - soft skills Josh Doody - salary negotiations Agner Fog optimization manuals and blog - I recommend spending some time to read at least his microarchitecture. pdf manual.  Random ASCII, null program, The ryg blog, and others (I must have forgotten a few :) ). Books: I really like to read books as well. This form of gaining knowledge works pretty well for me, but I must say that I used to read a lot more in the past, than I do now. For C++ there is a good collection of books on stackoverflow. I’ve read some of the books from this list. If you want to familiarize yourself with LLVM I would recommend LLVM essentials - it is a really light read, introducing you a high level concepts of LLVM. I haven’t read the classic dragon book, but I have one good book about compiler optimizations and code generation (not covering front-end) - Advanced Compiler Design and Implementation by Steven Muchnik. Not an easiest book in the world, but very informative. "
    }, {
    "id": 24,
    "url": "https://dendibakh.github.io/blog/2018/04/22/What-optimizations-you-can-expect-from-CPU",
    "title": "What optimizations you can expect from CPU?",
    "body": "2018/04/22 - Contents:  Zero Idiom Move elimination Other experiments     Redundant movs   Substracting zero   Known comparison    SummaryCompilers are known for doing all sorts of cool optimizations on the source code, generating very efficient assembly code. You can expect that there will be no useless computations in the compiled code. Even if you leave those inefficiencies, most major compilers will optimize everything away. Moreover, compilers are aware (to some degree) about microarchitectural details of the target CPU. So, it may seems that compiler is the one who is in charge for performance, but it’s not. Modern high-end CPUs are also known to be really greedy when it comes for performance, and they also do amazing job at running assembly code super-fast. In this post I tried to show what optimizations you can rely on, and what patterns are still beyond CPU capabilities. Zero Idiom: From Agner’s Fog microarchitecture. pdf:  The processor recognizes that certain instructions are independent of the prior value of the register if the two operand registers are the same. An instruction that subtracts a register from itself will always give zero, regardless of the previous value of the register. This is traditionally a common way of setting a register to zero. Many modern processors recognize that this instruction doesn’t have to wait for the previous value of the register. What is new in the Sandy Bridge is that it doesn’t even execute this instruction. The register allocater simply allocates a new empty register for the result without even sending it to the execution units. This means that you can do four zeroing instructions per clock cycle without using any execution resources. NOPs are treated in the same efficient way without using any execution unit. For example: sub eax, eaxxor eax, eaxTo illustrate this I ran 1000 iterations (edi == 1000) of the code below using uarch-bench tool. All experiments I’ve done on Ivy Bridge CPU: . loop:xor eax, eaxdec edijnz . loopResults:           Benchmark  Cycles  UOPS_EXECUTED. CORE  UOPS_ISSUED. ANY  UOPS_RETIRED. ALL         xor eax, eax   1. 02  1. 01         2. 04       2. 00As always, this tool shows the values for the counters per iteration. Remember, that dec + jnz are MacroFused into one uop, which is the only uop that was executed (utilizing execution units). Read more on this in my blog post about MacroFusion. Interestingly enough, that on Ivy Bridge mov eax, 0 is not recognized:           Benchmark  Cycles  UOPS_EXECUTED. CORE  UOPS_ISSUED. ANY  UOPS_RETIRED. ALL          mov eax, 0   1. 02  2. 02         2. 04       2. 00You can see that 2 uops were executed, meaning that mov instruction also utilized execution units. Move elimination: Again, from Agner’s Fog microarchitecture. pdf:  An eliminated move has zero latency and does not use any execution port. Zero latency instructions (for example NOP instructions) don’t consume scheduler resources.  However, those instructions still consume bandwidth in the decoders and reserve a number of slots in the reorder buffer.  Move elimination is not always successful. It fails when the necessary operands are not ready. But typically, move elimination succeeds in more than 80% of the possible cases. Chained moves can also be eliminated. Move elimination is possible with all 32-bit and 64-bit general purpose registers and all 128-bit and 256-bit vector registers. Example: . loop:add eax,4mov ebx,eax ; this mov can be eliminatedsub ebx,ecxdec edijnz . loop I ran 1000 iterations of the loop, and here is what I received:           Benchmark  Cycles  UOPS_EXECUTED. CORE  UOPS_ISSUED. ANY  UOPS_RETIRED. ALL       move elimination   1. 03  3. 02         4. 04       4. 01Once again, the number of executed uops is less than the number of issued and retired uops. dec + jnz were MacroFused into one uop and the mov inside the loop was eliminated. Other experiments: Zeroing instructions and move elimination are well known idioms, but let’s try to check what else patterns can be recognized by the CPU. Redundant movs: . loop:mov eax, 1 ; will be eliminated?mov eax, 2 dec edijnz . loopNow, I want to mention, that compilers will never generate this dumb code (if it’s not a bug in the compiler). Also in embedded world this code can make sense, when you need to write particular sequence of bytes into the microcontroller registers. As before I ran 1000 iterations and indeed CPU silently executes every assembly instruction:           Benchmark  Cycles  UOPS_EXECUTED. CORE  UOPS_ISSUED. ANY  UOPS_RETIRED. ALL    mov eax, 1; mov eax, 2   1. 02  3. 02         3. 05       3. 00Substracting zero: . loop:xor eax, eax sub ebx, eax ; will be eliminated? (eax is always 0)dec edijnz . loopResults show that CPU doesn’t recognize that eax is always zero and does subtracting operation on ebx register:           Benchmark  Cycles  UOPS_EXECUTED. CORE  UOPS_ISSUED. ANY  UOPS_RETIRED. ALL        implicit sub 0   1. 02  2. 01         3. 04       3. 01In this example, xor eax, eax consumed no execution resources, so that’s where the difference between the number of executed and issued uops comes from. I tried to do explicit subtraction of 0, and it also was not eliminated: sub ebx, 0 ; execution not eliminated on IvyBridge. Known comparison: . loop:mov eax, 0mov ebx, 0cmp eax, ebx ; eax and ebx are always equaljne . exitdec edijnz . loop. exit:Results:           Benchmark  Cycles  UOPS_EXECUTED. CORE  UOPS_ISSUED. ANY  UOPS_RETIRED. ALL      known comparison 1   2. 02  4. 05         4. 10       4. 01In this example there are 2 mov uops and 2 Macrofused cmp+jump uops, which give total of 4 uops. Each of them utilized execution resources and nothing was eliminated. Summary: Modern CPU are very powerful at doing computations, but don’t expect miracles from it. Zeroing instructions and move elimination are implemented using the microarchitectural features, and they require small amount of extra logic in the CPU frontend. I think that some of the sequences are faster to execute than try to preprocess them in the front-end. For example, the case with redundant movs (see mov eax, 1; mov eax, 2). Mechanism of register renaming and pending register writes works really well here. Trying to identify those 2 instructions with the same destination registers may just not worth the effort. Last two cases (substracting zero and known comparison) were rather a blind shot. In order to eliminate instructions in question we need to do comparisons of the inputs in the front-end, but it’s a job for a back-end, so we can just schedule them for the execution. "
    }, {
    "id": 25,
    "url": "https://dendibakh.github.io/blog/2018/04/03/Tools-for-microarchitectural-benchmarking",
    "title": "Tools for microarchitectural benchmarking.",
    "body": "2018/04/03 - Contents:  What do I mean by microarchitectural benchmarking? Static tools Dynamic tools Benchmark kernel IACA     how to use it   what is the output   limitations    llvm-mca     how to use it   what is the output   limitations    uarch-bench     how to use it   what is the output    likwid     how to use it   what is the output    Benchmarking using assembly instructionsI did a fair amount of low level experiments during the recent months and I tried different tools for making such experiments. In this post I just want to bring a qiuck summary for those tools in one place. Disclaimer: I have no intention to compare different tools. What do I mean by microarchitectural benchmarking?: Modern computers are so complicated that it’s really hard to measure something in isolation. It’s not enough to just run your benchmark and measure execution time. You need to think about context switches, CPU frequency scaling features (called “turboboost”), etc. There are a lot of details that can affect execution time. What would you do if you want just to benchmark two assembly sequences? Or you want to experiment with some HW feature to see how it works? Even if my benchmark is a simple loop inside main and I measure execution time of the whole binary - that’s not a benchmark that I want. There is a lot of code that runs before main, so it will add a lot of noise. So, running my binary under perf stat -e is not something that I want for this type of benchmarking. What I want is to have a fine-grained analysis for some specific code region, not the whole execution time. Microarchitectural benchmarking without collecting performance counters doen’t make much sense, so I want to have that as well. For describing such kind of experiments I came up with a term “microarchitectural benchmarking” and it maybe not very accurate, so I’m open for suggestions/comments here. In this post I will give you a taste of the tools available without going too much into the details. Also we need to distinguish between static and dynamic tools. Static tools: Static tools don’t run the actual code but try to simulate the execution keeping as much microarchitectural details as they can. Of course they are not capable of doing real measurements (execution time, performance counters) because they don’t run the code. The good thing about that is that you don’t need to have the real HW. You don’t need to have privileged access rights as well. Another benefit is that you don’t need to worry about consistency of the results. Static tools will always give you stable output, because simulation (in comparison with execution on a real hardware) is not biased in any way. The downside of static tools is that usually they can’t predict and simulate everything inside modern CPUs and thus are useless in some situations. Today we will look into two examples of such tools: IACA and llvm-mca. Dynamic tools: Dynamic tools are based on running the code on the real HW and collecting all sorts of information about the execution. The good thing about it is that this is the only 100% reliable method of proving things. As a downside, usually you are required to have privileged access rights to collect performance counters. Also, it’s not so easy to write a good benchmark and measure what you want to measure. Finally, you need to filter the noise and different kinds of side effects. From dynamic tools, today we will take a look at uarch-bench and likwid. Benchmark kernel: Microarchitectural benchmarking is often used when you want to stress some particular CPU feature or find the bottleneck in some small piece of code. I decided to come up with an assembly example that would be handled equally good by all the tools. I will try to run the same experiment under each of those tools and show the output. I will stress my IvyBridge CPU with example from my previous article about port contention. mov eax, DWORD [rsp]   ; goes to port 2 or 3mov eax, DWORD [rsp + 4] ; port 2 or 3bswap ebx		 ; goes to port 1bswap ecx		 ; goes to port 1 (port contention)IACA: IACA stands for Intel® Architecture Code Analyzer. IACA helps you statically analyze the data dependency, throughput and latency of code snippets on Intel® microarchitectures. how to use it: It has API for C, C++ and assembly languages. In order to use it you just need to wrap the code that you want to analyze with special markers. Then you need to run your binary under IACA and it will analyze the region of the code that you specified. mov ebx, 111 		; Start marker bytesdb 0x64, 0x67, 0x90 	; Start marker bytes	; kernelmov ebx, 222 		; End marker bytesdb 0x64, 0x67, 0x90 	; End marker bytesComplete code can be found on my github. Then we run the binary under IACA. . /iaca -arch HSW -trace iaca. log -trace-cycle-count 50 . /a. outUnfortunately, in latest version (3. 0) support for IVB was dropped, and previous version (2. 3) showed some really wierd results, so I decided to simulate it on HSW. what is the output: Here is the output that it produces: Intel(R) Architecture Code Analyzer Version - v3. 0-28-g1ba2cbb build date: 2017-10-23;16:42:45Analyzed File - . /a. outBinary Format - 64BitArchitecture - HSWAnalysis Type - ThroughputThroughput Analysis Report--------------------------Block Throughput: 1. 79 Cycles    Throughput Bottleneck: FrontEndLoop Count: 31Port Binding In Cycles Per Iteration:--------------------------------------------------------------------------------------------------| Port |  0  - DV  |  1  |  2  - D  |  3  - D  |  4  |  5  |  6  |  7  |--------------------------------------------------------------------------------------------------| Cycles | 1. 0   0. 0 | 1. 0 | 1. 0   1. 0 | 1. 0   1. 0 | 0. 0 | 1. 0 | 1. 0 | 0. 0 |--------------------------------------------------------------------------------------------------DV - Divider pipe (on port 0)D - Data fetch pipe (on ports 2 and 3)F - Macro Fusion with the previous instruction occurred* - instruction micro-ops not bound to a port^ - Micro Fusion occurred# - ESP Tracking sync uop was issued@ - SSE instruction followed an AVX256/AVX512 instruction, dozens of cycles penalty is expectedX - instruction not supported, was not accounted in Analysis| Num Of  |          Ports pressure in cycles             |   || Uops  | 0 - DV  | 1  | 2 - D  | 3 - D  | 4  | 5  | 6  | 7  |-----------------------------------------------------------------------------------------|  1   |       |   | 1. 0   1. 0 |       |   |   |   |   | mov eax, dword ptr [rsp]|  1   |       |   |       | 1. 0   1. 0 |   |   |   |   | mov eax, dword ptr [rsp+0x4]|  2   |       | 1. 0 |       |       |   |   | 1. 0 |   | bswap ebx|  2   | 1. 0     |   |       |       |   | 1. 0 |   |   | bswap ecx|  1*   |       |   |       |       |   |   |   |   | dec rdi|  0*F  |       |   |       |       |   |   |   |   | jnz 0xfffffffffffffff2Total Num Of Uops: 7IACA helps in finding bottlenecks of a loop body:  It provides throughput of the whole analyzed block (counted in cycles).  It predicts what would be the bottleneck source that will limit the throughput.  It tells what ports are under the high pressure. More detailed description of the output can be found in the IACA Users Guide. But the most interesting part is in the pipeline traces (generated by -trace option): it|in|Dissasembly                    :01234567890123456789012345678901234567890123456789 0| 0|mov eax, dword ptr [rsp]             :     |     |     |     |     0| 0|  TYPE_LOAD (1 uops)              :s---deeeew----R-------p    |     |     0| 1|mov eax, dword ptr [rsp+0x4]           :     |     |     |     |     0| 1|  TYPE_LOAD (1 uops)              :s---deeeew----R-------p    |     |     0| 2|bswap ebx                     :     |     |     |     |     0| 2|  TYPE_OP (2 uops)               :sdew----------R-------p    |     |     0| 3|bswap ecx                     :     |     |     |     |     0| 3|  TYPE_OP (2 uops)               : sdew----------R-------p   |     |     0| 4|dec rdi                      :     |     |     |     |     0| 4|  TYPE_OP (1 uops)               : sdw-----------R-------p   |     |     0| 5|jnz 0xfffffffffffffff2              :     |     |     |     |     0| 5|  TYPE_OP (0 uops)               : w-------------R-------p   |     |     1| 0|mov eax, dword ptr [rsp]             :     |     |     |     |     1| 0|  TYPE_LOAD (1 uops)              : s---deeeew----R-------p   |     |     1| 1|mov eax, dword ptr [rsp+0x4]           :     |     |     |     |     1| 1|  TYPE_LOAD (1 uops)              : s---deeeew----R-------p   |     |     1| 2|bswap ebx                     :     |     |     |     |     1| 2|  TYPE_OP (2 uops)               : sdew----------R-------p   |     |     1| 3|bswap ecx                     :     |     |     |     |     1| 3|  TYPE_OP (2 uops)               : Asdew---------R-------p   |     |     1| 4|dec rdi                      :     |     |     |     |     1| 4|  TYPE_OP (1 uops)               :  w-------------R-------p  |     |     1| 5|jnz 0xfffffffffffffff2              :     |     |     |     |     1| 5|  TYPE_OP (0 uops)               :  w-------------R-------p  |     |Once again, mode detailed description of the output can be found in the IACA Users Guide. Here is the most imortant part from it:  The kernel instructions are modeled, in order, from top to bottom while the processor’s cycles run from left to right. The ‘it’ column shows the iteration count of the entire kernel, the ‘in’ column shows the instruction count within the kernel and the ‘Disassembly’ column shows the instruction’s disassembly, along with the micro-architectural instruction fragment information.  The trace displays the micro-architectural stage of each fragment inside the processor at any given cycle from allocation to retire and even post retire. The stages and possible states are:  [A] – Allocated[s] – Sources ready[c] – Port conflict[d] – Dispatched for execution[e] – Execute[w] – Writeback[R] – Retired[p] – Post Retire[-] – pending[_] – Stalled due to unavailable resources I think this is really cool! It allows you to see how instructions progress through the pipeline, which is not only good for educational purposes, but also can give you a hint why your code executes not as fast as you want. Though, on HSW there is no port contention issue for this assembly code, because bswap can be also executed at least on 2 ports. For details, take a look at my post Understanding CPU port contention. I showed only first two iterations, but complete output of this run can be found on my github. limitations: I tried to run binaries from my Code Alignment post with inserted IACA markers and the tool showed no difference. Meaning that this tool doen’t take into account how hot piece of code is placed in the binary. I would be good to have complete list of limitations of that tool, but I haven’t found this information (would be glad if someone will provide it). llvm-mca: llvm-mca is a LLVM Machine Code Analyzer tool which is also a tool that does static analysis of the machine code. From it’s description:  llvm-mca is a performance analysis tool that uses information available in LLVM (e. g. scheduling models) to statically measure the performance of machine code in a specific CPU. It was fairly recently announced on llvm-dev mailing list (March 2018) and checked into llvm trunk. So, documentation for it is not yet mature enough, so the best source of information for now is this email thread. how to use it: What this tool needs is just assembly code, you don’t need to compile it. However, it accepts only AT&amp;T assembly syntax which is sad but there are assembly converters out there. Another thing is that options are a little bit misleading and I spent some time digging into the sources to understand what I should put into them. Usually -march identifies the CPU architecture (like ivybridge, skylake, etc. ), but OK… $ cat a. asmmovl (%esp), %eaxmovl 4(%esp), %eaxbswapl %ebxbswapl %ecx$ llvm-mca -march=x86-64 -mcpu=ivybridge -output-asm-variant=1 -timeline . /a. asm -o mca. outwhat is the output: The output was mostly inspired by IACA tool, so it looks really familiar to IACA users. Here is reduced output for my assembly code: Iterations:   70Instructions:  280Total Cycles:  144Dispatch Width: 4IPC:      1. 94Instruction Info:[1]: #uOps[2]: Latency[3]: RThroughput[4]: MayLoad[5]: MayStore[6]: HasSideEffects[1]  [2]  [3]  [4]  [5]  [6]	Instructions: 1   5   0. 50  *        	mov	eax, dword ptr [esp] 1   5   0. 50  *        	mov	eax, dword ptr [esp + 4] 2   2   1. 00          	bswap	ebx 2   2   1. 00          	bswap	ecxResources:[0] - SBDivider[1] - SBPort0[2] - SBPort1[3] - SBPort4[4] - SBPort5[5. 0] - SBPort23[5. 1] - SBPort23Resource pressure per iteration:[0]  [1]  [2]  [3]  [4]  [5. 0] [5. 1]  -   1. 00  2. 00  -   1. 00  1. 00  1. 00  Resource pressure by instruction:[0]  [1]  [2]  [3]  [4]  [5. 0] [5. 1] 	Instructions: -   -   -   -   -   0. 50  0. 50  	mov	eax, dword ptr [esp] -   -   -   -   -   0. 50  0. 50  	mov	eax, dword ptr [esp + 4] -   -   1. 00  -   1. 00  -   -   	bswap	ebx -   1. 00  1. 00  -   -   -   -   	bswap	ecxTimeline view:   	     0123456789  Index	0123456789     0123[0,0]	DeeeeeER .   .   .  . 	mov	eax, dword ptr [esp][0,1]	DeeeeeER .   .   .  . 	mov	eax, dword ptr [esp + 4][0,2]	DeeE---R .   .   .  . 	bswap	ebx[0,3]	. DeeE--R .   .   .  . 	bswap	ecx[1,0]	. DeeeeeER .   .   .  . 	mov	eax, dword ptr [esp][1,1]	. DeeeeeER .   .   .  . 	mov	eax, dword ptr [esp + 4][1,2]	. DeeE--R .   .   .  . 	bswap	ebx[1,3]	. D=eeE-R .   .   .  . 	bswap	ecx. . . &lt; iterations 2. . 8 &gt;[9,0]	.   .   .  DeeeeeE-R . 	mov	eax, dword ptr [esp][9,1]	.   .   .  DeeeeeE-R . 	mov	eax, dword ptr [esp + 4][9,2]	.   .   .  D====eeER. 	bswap	ebx[9,3]	.   .   .  D=====eeER	bswap	ecxD : Instruction dispatched. e : Instruction executing. E : Instruction executed. R : Instruction retired. = : Instruction already dispatched, waiting to be executed. - : Instruction executed, waiting to be retired. Resource pressure view doesn’t seem right, as we know that bswap instruction can be executed only on PORT1 on Ivy Bridge (UPD 06. 04. 2018: issue has been fixed r329211). However, reciprocal throughput is correct (equals to 1). Because throughput is correct, timeline view also seems to be correct. On later iterations (see iteration #9) we can spot that execution starts to be limited by bswap instructions. You can observe the same picture in my previous post Understanding CPU port contention. Complete output of this run can be found on my github. limitations: This tool is really fresh and has significant restrictions. From this email thread:  The tool only models the out-of-order portion of a processor. Therefore, the instruction fetch and decode stages are not modeled. Performance bottlenecks in the frontend are not diagnosed by this tool. The tool assumes that instructions have all been decoded and placed in a queue. Also, the tool doesn’t know anything about branch prediction and simultaneous mutithreading.  Also the tool has very relaxed model for LSUnit (load and store unit). It doesn’t know when store-to-load forwarding may occur and doesn’t attempt to predict whether a load or store hits or misses the L1 cache. uarch-bench: I made quite big amount of experiments with uarch-bench in my recent posts, so readers of my blog might had a chance to get familiar with it already. From it’s description:  Uarch-bench is a fine-grained micro-benchmark intended to investigate micro-architectural details of a target CPU, or to precisely benchmark small functions in a repeatable manner. how to use it: All the benchmarks are integrated in the main binary, so in order to write your own benchmark in assembly code you need to insert it into x86_methods. asm and register it in misc-benches. cpp: GLOBAL PortContentionPortContention:push rcxpush rbxALIGN 16. loop:mov eax, DWORD [esi] mov eax, DWORD [esi + 4]bswap ebxbswap ecxdec edijnz . looppop rbxpop rcxretwhat is the output: I compiled and run it: $ . /uarch-bench --test-name= PortContention  --timer=libpfc --extra-events=UOPS_DISPATCHED_PORT. PORT_1,UOPS_DISPATCHED_PORT. PORT_2,UOPS_DISPATCHED_PORT. PORT_3,UOPS_DISPATCHED_PORT. PORT_5USE_LIBPFC=1make: Nothing to be done for 'all'. Welcome to uarch-bench (c75eeb8-dirty)libpfm4 initialized successfullyEvent 'UOPS_DISPATCHED_PORT. PORT_1' resolved to 'ivb::UOPS_DISPATCHED_PORT:PORT_1:k=1:u=1:e=0:i=0:c=0:t=0, short name: 'UOPS_D' with code 0x5302a1Event 'UOPS_DISPATCHED_PORT. PORT_2' resolved to 'ivb::UOPS_DISPATCHED_PORT:PORT_2:k=1:u=1:e=0:i=0:c=0:t=0, short name: 'UOPS_D' with code 0x530ca1Event 'UOPS_DISPATCHED_PORT. PORT_3' resolved to 'ivb::UOPS_DISPATCHED_PORT:PORT_3:k=1:u=1:e=0:i=0:c=0:t=0, short name: 'UOPS_D' with code 0x5330a1Event 'UOPS_DISPATCHED_PORT. PORT_5' resolved to 'ivb::UOPS_DISPATCHED_PORT:PORT_5:k=1:u=1:e=0:i=0:c=0:t=0, short name: 'UOPS_D' with code 0x5380a1Pinned to CPU 0lipfc init OKRunning benchmarks groups using timer libpfc** Running benchmark group PortContention tests **           Benchmark  Cycles  UOPS_D  UOPS_D  UOPS_D  UOPS_D        PortContention   2. 00   2. 00   1. 01   1. 01   1. 00Notice, how I specified the performance counters that I want to collect with --extra-events option. I did 1000 iterations, but the tool already calculated all the metrics per 1 iteration. So, we run at 2 cycles per iteration in which 2 uops were dispatched to PORT1 and ports 2, 3 and 5 handled 1 uop each. likwid: Likwid is more than just a tool for doing microarchitectural benchmarking. It consists of many utilities for people doing HPC stuff. You can find the complete list on the main page of the tool. Here is the great article describing it’s basic usages. Also likwid has very detailed wiki so you can use it as well. Here are the instructions how to build likwid tools. We will only use likwid-perfctr which allows to configure and read out hardware performance counters. how to use it: Likwid has marker API but only for C/C++, so in order to write a benchmark I wrote a function in assembly and invoked it from C: #define N 10000void benchmark(int iters, void* ptr);int main(int argc, char* argv[]){  int data[N];  LIKWID_MARKER_INIT;  LIKWID_MARKER_THREADINIT;  LIKWID_MARKER_START( foo );  benchmark(N, data);  LIKWID_MARKER_STOP( foo );  LIKWID_MARKER_CLOSE;  return 0;}GLOBAL benchmarkbenchmark:push rbxpush rcx. loop:mov eax, DWORD [rsi] mov eax, DWORD [rsi + 4]bswap ebxbswap ecxdec rdijnz . loopmov eax, 0pop rcxpop rbxretI compiled everything like this: $ export LIKWID_INCLUDE=/usr/local/bin/. . /include/$ export LIKWID_LIB=/usr/local/bin/. . /lib/$ nasm -f elf64 benchmark. asm$ gcc -c -DLIKWID_PERFMON -I$LIKWID_INCLUDE test. c -o test. o $ gcc benchmark. o test. o -o a. out -L$LIKWID_LIB -llikwidwhat is the output: $ export LD_LIBRARY_PATH= $LIKWID_LIB:$LD_LIBRARY_PATH $ likwid-perfctr -C S0:0 -g UOPS_DISPATCHED_PORT_PORT_1:PMC0,UOPS_DISPATCHED_PORT_PORT_2:PMC1,UOPS_DISPATCHED_PORT_PORT_3:PMC2,UOPS_DISPATCHED_PORT_PORT_5:PMC3 -m . /a. out--------------------------------------------------------------------------------CPU name:	Intel(R) Core(TM) i3-3220T CPU @ 2. 80GHzCPU type:	Intel Core IvyBridge processorCPU clock:	2. 79 GHz----------------------------------------------------------------------------------------------------------------------------------------------------------------Region foo, Group 1: Custom+-------------------+----------+|  Region Info  | Core 0 |+-------------------+----------+| RDTSC Runtime [s] | 0. 000014 ||   call count  |    1 |+-------------------+----------++-----------------------------+---------+--------------+|      Event      | Counter |  Core 0  |+-----------------------------+---------+--------------+|   Runtime (RDTSC) [s]   |  TSC  | 1. 399020e-05 || UOPS_DISPATCHED_PORT_PORT_1 |  PMC0 |    21010 || UOPS_DISPATCHED_PORT_PORT_2 |  PMC1 |    11035 || UOPS_DISPATCHED_PORT_PORT_3 |  PMC2 |    11169 || UOPS_DISPATCHED_PORT_PORT_5 |  PMC3 |    12097 ||   INSTR_RETIRED_ANY   | FIXC0 |    64462 ||  CPU_CLK_UNHALTED_CORE  | FIXC1 |    36128 ||   CPU_CLK_UNHALTED_REF  | FIXC2 |    63224 |+-----------------------------+---------+--------------+Notice, that again I was able to specify the counters I want to measure using -g option. Keeping in mind that we did 10000 iterations, results somewhat match with uarch-bench (see above). Sources and output of this experiment can be found on my github. Benchmarking using assembly instructions: In theory it is possible to write the benchmark and collect performance counters yourself using special assembly instructions. It might be useful, for example, on bare metal systems. I haven’t tried it myself but if someone decides to go that road here is some links to start with:  How to Benchmark Code Execution Times on Intel® IA-32 and IA-64 Instruction Set Architectures How to read performance counters by rdpmc instruction?"
    }, {
    "id": 26,
    "url": "https://dendibakh.github.io/blog/2018/03/21/port-contention",
    "title": "Understanding CPU port contention.",
    "body": "2018/03/21 - Contents:  Utilizing full capacity of the load instructions Performance counters that I use Results Why we have 8 cycles per iteration? Some explanations for this pipeline diagram Utilizing other available ports in parallel Overutilizing ports Additional resourcesI continue writing about performance of the processors and today I want to show some examples of issues that can arise in the CPU backend. In particular today’s topic will be CPU ports contention. Modern processors have multiple execution units. For example, in SandyBridge family there are 6 execution ports:  Ports 0,1,5 are for arithmetic and logic operations (ALU).  Ports 2,3 are for memory reads.  Port 4 is for memory write. Today I will try to stress this side of my IvyBridge CPU. I will show when port contention can take place, will present easy to understand pipeline diagramms and even try IACA. It will be very interesting, so keep on reading!  Disclaimer: I don’t want to describe some nuances of IvyBridge achitecture, but rather to show how port contention might look in practice. Utilizing full capacity of the load instructions: In my IvyBridge CPU I have 2 ports for executing loads, meaning that we can schedule 2 loads at the same time. Let’s look at first example where I will read one cache line (64 B) in portions of 4 bytes. So, we will have 16 reads of 4 bytes. I make reads within one cache-line in order to eliminate cache effects. I will repeat this 1000 times: max load capacity ; esi contains the beginning of the cache line; edi contains number of iterations (1000). loop:mov eax, DWORD [esi] mov eax, DWORD [esi + 4]mov eax, DWORD [esi + 8] mov eax, DWORD [esi + 12] mov eax, DWORD [esi + 16] mov eax, DWORD [esi + 20] mov eax, DWORD [esi + 24] mov eax, DWORD [esi + 28] mov eax, DWORD [esi + 32] mov eax, DWORD [esi + 36] mov eax, DWORD [esi + 40] mov eax, DWORD [esi + 44] mov eax, DWORD [esi + 48] mov eax, DWORD [esi + 52] mov eax, DWORD [esi + 56]mov eax, DWORD [esi + 60]  dec edijnz . loopI think there will be no issue with loading values in the same eax register, because CPU will use register renaming for solving this write-after-write dependency. Performance counters that I use:  UOPS_DISPATCHED_PORT. PORT_X - Cycles when a uop is dispatched on port X.  UOPS_EXECUTED. STALL_CYCLES - Counts number of cycles no uops were dispatched to be executed on this thread.  UOPS_EXECUTED. CYCLES_GE_X_UOP_EXEC - Cycles where at least X uops was executed per-thread. Full list of performance counters for IvyBridge can be found here. Results: I did my experiments on IvyBridge CPU using uarch-bench tool.            Benchmark  Cycles  UOPS. PORT2  UOPS. PORT3  UOPS. PORT5       max load capacity  8. 02   8. 00     8. 00     1. 00 We can see that our 16 loads were scheduled equally between PORT2 and PORT3, each port takes 8 uops. PORT5 takes MacroFused uop appeared from dec and jnz instruction. The same picture can be observed if use IACA tool (good explanation how to use IACA): Architecture - IVBThroughput Analysis Report--------------------------Block Throughput: 8. 00 Cycles    Throughput Bottleneck: Backend. PORT2_AGU, Port2_DATA, PORT3_AGU, Port3_DATAPort Binding In Cycles Per Iteration:-------------------------------------------------------------------------| Port | 0  - DV | 1  | 2  - D  | 3  - D  | 4  | 5  |-------------------------------------------------------------------------| Cycles | 0. 0  0. 0 | 0. 0 | 8. 0  8. 0 | 8. 0  8. 0 | 0. 0 | 1. 0 |-------------------------------------------------------------------------N - port number or number of cycles resource conflict caused delay, DV - Divider pipe (on port 0)D - Data fetch pipe (on ports 2 and 3), CP - on a critical pathF - Macro Fusion with the previous instruction occurred| Num Of |       Ports pressure in cycles        |  || Uops | 0 - DV | 1 | 2 - D | 3 - D | 4 | 5 |  |---------------------------------------------------------------------|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0x4]|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp+0x8]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0xc]|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp+0x10]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0x14]|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp+0x18]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0x1c]|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp+0x20]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0x24]|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp+0x28]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0x2c]|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp+0x30]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0x34]|  1  |      |   | 1. 0  1. 0 |      |   |   | CP | mov eax, dword ptr [rsp+0x38]|  1  |      |   |      | 1. 0  1. 0 |   |   | CP | mov eax, dword ptr [rsp+0x3c]|  1  |      |   |      |      |   | 1. 0 |  | dec rdi|  0F  |      |   |      |      |   |   |  | jnz 0xffffffffffffffbeTotal Num Of Uops: 17Why we have 8 cycles per iteration?: On modern x86 processors load instruction takes at least 4 cycles to execute even the data is in the L1-cache. Although according to Agner’s instruction_tables. pdf it has 2 cycles latency. Even if we would have latency of 2 cycles we would have (16 [loads] * 2 [cycles]) / 2 [ports] = 16 cycles. According to this calculations we should receive 16 cycles per iteration. But we are running at 8 cycles per iteration. Why this happens? Well, like most of execution units, load units are also pipelined, meaning that we can start second load while first load is in progress on the same port. Let’s draw a simplified pipeline diagram and see what’s going on. This is simplified MIPS-like pipeline diagram, where we usually have 5 pipeline stages:  F(fetch) D(decode) I(issue) E(execute) or M(memory operation) W(write back)It is far from real execution diagram of my CPU, however, I preserved some important constraints for IvyBridge architecture (IVB):  IVB front-end fetches 16B block of instructions in a 16B aligned window in 1 cycle.  IVB has 4 decoders, each of them can decode instructions that consist at least of a single uop.  IVB has 2 pipelined units for doing load operations. Just to simplify the diagrams I assume load operation takes 2 cycles. M1 and M2 stage reflect that in the diagram. It just need to be said that I omitted one important constraint. Instructions always retire in program order, in my later diagrams it’s broken (I simply forgot about it when I was making those diagrams). Drawing such kind of diagrams usually helps me to understand what is going on inside the processor and finding different sorts of hazards. Some explanations for this pipeline diagram:  In first cycle we fetch 4 loads. We can’t fetch LOAD5, because it doesn’t fit in the same 16B aligned window as first 4 loads.  In second cycle we were able to decode all 4 fetched instructions, because they all are single-uop instructions.  In third cycle we were able to issue only first 2 loads. One of such load goes to PORT2, the second goes to PORT3. Notice, that LOAD3 and LOAD4 are stalled (typically waiting in Reservation Station).  Only in cycle #4 we were able to issue LOAD3 and LOAD4, because we know M1 stages will be free to use in next cycle. Continuing this diagram further we could see that in each cycle we are able to retire 2 loads. We have 16 loads, so that explains why it takes only 8 cycles per iteration. I made additional experiment to prove this theory. I collected some more performance counters:           Benchmark  Cycles  CYCLES_GE_3_UOP_EXEC  CYCLES_GE_2_UOP_EXEC  CYCLES_GE_1_UOP_EXEC       max load capacity  8. 02   1. 00          8. 00          8. 00 Results above show that in each of 8 cycles (that it took to execute one iteration) at least 2 uops were issued (two loads issued per cycle). And in one cycle we were able to issue 3 uops (last 2 loads + dec-jnz pair). Conditional branches are executed on PORT5, so nothing prevents us from scheduling it in parrallel with 2 loads. What is even more interesting is that if we do simulation with assumption that load instruction takes 4 cycles latency, all the conclusions in this example will be still valid, because the throughput is what matters (as Travis mentioned in his comment). There will be still 2 retired load instructions each cycle. And that would mean that our 16 loads (inside each iteration) will retire in 8 cycles. Utilizing other available ports in parallel: In the example that I presented, I’m only utilizing PORT2 and PORT3. And partailly PORT 5. What does that mean? Well, it means that we can schedule instructions on another ports in parrallel with loads just for free. Let’s try to write such an example. I added after each pair of loads one bswap instruction. This instruction reverses the byte order of a register. It is very helpful for doing big-endian to little-endian conversion and vice-versa. There is nothing special about this instruction, I just chose it because it suites best to my experiments. According to Agner’s instruction_tables. pdf bswap instruction on a 32-bit register is executed on PORT1 and has 1 cycle latency. max load capacity + 1 bswap ; esi contains the beginning of the cache line; edi contains number of iterations (1000). loop:mov eax, DWORD [esi] mov eax, DWORD [esi + 4]bswap ebxmov eax, DWORD [esi + 8] mov eax, DWORD [esi + 12] bswap ebxmov eax, DWORD [esi + 16] mov eax, DWORD [esi + 20] bswap ebxmov eax, DWORD [esi + 24] mov eax, DWORD [esi + 28] bswap ebxmov eax, DWORD [esi + 32] mov eax, DWORD [esi + 36] bswap ebxmov eax, DWORD [esi + 40] mov eax, DWORD [esi + 44] bswap ebxmov eax, DWORD [esi + 48] mov eax, DWORD [esi + 52] bswap ebxmov eax, DWORD [esi + 56]mov eax, DWORD [esi + 60]  bswap ebxdec edijnz . loopHere are the results for such experiment:           Benchmark  Cycles  UOPS. PORT1  UOPS. PORT2  UOPS. PORT3  UOPS_PORT5  max load capacity + 1 bswap  8. 03   8. 00     8. 01     8. 01     1. 00First observation is that we get 8 more bswap instructions just for free (we are running still at 8 cycles per iteration), because they do not contend with load instructions. Let’s look at the pipeline diagram for this case: We can see that all bswap instructions nicely fit into the pipeline causing no hazards. Overutilizing ports: Modern compilers will try to schedule instructions for particular target architecture to fully utilize all execution ports. But what happens when we try to schedule too much instruction for some execution port? Let’s see. I added one more bswap instruction after each pair of loads: port 1 throughput bottleneck ; esi contains the beginning of the cache line; edi contains number of iterations (1000). loop:mov eax, DWORD [esi] mov eax, DWORD [esi + 4]bswap ebxbswap ecxmov eax, DWORD [esi + 8] mov eax, DWORD [esi + 12] bswap ebxbswap ecxmov eax, DWORD [esi + 16] mov eax, DWORD [esi + 20] bswap ebxbswap ecxmov eax, DWORD [esi + 24] mov eax, DWORD [esi + 28] bswap ebxbswap ecxmov eax, DWORD [esi + 32] mov eax, DWORD [esi + 36] bswap ebxbswap ecxmov eax, DWORD [esi + 40] mov eax, DWORD [esi + 44] bswap ebxbswap ecxmov eax, DWORD [esi + 48] mov eax, DWORD [esi + 52] bswap ebxbswap ecxmov eax, DWORD [esi + 56]mov eax, DWORD [esi + 60]  bswap ebxbswap ecxdec edijnz . loopWhen I measured result using uarch-bench tool here is what I received:           Benchmark  Cycles  UOPS. PORT1  UOPS. PORT2  UOPS. PORT3  UOPS_PORT5 port 1 throughput bottleneck  16. 00  16. 00    8. 01     8. 01     1. 00To understand why we now run at 16 cycles per iteration, it’s best to look at the pipeline diagram again: Now it’s clear to see that we have 16 bswap instructions and only one port that can handle this kind of instructions. So, we can’t go faster than 16 cycles in this case, because IVB processor executes them sequentially. Different architectures might have more ports to handle bswap instructions which may allow them to run faster. By now I hope you understand what port contention is and how to reason about such issues. Know limitations of your hardware! Additional resources: More detailed information about execution ports of your processor can be found in Agner’s microarchitecture. pdf and for Intel processors in Intel’s optimization manual. All the assembly examples that I showed in this article are available on my github. UPD 23. 03. 2018 Several people mentioned that load instructions can’t have 2 cycles latency on modern Intel Architectures. Agner’s tables seems to be not accurate there. I will not redo the diagrams as it will be difficult to understand them, and they will shift the focus from the actual thing I wanted to explain. Again, I didn’t want to reconstruct how the pipeline diagram will look in reality, but rather to explain the notion of port contention. However, I totally accept the comment and it should mentioned. But also if we assume that load instruction takes 4 cycles latency in those examples, all the conclusions in the post are still valid, because the throughput is what matters (as Travis mentioned in his comment). There will be still 2 retired load instructions per cycle. Another important thing to mention is that hyperthreading helps utilize execution “slots”. See more details in HackerNews comments. "
    }, {
    "id": 27,
    "url": "https://dendibakh.github.io/blog/2018/03/12/Embo_2018_trip_report",
    "title": "Embo 2018 trip report.",
    "body": "2018/03/12 - It was a great pleasure to be with embo 2018. : I really enjoyed being on embo++ conference this year. It was 2nd edition which was held in Bochum(Germany) from 8-12 March 2018. Everything was well organized starting from picking every attendee from the underground station to the quality and amount of food and drinks provided to us.  Also I really want to say big “Thank you!” to all the sponsors for nice gifts, great venue and tasty food. Breaks between the talks were long enough, so there was enough time for networking and just hanging with the nerds. I met cool people that are doing interesting projects: tiny devices with bare metal programming, automotive industry robots with tough real-time constraints, huge telescopes with a lot of precision and math inside them, and so on. Really had a great time just talking to the people. I think that every attendee was interesting enough to spend couple of hours discussing his/her project and ideas. Talks and workshops: The conference was made of 2 main days, pre- and post-event.  There were lightning talks at the pre-event, where I presented how compilers can generate multiple assembly code versions for the single piece of code. This talk was basically the essentials of the series of my posts about vectorization. On this pre-event I want to specifically mention guys (Phillip and Benjamin) from Bochum university who reverse engineered AMD microcode. Here is their paper presented on Usenix 2017 on August 2017. Also the video is available on youtube. Then there was a workshop day. I was on the Rainer Grimm’s (@rainer_grimm) lecture about templates and Kris Jusiak’s (@krisjusiak) workshop about state machines. Even though it does not affects my work right now, it always good to refresh knowledge about C++ templates and learn something interesting about state machines. On the second day there only talks (including mine), so it was more dynamic in a way. I especially liked the talks by:  Ben Craig with proposal about standardizing OS-less version of STL.  Odin Holmes (@odinthenerd) summarizing everything that was presented in the conference.  Niklas Hauser (@salkinium) about some new security features of ARM processors. The talks were scheduled really nicely in my opinion. On both days speakers presented their take on the problems in embedded domain and then Odin summarized everything in his closing keynote. So, you kind of understand how that all fits together. The last day was a study group meeting, where we tried to come up with the future plans. Here is one photo from this: My talk: My talk was about performance analysis and was well received(I guess) although it was not strictly related to embedded programming. It was my first experience speaking at the major conferences, so I was nervous a little bit at the beginning, but suddenly this “nervous switch” was turned off, and everything went fine from there.  Hope to see the recordings and reflect on that soon. :)Slides for my talk are available on my github. Simon Brand (@TartanLlama) noted that we should stop writing “C/C++” which is exactly what I did :) . My talk has title “Dealing with performance analysis in C/C++”. Those two languages are not the same and it’s wrong to mix them. In the end, I didn’t meant to mix them, rather just wanted to attract both C and C++ developers. Actually, my talk was focused even more on assembly than C++. Anyway thanks, Simon, I will try to avoid mixing C and C++ it in the future. Embedded world needs help !!!: Especially people who target bare metal. In most cases they are not allowed to dynamically allocate memory, because heap is not supported on their system. So they just simply can’t use most part of STL, because std::vector, std::string, std::function and so on might allocate. Another “no-no” for the STL is C++ exceptions. On most of such systems they are not supported. So, Ben Craig tries to come up with the proposal to C++ comitee for standardizing OS-less (free-standing) version of STL where such things will be removed. It will help to write portable code for bare metal projects. I wish him good luck as he goes through the standardization process. If you are in embedded domain and want to solve your problems more efficiently and willing to help, reach out to the guys at embo (@emBOconference). They will be really glad for having more hands on this stuff. "
    }, {
    "id": 28,
    "url": "https://dendibakh.github.io/blog/2018/03/09/Store-forwarding",
    "title": "Store forwarding by example.",
    "body": "2018/03/09 - Contents:  Store to load forwarding Experiments Counters that I use Results What else can prevent store forwarding? One more interesting experiment ConclusionIn this post I will discussed another interesting feature of Intel processor that is called store forwarding. Store to load forwarding: In order to describe it I will quote Agner’s Fog microarchitecture. pdf:  The processor can forward a memory write to a subsequent read from the same address under certain conditions. Store forwarding works if a write to memory is followed by a read from the same address when the read has the same operand size. Here is the example of a successful store to load forwarding: mov DWORD [esi], edimov eax, DWORD [esi] In this example the temporary 4-byte store will be kept in Store Buffer without even writing it to L1. Load will take those 4 bytes directly from Store Buffer. But there are some situations where store to load forwarding fails. For example: mov WORD [esi], di  ; small writemov eax, DWORD [esi] ; big read (stall)Experiments: I put all those two examples in a tight loop and microbenchmarked them using uarch-bench tool. successful store forwarding: . loop:mov DWORD [esi + edi * 4], edimov eax, DWORD [esi + edi * 4] dec edijnz . loopbig read after small write: . loop:mov WORD [esi + edi * 4], di  ; small writemov eax, DWORD [esi + edi * 4] ; big read (stall)dec edijnz . loopretCounters that I use: I did my experiments on IvyBridge CPU. The counters that I will show are (details here):  LD_BLOCKS. STORE_FORWARD - Loads blocked by overlapping with store buffer that cannot be forwarded.  UOPS_RETIRED. STALL_CYCLES - Cycles without actually retired uops. Results: The benchmark runs 1000 iterations of this loop and the counters presented below are per iteration:           Benchmark  Cycles   LD_BLOCKS. STORE_FORWARD  UOPS_RETIRED. STALL_CYCLES  successful store forwarding   1. 02   0. 00           0. 02  big read after small write  15. 00   1. 00           14. 00So, here you can see that we are running super fast when nothing prevents store to load forwarding. Everything is nicely pipelined. But when store forwarding failed we run 15 times worse, which is really nasty. The LD_BLOCKS. STORE_FORWARD counter shows us that we have 1 such issue per iteration which results in additional 14 penalty cycles per iteration. But according to Agner’s Fog microarchitecture. pdf on the SandyBridge family the penalty for a failed store forwarding is approximately 12 clock cycles in most cases. But we see 14 cycles penalty. Because load-store reordering is not allowed in x86 (even though store-load is) only one blocked load can execute at a time and perhaps the subsequent stores (on next iterations) are also blocked from committing to preserve memory ordering. I think that explains why why we might have additional 2 cycles penalty, although I’m not 100% sure in that. But also I think that doesn’t mean that the whole pipe is stalled. If you will add lots of math instructions in the loop, they will not be blocked: mov WORD [esi + edi * 4], di  ; small writemov eax, DWORD [esi + edi * 4] ; big read (stall)add ebx, 1           ; not stalledWhat else can prevent store forwarding?: You can have the same effect when load start address is not the same as store start address. Example: mov DWORD [esi], edimov eax, DWORD [esi + 1] ; not the same start address (stall)When I benchmarked this assembly sequence I basically received the same numbers as for “big read after small write” case. The best way to find complete list of things that can prevent store forwarding for particular architecture is to find them in microarchitecture. pdf. One more interesting experiment: I did one more interesting experiment where I tried to hide the store forwarding fail under another store forwarding fail. I did 2 experiments, where I’m just accessing one cache line, writing 2 bytes and reading 4 bytes at a time (store forwarding stall) : full unroll mov WORD [esi], dimov eax, DWORD [esi]mov WORD [esi + 4], dimov eax, DWORD [esi + 4]mov WORD [esi + 8], dimov eax, DWORD [esi + 8]; . . . more stores and loadsmov WORD [esi + 60], dimov eax, DWORD [esi + 60]full interleave mov WORD [esi], dimov WORD [esi + 4], dimov WORD [esi + 8], di; . . . more storesmov WORD [esi + 60], dimov eax, DWORD [esi]mov eax, DWORD [esi + 4]mov eax, DWORD [esi + 8]; . . . more loadsmov eax, DWORD [esi + 60]The difference is that in “full unroll” loads and stores are intermixed, but in “full interleave” I first write to the entire cache line and after that start reading from it. Results:           Benchmark  Cycles  LD_BLOCKS. STORE_FORWARD  UOPS_RETIRED. STALL_CYCLES    cache line: full unroll  235. 00  16. 00           220. 00  cache line: full interleave  29. 00  16. 00           4. 00From this experiment you can see that in “full unroll” case every time we experience store forwarding stall we stop right there. But in the second case we were almost fully able to hide all the store forwarding penalty. But notice that the number of LD_BLOCKS. STORE_FORWARD is the same in both cases. Conclusion: I hope that now you understand what store forwarding is and you will be able to detect issues with it in a real case. "
    }, {
    "id": 29,
    "url": "https://dendibakh.github.io/blog/2018/02/23/MacroFusion-in-Intel-CPUs",
    "title": "MacroFusion in Intel CPUs.",
    "body": "2018/02/23 - Contents:  Example Limitations Micro + Macro Fusion. In my previous post I wrote about MicroFusion which is the thing that happens when multiple uops from the same assembly instruction are fused into one. Another interesting feature of Intel Architecture (IA) that was introduced in Core2 and Nehalem architectures is MacroFusion. It names the situation when uops from different assembly instruction fuse together into one uops. Description of it can be found in microarchitecture manual by Agner Fog:  The decoders will fuse arithmetic or logic instruction with a subsequent conditional jump instruction into a single compute-and-branch µop in certain cases. The compute-and-branch µop is not split in two at the execution units but executed as a single µop by the branch unit. This means that macro-op fusion saves bandwidth in all stages of the pipeline from decoding to retirement. Example: . loop:dec rdijnz . loopretThere is not much useful work done inside this assembly, but it it the easiest example of MacroFusion. We just decrement rdi on each iteration and when it reaches 0, just exit the loop and return. As before I did my experiments on Ivy Bridge processor using uarch-bench tool: Benchmark      Cycles  INSTRUCTIONS_RETIRED  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLdec + jnz      1. 02   2. 00          1. 00            1. 00The counters I mentioned above:  UOPS_RETIRED. RETIRE_SLOTS - Counts the number of retirement slots used each cycle. (fused domain) UOPS_RETIRED. ALL - Counts the number of micro-ops retired. (unfused domain)You can find more detailed description of them in my MicroFusion post. As we can see that the number of instructions retired at each cycle is 2. But they are fused in the decoders into one uop, which is executed as fused. We can state this because number of uops retired is the same in fused and unfused domains. Limitations: There is number of limitations which varies across different architectures. For example, if you put nop in between it will break MacroFusion: . loop:dec rdinopjnz . loopretMeasurements: Benchmark      Cycles  INSTRUCTIONS_RETIRED  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLdec + nop + jnz   1. 02   3. 00          3. 00            3. 00Here we can see that no MacroFusion happens in this case. This limitation is valid even for Skylake architecture. I will quote Agner here:  The programmer should keep fuseable arithmetic instructions together with a subsequent conditional jump rather than scheduling other instructions in-between. I will not mention other limitations, it’s best to read about them in microarchitecture manual (just search for MacroFusion). Micro + Macro Fusion. : It is possible to have micro-op and macro-op fusion at the same time: . loop:add rsi, 4cmp DWORD [rsi], edijnz . loopretThis code is searching for a value edi in an array that is indexed by rsi. I’m calling this assembly function like that: int a[1024];for (int i = 0; i &lt; 1024; ++i){ a[i] = i;}// according to x86 calling conventions first two arguments // will land in rdi and rsi respectively. benchmark_func(1024, a);In this example cmp instruction effectively does a load and compare operations, but due to Microfusion those uops are fused into one. Moreover, this uop is macro-fused with jnz instruction. Benchmark      Cycles  INSTRUCTIONS_RETIRED  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLmicro + macro    1. 10   3. 00          2. 00            3. 00Here we can see that we have 2 uop in fused domain: inc and micro-macro-fused. But later this micro-macro-fused uop was split at the execution unit, resulting in total 3 uops in unfused domain. "
    }, {
    "id": 30,
    "url": "https://dendibakh.github.io/blog/2018/02/15/MicroFusion-in-Intel-CPUs",
    "title": "MicroFusion in Intel CPUs.",
    "body": "2018/02/15 - Contents:  MicroFusion Why do we want MicroFusion? Fused/unfused domain Example 1: double fusion Example 2: half fusion Example 3: no fusion Unlamination Unlamination example 1 Unlamination example 2 Unlamination example 3 Resources:My previous post about Instruction Fusion spawned lots of comments. What I really wanted to benchmark was fused assembly instructions, but it turned out that some other microarchitectural features were involved in that example, which I was not aware about. With the help of Travis Downs and others on HackerNews I did more investigation on this and I want to summarize it in this post. This post is not intended to cover all possible issues one can face. I rather want to present the high-level concept here. I did experiments only on IvyBridge architecture. In the end of the article I provide the links where you can find more details for particular architecture. MicroFusion: There is a nice explanation of MicroFusion in Agner’s manual #3: The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers Chapter 7. 6 (search for chapters with the same name for later architectures):  In order to get more through these bottlenecks, the designers have joined some operations together that were split in two μops in previous processors. They call this μop fusion.  The μop fusion technique can only be applied to two types of combinations: memory write operations and read-modify operations.  ; Example 7. 2. Uop fusion mov  [esi], eax        ; 1 fused uop add  eax, [esi]        ; 1 fused uop add  [esi], eax        ; 2 single + 1 fused uopWhy do we want MicroFusion?: Again, I will better quote Agner here:  μop fusion has several advantages:    Decoding becomes more efficient because an instruction that generates one fused μop can go into any of the three decoders while an instruction that generates two μops can go only to decoder D0.   The load on the bottlenecks of register renaming and retirement is reduced when fewer μops are generated.   The capacity of the reorder buffer (ROB) is increased when a fused μop uses only one entry.  Fused/unfused domain: In order to understand all the benchmarks and the performance counters we need to know about fused and unfused domain.  Fused domain indicates the number of uops generated by the decoders (including DSB). Fused uops count as one. Execution units can’t execute fused uops, so such uops are split at the execution stage in the pipeline. Unfused domain indicates the number of uops that go to each execution port. Also there is description of this in the Agner’s instruction_tables. pdf, for example in Ivy Bridge section. Let’s look at the example for instruction add DWORD [rsp + 4], 1 in this manual for Ivy Bridge: We can see here that there are 2 fused uops and 4 unfused uops (1+2+1). It means that after decoding 2 fused uops were produced. But on the execution stage they were split into 4 uops, 1 of which can go to either p0 (port 0), p1 or p5, 2 of them go to p2 or p3, and 1 go to p4. In this post I will try to confirm this example (and more), but to do that we need to know which counters to use:  Fused domain: IDQ. DSB_UOPS and UOPS_RETIRED. RETIRE_SLOTS Unfused domain: UOPS_RETIRED. ALLDescription of the counters mentioned can be found here.  IDQ. DSB_UOPS - Number of uops delivered to IDQ from DSB path.  UOPS_RETIRED. RETIRE_SLOTS - Counts the number of retirement slots used each cycle.  UOPS_RETIRED. ALL - Counts the number of micro-ops retired. As of my knowledge, if the instruction stays fused in the Retire Unit we can count retired instruction both in fused and unfused domain. But if it is split in Retire Unit, those two counters will show the same number. If you don’t understand it - keep on reading, hope it will be clear when we will discuss un-lamination. I did all experiments on Intel Core i3-3220T (Ivy bridge), however most of the examples will behave similar on later architectures (will be specifically mentioned). Example 1: double fusion: add   DWORD [rsp + 4], 1Full code of the assembly function that I benchmarked can be found here. See also microbenchmarks integrated into uarch-bench. Benchmark      Cycles  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLadd [esp], 1    1. 10   2. 08            4. 08No surprises in those results. We can observe the same behavior as shown in Agner’s tables. Decoded RMW instruction was decoded into 2 fused uops: load + op and write_addr + write_data. Number of unfused uops is 4, which also matches the results. Example 2: half fusion: But let’s just change add [mem], 1 to inc [mem] and see what happens. inc   DWORD [rsp + 4]Full assembly function, uarch-bench tests. Benchmark      Cycles  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLinc [esp]      1. 12   3. 08            4. 08Well, here we can see that there is only one fusion. I don’t know for sure which pair does not fuse, see more discussion on this on HackerNews discussion, comment by BeeOnRope. Agner’s tables also confirm that (notice, the number of uops in fused domain is 3): The same behavior holds for Skylake architecture. Example 3: no fusion: Often times complex addressing mode (with multiple inputs) can cause no fusion to happen: add   DWORD [rsp + rcx + 4], 1Full assembly function. See also microbenchmarks integrated into uarch-bench. Benchmark      Cycles  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLadd [esp + ecx], 1 1. 40   4. 18            4. 20The measurements show that number of uops in fused and unfused domains are the same. You can find broader answer on StackOverflow, in particular:  SnB (and I assume also IvB): indexed addressing modes are always un-laminated, others stay micro-fused. Unlamination: In order to show the case of un-lamination we will use the IDQ. DSB_UOPS counter, which also “operates” in fused domain. Unlamination for SandyBridge is described in Intel® 64 and IA-32 Architectures Optimization Reference Manual in chapter “2. 3. 2. 4: Micro-op Queue and the Loop Stream Detector (LSD)”:  The micro-op queue provides post-decode functionality for certain instructions types. In particular, loads combined with computational operations and all stores, when used with indexed addressing, are represented as a single micro-op in the decoder or Decoded ICache. In the micro-op queue they are fragmented into two micro-ops through a process called un-lamination, one does the load and the other does the operation Also I found very clear explanation on HackerNews thread. Especially, this note by BeeOnRope:  When instructions are fused at decode, but are “unlaminated” before rename, it usually has similar performance to no fusion at all (but it does save space in the uop cache), since RAT is more likely to be a performance limitation. Unlamination example 1: I tried to put the instruction from the Intel optimization manual (section 2. 3. 2. 4) in the tight loop and benchmark it: . loop:add   ebx, DWORD [rsp + rdi * 4 - 4]dec   rdijnz . loopFull assembly function. Note, that now we are not measuring just this one add instruction, but instead the whole loop (showed below). Benchmark       Cycles  IDQ. DSB_UOPS  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLadd ebx, [esp + edx]  1. 20   2. 13      3. 27            3. 30Let me explain what is going on here. On each iteration 2 instructions were delivered from DSB to IDQ. They are one fused uop from add instruction and one uop macro-fused from dec and jnz. See explanation of MacroFusion in the Intel Optimization Manual, section “2. 3. 2. 1 Legacy Decode Pipeline”. However, micro-fused uop (from add) then was un-laminated (probably at IDQ) and further executed as two separate uops. That’s why we see the same number of uops retired for fused and unfused domain. To understand the reason for this behavior, again, I will quote this great StackOverflow answer:  Why SnB-family un-laminates:  Sandybridge simplified the internal uop format to save power and transistors (along with making the major change to using a physical register file, instead of keeping input / output data in the ROB). SnB-family CPUs only allow a limited number of input registers for a fused-domain uop in the out-of-order core. For SnB/IvB, that limit is 2 inputs (including flags). For HSW and later, the limit is 3 inputs for a uop. In this example add ebx, DWORD [rsp + rdi * 4 - 4] has 4 inputs: 3 registers and 1 FLAGS register. But the limit for SnB family is 2, that’s why it un-laminates in IDQ. I think Agner’s instruction tables make no distinction between “full fusion” and “un-lamination”, so you should be aware about that. The number in the fused domain column always provide optimistic case, when no un-lamination happens. Unlamination example 2: The same basically happens for RMW instruction: . loop:add   DWORD [rsp + rdi * 4 - 4], 1dec   rdijnz . loopFull assembly function. Benchmark         Cycles  IDQ. DSB_UOPS  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLadd DWORD[rsp + rdi], 1  1. 47   3. 07      5. 13            5. 13In this example add DWORD [rsp + rdi * 4 - 4], 1 has 3 inputs: 2 registers and 1 FLAGS register. Again, the limit is exceeded. Both 2 uops from add are fused in the DSB, but then they are un-laminated. I haven’t tested this on HSW and later architectures, but I assume there should be no un-lamination. Unlamination example 3: To avoid un-lamination I tried to use simple addressing mode as below: . loop:add   DWORD [rcx], 1add	rcx, 4dec	rdijnz . loopFull assembly function. Benchmark         Cycles  IDQ. DSB_UOPS  UOPS_RETIRED. RETIRE_SLOTS  UOPS_RETIRED. ALLadd DWORD[rcx] + add rcx 1. 47   4. 07      4. 30            6. 33Now there is no un-lamination happening, however the amount of executed uops is increased by one (this uop comes from add rcx, 4, which was ). Resources::  Intel® 64 and IA-32 Architectures Optimization Reference Manual section 2. 3. 2 “The Front End”.  Agner Fog: The microarchitecture of Intel, AMD and VIA CPUs: An optimization guide for assembly programmers and compiler makers.  Agner Fog: Instruction tables: Lists of instruction latencies, throughputs and micro-operation breakdowns for Intel, AMD and VIA CPUs Stack-overflow answer. "
    }, {
    "id": 31,
    "url": "https://dendibakh.github.io/blog/2018/02/04/Micro-ops-fusion",
    "title": "Microbenchmarking fused instruction.",
    "body": "2018/02/04 - Contents:  My story Fusion features in x86 Benchmark Conclusion     UPD 05. 02. 2018:   UPD 09. 02. 2018:   Let me start this post with a question: “Do you think number of executed(retired) instructions is a good metric for measuring performance of your application?”. Well, it is a decent proxy, but not an one-to-one match to the timings of the benchmark. And in this post I will show when it can be the case. My story: One day I was dealing with performance degradation in some benchmark. I immediately spotted lots of differences in assembly code between “good” and “bad” cases. For the same C++ source code: // for loop with induction variable i and array aa[i]++;for “good” case there was an assembly like the following: inc DWORD [&lt;memory address&gt;]and for “bad” case there was assembly like that: mov edx, DWORD [&lt;memory address&gt;]inc edxmov DWORD [&lt;memory address&gt;], edxSo, in the latter snippet it’s basically the same instruction but split in 3 simpler instructions. Performance delta was not that big: around 5%. I checked profiles - no other significant difference, besides… Number of instructions retired in “good” case was ~50% lower than in the “bad” case. Similar patterns can be observed in many different places in the hot path of the benchmark. At that point I considered: “problem solved, there is no sense in splitting instructuions like that”. Or rather say, not fusing them. I thought that some pass in the code generation phase failed at combining 3 simple instructions into a fused one. I revisited that case after a few days when my colleague pointed out to me that this shoudn’t be the source of the problem. I did more experiments just to find out that it was yet another code alignment problem (check out my recent post on this topic). With adding one of the code alignment options to the compilation yielded the same performance for both cases. Fusion features in x86: If we look closer at the fused instruction it actually consists of a three operations: load from memory, increments the value and store it back. There is no magic here, CPU will execute those operations either way. Before I present the benchmark I want to say a few words about fusion features that exist in Intel Architecture Front End starting from “Sandy Bridge”. Execution engine (back-end) inside the cpu can only execute so-called “micro-ops” (uops), that were provided by the front-end. So, back-end can’t execute fused instruction but only a simple ones. There are some limitations to which operations can be fused and which not, more about this feature you can read in Intel® 64 and IA-32 Architectures Optimization Reference Manual, section “2. 4. 2. 1 Legacy Decode Pipeline”. Please do not be confused about the difference between InstructionFusion, MicroFusion and MacroFusion. According Intel documentation:  InstructionFusion is when multiple RISC-like assembly instructions are merged into CISC-like one assembly instruction (see example above). This is made by the compiler / asm developer.  MicroFusion is when multiple uops from the same assembly instruction are merged into one uop. This is made by the decoding pipeline inside CPU.  MacroFusion is when multiple uops from different assembly instructions are merged into one uop. This is made by the decoding pipeline inside CPU. Benchmark: I decided to use uarch-bench for my experiments as it allows quite precise collection of performance counters for the snippet of assembly you provide. Here is the difference in assembly for the two benchmarks I ran:Those two assembly functions (fused and unfused) take number of iterations as an argument (that ends up in rdi, see x86 calling conventions). Also they allocate integer array on the stack with the number of elements equal to the number of iterations. In the nutshell this assembly code is equivalent to this C code (number of iterations = 1024): void fused(/*int iters = 1024*/){ int a[1024]; for (int i = 0; i &lt; 1024; ++i) {  a[i]++; }}I ran two benchmarks on my home Intel Core i3-3220T (Ivy bridge). I expect to see similar results on more modern architectures like Haswell and Skylake. Here are the results I received: | Benchmark | Cycles | INSTRUCTIONS_RETIRED | UOPS_ISSUED | UOPS_RETIRED | LSD. UOPS ||-----------|--------|----------------------|-------------|--------------|----------|| fused   | 2. 32  | 3. 05         | 5. 36    | 5. 36     | 4. 80   || unfused  | 2. 32  | 5. 05         | 5. 36    | 5. 36     | 4. 88   |“Cycles” shows how many cycles were executed per one loop iteration. So, essentially, harness calls the function, measures performance counter that you requested and then devides it by the number of iterations. If we do the calculation for INSTRUCTIONS_RETIRED:  fused: INSTRUCTIONS_RETIRED = 3 (function header) + 1024 * 3 (loop) + 2 (function footer) / 1024 = 3. 005 unfused: INSTRUCTIONS_RETIRED = 3 (function header) + 1024 * 5 (loop) + 2 (function footer) / 1024 = 5. 005We can also see that measurement overhead is (3. 05 - 3. 005) * 1024 = ~46 instructions. But if we look at UOPS_ISSUED metric we will see that they are equal. That leads us to the thought that the fused instruction was split inside the decoder into 3 smaller uops. And after that there is no impact on the execution engine, so we have a strong proof why performance of those two cases is on par. One more interesting thing I want to mention is that both of those 2 loops run almost fully out of LSD. In both cases LSD. UOPS is very close to UOPS_ISSUED meaning that LSD recognized the loop after some number of iterations. After that it started feeding back-end with already fetched and decoded uops. But it takes some time for LSD to detect the loop that’s why this number is slightly lower than the total number of issued uops. More information about LSD can be found in Intel® 64 and IA-32 Architectures Optimization Reference Manual, section “2. 4. 2. 4 Micro-op Queue and the Loop Stream Detector (LSD)”. Conclusion: Intel Optimization Manual (that I mentioned several times already) says the following:  Coding an instruction sequence by using single-uop instructions will increases the code size, which can decrease fetch bandwidth from the legacy pipeline. Out of all runs that I did unfused version was never faster then the fused one. Encoding of fused instructions in my example takes 4 bytes, when unfused version takes 10 bytes. Significant win. It also involves different alignment of the code that goes after that instruction which potentially can make a difference in performance going up or down. I was also trying to expose the gain from better utilization of fetch bandwidth, but looks like it’s not so straightforward. I tried manually unrolling the loop and doing other sort of things, but as what I understand in my toy examples latency of the memory operations (even though data should be in the L1-cache) are big enough to hide the inefficiencies in fetch and decode bandwidth. According to Agner Fog’s instruction tables fused load-op-store operation takes 6 cycles. If anyone will have a good example where fused version is significantly faster - please share it with me. Another interesting thing which can cause difference in performance for the cases mentioned in the post is connected with decoders. From the Intel Optimization Manual:  There are four decoding units that decode instruction into micro-ops. The first can decode all IA-32 and Intel 64 instructions up to four micro-ops in size. The remaining three decoding units handle single-micro-op instructions. All four decoding units support the common cases of single micro-op flows including micro-fusion and macro-fusion. However, I haven’t tried to write a microbenchmark for that. Unfused instructions also add register pressure to the compiler, because it needs to find the free register to load the value from the memory. UPD 05. 02. 2018:I found that in the comments that there were lots of confusion in terminology between Instruction fusion, MicroFusion and MacroFusion. I tried to use the same terminology as in Intel documentation. Please see updated “Fusion features in x86” chapter. UPD 09. 02. 2018:Title of the post was changed. I used it by a mistake and it caused a lot of confusion. "
    }, {
    "id": 32,
    "url": "https://dendibakh.github.io/blog/2018/01/25/Code_alignment_options_in_llvm",
    "title": "Code alignment options in llvm.",
    "body": "2018/01/25 - Contents:  align-all-functions align-all-blocks align-all-nofallthru-blocks ConclusionIn my previous post I discussed code alignment issues that could arise when you benchmarking your code. Simon in the comments mentioned code alignment option ‘-align-all-nofallthru-blocks’. If we look at what description says about this option it’s not clear what this option is doing. So, I decided to give some clear examples of what it’s doing. In latest llvm (as of 25. 01. 2018) there are 3 machine-independent option for controling code alignment: -align-all-blocks=&lt;uint&gt;       Force the alignment of all blocks in the function. -align-all-functions=&lt;uint&gt; Force the alignment of all functions. -align-all-nofallthru-blocks=&lt;uint&gt; Force the alignment of all blocks that have no fall-through  predecessors (i. e. don't add nops that are executed). Let’s take an example like this: int foo();int bar();void func(int* a){ for (int i = 0; i &lt; 32; ++i)  a[i] += 1; if (a[0] == 1)  a[0] += foo(); else  a[0] += bar();}For this code compiled with -O2 -march=skylake -fno-unroll-loops clang will produce this assembly: 0000000000000040 &lt;_Z4funcPi&gt;: 40:	push  rbx 41:	mov  rbx,rdi 44:	mov  rax,0xffffffffffffff80 4b:	vpcmpeqd ymm0,ymm0,ymm0 4f:	nop 50:	vmovdqu ymm1,YMMWORD PTR [rbx+rax*1+0x80] 59:	vpsubd ymm1,ymm1,ymm0 5d:	vmovdqu YMMWORD PTR [rbx+rax*1+0x80],ymm1 66:	add  rax,0x20 6a:	jne  50 &lt;_Z4funcPi+0x10&gt; 6c:	cmp  DWORD PTR [rbx],0x1 6f:	jne  7b &lt;_Z4funcPi+0x3b&gt; 71:	vzeroupper  74:	call  79 &lt;_Z4funcPi+0x39&gt; 79:	jmp  83 &lt;_Z4funcPi+0x43&gt; 7b:	vzeroupper  7e:	call  83 &lt;_Z4funcPi+0x43&gt; 83:	add  DWORD PTR [rbx],eax 85:	pop  rbx 86:	ret Note that the loop is already aligned on a 16B boundary. And here is the vizualization (created with this tool) for this assembly where we can see the basic blocks (BB): Below I will show the difference in different code aligning options. All the code and scripts that I used can be found here. align-all-functions: This option will align all your functions on a bounday specified in the parameter. For example, -mllvm -align-all-functions=5 will align all functions on a 32B boundary (2^5=32). Regarding our case (don’t look at the offsets in visual representation) function is already aligned at 64B boundary so, the only difference will be if we specify -mllvm -align-all-functions=7: align-all-blocks: Apply this option carefully because it can cause lot of nops be added into the assembly. Adding -mllvm -align-all-blocks=5 yields this diff:  Note that this option does not align the function beginning, but rather it’s first basic block. I will not show the results of what will happen if I will specify 6 or 7, because it won’t fit on a screen. align-all-nofallthru-blocks: This option as opposed to blindly aligning all blocks does it in a smarter way. The description looks complicated, but in fact it’s really simple. Algorithm looks like this: for each BB we check if a previous BB can reach current BB by falling through. If it can, we don’t align such current, because it means that we will insert NOPs into the executed path (as the opposite to -align-all-blocks). If the previous BB can’t reach current BB by falling through, it means that the only way we can reach current BB is by jumping into it and the previous block ends with unconditional branch, so we can safely insert nops between previous and current BB, knowing that those NOPs won’t be executed. In our function there is only one such BB (that has a call to bar()). Here is the diff for -mllvm -align-all-nofallthru-blocks=5: Again, all the code and scripts that I used can be found here, so free to play with different options. Conclusion: By now I hope it’s clear what those code alignment options mean, but I encourage you to use them with care. The most safe IMHO is the -align-all-nofallthru-blocks, however it also doesn’t come for free - it increases the binary size. "
    }, {
    "id": 33,
    "url": "https://dendibakh.github.io/blog/2018/01/18/Code_alignment_issues",
    "title": "Code alignment issues.",
    "body": "2018/01/18 - Contents:  Introduction Numbers Let’s try to understand why that happens.  Code alignment matters To make things even more funny Collected performance counters Caveats Why not always align? ConclusionsIntroduction: How hard it is to benchmark the simple function like that? // func. cppvoid benchmark_func(int* a){	for (int i = 0; i &lt; 32; ++i)		a[i] += 1;}You just stick it into the microbenchmark, call it enough times to have sustainable results and report it, right?Well, yes we could also check the assembly to make sure nothing was optimized away. We can also collect profiles to see that our loop is the only hot spot in the benchmark. Kind of make sense. We know what we measure. Let’s suppose that in your file there is one more function which you are also benchmarking but separately from the benchmark_func, like that: // func. cppvoid foo(int* a){	for (int i = 0; i &lt; 32; ++i)		a[i] += 1;}void benchmark_func(int* a){	for (int i = 0; i &lt; 32; ++i)		a[i] += 1;}One day your manager comes to you angry with the numbers he received from the customers. The numbers are lower than you measured of course :). Customer is only interested in benchmark_func, so he just sticked only this function into the microbenchmark (original version in the very beginning of the article). Numbers: I compiled this code with recent clang (from 13. 01. 2018) with the options: -O2 -march=skylake -fno-unroll-loops I ran this on the Intel Core i7-6700 Skylake processor. Full code along with building scripts are available here. Note, that you need google benchmark library to compile the code. Let’s take the case with 2 function as a baseline and let’s call the case with only benchmark_func “no_foo”. Here are the results: $ . /baseline. sh ---------------------------------------------------------Benchmark     CPU  Iterations Throughput  Clockticks/iter---------------------------------------------------------func_bench_median 4 ns 191481954  32. 5626GB/s 74. 73$ . /no_foo. sh           ---------------------------------------------------------Benchmark     CPU  Iterations Throughput  Clockticks/iter---------------------------------------------------------func_bench_median 4 ns 173214907  29. 5699GB/s 84. 54I calculated Clockticks/iter metric myself by dividing total number of clockticks for benchmark_func by the number of iterations. Suddenly, just because I removed foo before(!) benchmark_func, performance goes down by ~10%. Let’s try to understand why that happens. : Going ahead a little bit, for all examples that I will show the generated assembly for benchmark_func is identical with the only difference is it’s placement in the binary and internal loop alignment. In order to understand this swing lets look at the dissassembled code for the baseline: $ objdump -d a. out -M intel | grep  &lt;_Z14benchmark_funcPi&gt;:  -A1500000000004046c0 &lt;_Z14benchmark_funcPi&gt;: 4046c0:    48 c7 c0 80 ff ff ff  mov  rax,0xffffffffffffff80 4046c7:    c5 fd 76 c0       vpcmpeqd ymm0,ymm0,ymm0 4046cb:    0f 1f 44 00 00     nop  DWORD PTR [rax+rax*1+0x0] 4046d0:    c5 fe 6f 8c 07 80 00  vmovdqu ymm1,YMMWORD PTR [rdi+rax*1+0x80] 4046d7:    00 00  4046d9:    c5 f5 fa c8       vpsubd ymm1,ymm1,ymm0 4046dd:    c5 fe 7f 8c 07 80 00  vmovdqu YMMWORD PTR [rdi+rax*1+0x80],ymm1 4046e4:    00 00  4046e6:    48 83 c0 20       add  rax,0x20 4046ea:    75 e4          jne  4046d0 &lt;_Z14benchmark_funcPi+0x10&gt; 4046ec:    c5 f8 77        vzeroupper  4046ef:    c3           ret We can see that code is aligned at the i-cache line boundary (0x406c0 mod 0x40 == 0x0), that’s good. However there is something more we need to know about the Intel Architecture (IA) front-end. For Skylake family there is MITE (Micro-instruction Translation Engine) which fetches instructions 16 bytes each cycle. Important note here is that those 16 bytes are always represent 16B aligned window, meaning you can’t fetch instructions from different 16B aligned windows. After we fetch the decoder decodes those instructions into the sequence of smaller operations (uops). And then it feeds them into the rest of the pipeline. But also there is another HW unit called DSB (Decoded Stream Buffer), which is essentially the uops cache. If we want to execute something that was already executed before we first look into the DSB. If it happens to be there we are not fetching it from memory, we feed the back-end with already decoded uops. However there are some constraints about how uops can land in the DSB, we will discuss it further. In the assembly above you can see that code was vectorized and there are only 4 iterations of the loop, which is good for this example, because otherwise the LSD (Loop Stream Detector) will spot the loop and stop fetching instruction from the memory. More information about IA front-end is available in the “Intel 64 and IA-32 Architectures Optimization Reference Manual”. And also I want to share a presentation made by my colleague Zia Ansari from Intel at the LLVM dev meeting 2016. I strongly encourage the reader to take a break now a go watch at least this talk. Code alignment matters: I think you already starting to understand where I’m going with this. So let me show you how benchmark_func is placed in the code for those two cases. baseline: no_foo: Thick boxes in those tables represent 32B aligned windows and I highlighted with yellow instructions which are hot (the body of the loop). First observation is that second layout is better than the baseline, because all the hot code fits directly into one 32B aligned window. And indeed, second case has twice as less DSB misses (DSB_MISS_PS 1800M vs 888M) and exactly 0 DSB-MITE switches penalty (DSB2MITE_SWITCHES,PENALTY_CYCLES 888M vs 0). But why it performes 10% worse? This is probably some other architectural subtle detail that I don’t know about. I made several attempts of proving some hypothesis by predicting how decoded instructions will land in DSB, however I’m not 100% sure if I got it right. My take on this can be found DSB layout. pdf. Profiles and performance counters don’t show any anomaly. But second case is much more front-end bound than the baseline (IDQ_UOPS_NOT_DELIVERED,CYCLES_0_UOPS_DELIV 4100M vs 5200M). I present all the collected counters as well as explanation for them in the end of this post. To make things even more funny: I did 2 more experiments with explicitly specified alignment: -mllvm -align-all-functions=5 and -mllvm -align-all-blocks=5: $ . /aligned_functions. sh ---------------------------------------------------------Benchmark     CPU  Iterations  Throughput  Clockticks/iter---------------------------------------------------------func_bench_median 3 ns 218294614  36. 8538GB/s 63. 37$ . /aligned_blocks. sh      ---------------------------------------------------------Benchmark     CPU  Iterations  Throughput  Clockticks/iter---------------------------------------------------------func_bench_median 3 ns 262104631  44. 3106GB/s 46. 25With aligning benchmark_func at the boundary of 32 bytes I had +13% improvement and by aligning all basic blocks (including the beginning of the function) inside benchmark_func at the 32-byte boundary I had +36% improvement. Funny, ain’t it? Function placement for the case with aligned beginning of the function doesn’t differ much from the baseline:So we have same sort of issues with DSB as we had in the baseline case. Counters show even worse utilization of the DSB: DSB_MISS_PS 2600M vs 1800M. However, it is much less front-end bound withIDQ_UOPS_NOT_DELIVERED,CYCLES_0_UOPS_DELIV 330M vs 4100M. In the end what really matters is how good we can supply back-end with the decoded uops. The case with aligned basic blocks has the following layout:It has good level of DSB utilization as well as the amount of cycles when uops were not delivered to the back-end is very low. See the aggregate table for exact values of the counters. Again I’m sorry for not having the explanation for those things, but maybe I will have it someday and present it in the next posts. Collected performance counters: Once again all the code as well as the building scripts are located here. Note, that you need google benchmark library to compile the code. Description for all the counters for Skylake architecture can be found here:  FRONTEND_RETIRED. DSB_MISS_PS - Counts retired Instructions that experienced DSB (Decode Stream Buffer) miss.  DSB2MITE_SWITCHES. PENALTY_CYCLES - Counts Decode Stream Buffer (DSB)-to-MITE switch true penalty cycles. &lt;…&gt; A Decode Stream Buffer (DSB) hit followed by a Decode Stream Buffer (DSB) miss can cost up to six cycles in which no uops are delivered to the IDQ. Most often, such switches from the Decode Stream Buffer (DSB) to the legacy pipeline cost 0–2 cycles.  IDQ. ALL_DSB_CYCLES_4_UOPS - Counts the number of cycles 4 uops were delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path.  IDQ. ALL_DSB_CYCLES_ANY_UOPS - Counts the number of cycles uops were delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path.  IDQ_UOPS_NOT_DELIVERED. CORE - Counts the number of uops not delivered to Resource Allocation Table (RAT) per thread adding “4 – x” when Resource Allocation Table (RAT) is not stalled and Instruction Decode Queue (IDQ) delivers x uops to Resource Allocation Table (RAT) (where x belongs to {0,1,2,3}).  IDQ_UOPS_NOT_DELIVERED. CYCLES_0_UOPS_DELIV. CORE - Counts, on the per-thread basis, cycles when no uops are delivered to Resource Allocation Table (RAT). IDQ_Uops_Not_Delivered. core =4. Caveats: For this particular case alignment issues go away as we increase the number of iterations, say, to 1024. Here is when LSD kicks in. It recognizes that we are in the loop and executing the same sequence of instructions. So it just shuts down the front-end (so we are not fetching instructions anymore) and start executing out of the LSD. So, it’s no more important how our code is aligned. Another interesting case is that I received -10% drop in performance when using gold linker (-fuse-ld=gold). It’s not that this linker is bad, just the reason is, again, code alignment. I leave this for you if someone will be interested in investigating this. Why not always align?: Aligning the code means compiler will insert NOPs before the code you want to align. That increases binary size and might cost you performance if you insert a lot of nops in the hot path. In the end executing nops doesn’t come for absolutely free. You need to fetch and decode it. Conclusions: As you can see, even with such a small amount of code things may get incredibly complicated. I’m not saying we all should be experts in the hardware we are coding for, but at least be informed about such issues. Don’t take the first measured value as a final one. Collect profiles and check that you wasn’t hit by some architectural performance issue. UPD 23. 01. 2018:  Fixed MITE description (thanks to Travis).  Added clockticks per iteration metric.  Added my prediction of DSB layout. UPD 27. 01. 2018:  @fernzeit on Reddit shared amazing paper “Producing Wrong Data Without Doing Anything Obviously Wrong!”. It shows how linking order may affect performance and even how -O2 can be better than -O3. True gem!"
    }, {
    "id": 34,
    "url": "https://dendibakh.github.io/blog/2017/11/21/Codedive_2017_trip_report",
    "title": "Code::Dive 2017 trip report.",
    "body": "2017/11/21 - This is my third time Code::dive conference and I was really happy to be back to Wroclaw. To be back to this wonderful city, to my Nokia ex-colleagues and friends. This year it was again 2-day conference with 4 tracks and two 90-minute and three 60-minute session per day. As before this conference is completely free and open for everyone (you just need to register on time), so I’m really thankful to Nokia for such a great event! This time the focus was made not only on C++ but also on other languages like Rust and Go. There were also some amount of talks about security, embedded, IoT and DevOps. Speaking of speakers: we had John Lakos, Eric Niebler, Mark Isaacson (Facebook), Alex Crichton (Mozilla) and others. All videos will be on youtube and codedive. pl within couple of weeks I hope. The talks that I enjoyed the most are:  Andrzej Krzemieński (@akrzem9) - Faces of undefined behavior.  Mateusz Pusz (@mateusz_pusz) - Striving for ultimate low latency.  Alex Crichton - Intro to Rust. There was a cool performance by Mark Isaacson and the whole audience on his “Developing C++ @ Facebook scale” talk. There was a rain simulation by the people in the room and by Mark. You should definetly go see it when the video will be published. Nice surprise was the escape room, prepared by the Sławomir Zborowski and others. I didn’t manage to try it, but what I heard from others - it was quite hard, but lots of fun. Best quote from the conference for me was by Mark Isaacson from his “Exploring C++17 and Beyond” talk. He said: “We should not like metaprogramming”. And the context was that people tend to use dirty metaprogramming tricks because they didn’t have better choice. Now with c++14 and c++17 it’s getting more and more easier to do compile-time computations without going into hacks. And I want to point out one thing that I didn’t like. There were a slots with only one c++ talk. And even if it was not a world-famous speaker, still there was a “full house” on this talk. Just because there was no other alternatives. I was lucky to enter the room for all the talks I wanted, but I know people that weren’t. Now I want to share my insights from the conference How UB is connected with opt level: The talk by Andrzej about UB was great. First of all, he showed why we should not do defensive checking against UB, thus widening the contract of our function: int foo(int x){ return x + 1; // potential integer overflow}We shouldn’t insert check in our function because it will block static analyzers (clang in particular) from detecting real bugs on caller side. What we should do is fix a bug on a caller side instead. Andrzej showed us really simple example of UB: int foo(){  int* p = nullptr;  return *p;}With -O0 clang just inserted the dereferencing of null pointer in our machine code, but starting from -O1 this function has just one instruction: ret. So, clang assumed that UB can’t happen and just optimized it away. What got me interested during this talk was “How UB detection is connected with optimization level?” After I came back from the conference I experimented with different examples a little bit (also asked llvm-dev community) and found out that front-end (clang) is rather light-weight in detecting UB, and all the cool stuff is done inside middle- and back-end (llvm). Optimization of UBs is spread across different passes, so there is no single pass, dedicated for exactly this purpose. You can check the sequence of passes for different opt levels here. The full thread on llvm-dev mailing list can be found here. Loop interchange in modern compilers: Another great talk was done by Mateusz Pusz about what you should do when you want to optimize for low latency, not the throughput. And he showed one simple example of a not-cache-friendly loop: void foo(int** a, int** b){  for (int i = 0; i &lt; N; ++i)    for (int j = 0; j &lt; N; ++j)      a[j][i] += b[j][i];}I was wondering if compilers can interchange the loop to make it cache-friendly again. So I did some experiments at home and asked clang/gcc compiler devs about that. The results I received is that they are capable of doing that in general, but clang and gcc are missing proper data analysis to make this transformation. Specifically for clang there is special opt pass which is not enabled by default, you should pass -mllvm -enable-loopinterchange explicitly. GCC is also making this transformation if it can prove that a and b do not alias, and even vectorize this loop which is quite optimal. Full tread on llvm-dev mailing list can be found here. That’s all for year 2017. Hope to be on CodeDive again in 2018! "
    }, {
    "id": 35,
    "url": "https://dendibakh.github.io/blog/2017/11/10/Tips_for_writing_vectorizable_code",
    "title": "Vectorization part7. Tips for writing vectorizable code.",
    "body": "2017/11/10 - This post is wrapping up the series. We just saw some really simple examples when vectorization either happens or not. But usually you have more complicated code. What to do in this case, how make use of vectorization capabilities of your CPU? To best answer this question I want to highlight the typical reasons for not vectorized code and guidlines for writing vectorizable code. Typical reasons for loop not being vectorized. :  Low trip count Not Inner Loop Existence of vector dependence Vectorization possible but seems inefficient Condition may protect exception Data type unsupported Subscript too complex Unsupported loop structure Statement inside the loop unsuited for vectorizationGeneral tips for writing vectorizable code. :  Favor simple for loops Write straight line code. Avoid:     Function calls   Branches that cannot be treated as masked assignments    Avoid dependencies between loop iterations     Avoid read-after-write dependencies    Prefer array notation to the use of pointers     Or provide help for compiler to understand   Try to use the loop index directly in array subscripts, instead of incrementing a separate counter for use as an array address    Use efficient memory addresses     Favor inner loops with unit stride   Minimize indirect addressing    Align your data where possible to some boundary (32 bytes in case of AVX)However, the main advice is: see compiler opt reports to understand what compiler did for you. If you measured and your code stil Other resources: Some items from the two checklists below were taken from Intel Compiler Autovectorization Guide. I really recommend it, even though it is slightly outdated. Specifically I want to point out that compilers can do all sorts of loop transformations to make vectorization possible. I recommend to at least familiarize yourself with the basic loop transformations. For example, compiler can perform some of them if it will help to eliminate some loop dependency. Doing so will enable vectorization. This is really nice article with lots of examples: Crunching numbers with AVX and AVX2. It is a good guide if you want to try out writing vector intrinsics. This post has nice pictures of how some particular hardware instruction works. Vectorization codebook has rather high-level view for the topic with links to the more detailed documents. All posts from this series::  Vectorization intro.  Vectorization warmup.  Checking compiler vectorization report.  Vectorization width.  Multiversioning by data dependency.  Multiversioning by trip counts.  Tips for writing vectorizable code (this article). "
    }, {
    "id": 36,
    "url": "https://dendibakh.github.io/blog/2017/11/09/Multiversioning_by_trip_counts",
    "title": "Vectorization part6. Multiversioning by trip counts.",
    "body": "2017/11/09 - In this post we will dig deep into the different type of multiversioning. This time we will look at creating multiple versions of the same loop that have different trip counts. If you haven’t read part 4: vectorization width yet I encourage you to do that, because we will use knowledge form this post a lot. In the post that I mentioned above I showed how it can be beneficial to optimize your function if you know the data you are working with. Specifically, in the section “Why we care about vectorization width?” I left off on the case when there are two different trip counts and you can’t just optimize for one of it. Let’s get back to this case. Baseline: I decided to write a benchmark to show how we can use the knowledge of our data to improve performance. Here is my baseline code: void add_arrays_scalar(unsigned char* a, unsigned char* b, unsigned n){ unsigned chunks = 32 / n; for (unsigned k = 0; k &lt; chunks; ++k)  {  for (unsigned i = 0; i &lt; n; ++i)    a[i] += b[i];  a += n;  b += n; }}This code processes two 32-byte arrays in chunks, specified by n. So, for n = 4 there will be 8 4-byte chunks with 8 outer loop iterations. Example is obviously contrived, but it is quite common for image processing when pixels are processed by chunks with some stride. I have seen real world code examples when this code makes sense and improvement that we will see in this post is valid. Let’s say we know all possible values for n. They are 4, 8 and 16. Invocation of this function looks like this: // 1. All trip counts (tcXX variables) are read from the file, //  so compiler doesn't know them at compile time// 2. a and b have random numbers and values can wrap around,//  we don't care about it now. add_arrays_scalar(a, b, tc4); // tc4 = 4 (8 inner loop iters)add_arrays_scalar(a, b, tc8); // tc8 = 8 (4 inner loop iters)add_arrays_scalar(a, b, tc16); // tc16 = 16 (2 inner loop iters)// In the end I ensure the results are not optimized away. Compiler options are: -O3 -march=core-avx2. By default clang 5. 0 autovectorize the inner loop with vectorization width = 32 and interleaved by a factor of 4, so processing 128 bytes in one iteration. Also it does multiversioning by DD, i. e. creating two scalar version of the loop unrolled by a factor of 8 with a run-time trip count dispatching. The latter simply means that there are two scalar versions: one is unrolled by a factor of 8 and second is no unrolled, processing one byte at a time (sort of a fallback option). All code for the benchmark can be found here. If we profile the baseline case we will notice that vector code is completely cold. Well, that’s not surprising, because all our possible trip counts are smaller that 128 (vectorization width * interleave factor). For n = 8,16 scalar unrolled version is used and for n = 4 scalar basic version is used. Another thing worth to mention is that those function invocations have different weights. Because for n = 4 inner loop is executed 8 times and for n = 16 only 2. That’s why the fact that our function calls with n = 4 are executed by a simple scalar loop version (processing element by element) hurts our performance more than if we would miss using unrolled loop in the case n = 16. The reasoning behind this is: for the larger trip counts less amount of branch instructions are executed. We will see the prove for that when we will try to optimize our function for different trip counts. Attempts to improve performance: So, I tried another 3 ways to optimize the function:  Vectorization width = 4 Vectorization width = 8 Vectorization width = 16 Multiversioning by all possible trip counts. First three versions try to optimize for specific trip count by adding pragma to the inner loop: #pragma clang loop vectorize(enable) vectorize_width(X)#pragma clang loop interleave(disable) unroll(disable), where X is the vectorization width. As you can see I disabled unrolling and interleaving, because otherwise clang will do this and I will not get the hit in vector version of the loop. The last version (multiversioning by trip counts) looks like this: void add_arrays_multiver_by_trip_counts(	unsigned char* a, 	unsigned char* b, 	unsigned n){ unsigned chunks = 32 / n; for (unsigned k = 0; k &lt; chunks; ++k)  {  if (n == 4)  {   #pragma clang loop vectorize(enable) vectorize_width(4)   #pragma clang loop interleave(disable) unroll(disable)   for (unsigned i = 0; i &lt; 4; ++i)     a[i] += b[i];  }  else if (n == 8)  {   #pragma clang loop vectorize(enable) vectorize_width(8)   #pragma clang loop interleave(disable) unroll(disable)   for (unsigned i = 0; i &lt; 8; ++i)     a[i] += b[i];  }  else if (n == 16)  {   #pragma clang loop vectorize(enable) vectorize_width(16)   #pragma clang loop interleave(disable) unroll(disable)   for (unsigned i = 0; i &lt; 16; ++i)     a[i] += b[i];  }  else  {   for (unsigned i = 0; i &lt; n; ++i)     a[i] += b[i];  }  a += n;  b += n; }}Measurements: I ran the benchmark on a Intel Core i7-6700 processor: ---------------------------------------------------------------Benchmark         Time  CPU Iterations Throughput---------------------------------------------------------------Scalar          72 ns 72 ns  96790551 370. 835MB/s (baseline)Vectorization_width_4  52 ns 52 ns 133665303 510. 321MB/s (+37. 6%)Vectorization_width_8  48 ns 48 ns 146755270 559. 533MB/s (+50. 9%)Vectorization_width_16  55 ns 55 ns 128159636 488. 847MB/s (+31. 8%)Multiver_by_trip_counts 44 ns 44 ns 157490466 601. 103MB/s (+62. 1%)Justification: Our final version is 62% faster than the baseline. Let’s figure out why. I profiled case by case to understand what is going on in each case:  Vectorization width = 4 : scalar code is cold, which is good, but we are processing only 4 bytes per iteration (utilizing only 1/4 of the xmm register capacity). We can do better for trip counts 8,16.  Vectorization width = 8 : scalar code is hot for trip count = 4. For trip counts 8,16 vectorized version is used. We missed using vector version for the trip count with the highest weight (n = 4). However, it was a surprise to me that this version outperforms the version optimized for n = 4.  Vectorization width = 16 : scalar code is hot for trip counts 4,8. We are executing vector version only for trip count = 16. That’s still ~30% better than the baseline, but still not quite super optimal.  Multiversioning by trip count : scalar code is cold. All 3 possible trip counts use it’s own (specifically optimized) vector version of the loop, which gives us the most performance in this case. Caveat: This post is not to encourage you to optimize every routine like this - please don’t do that. Do this only if your measurements show significant benefit of such change. If your compiler is not doing the best job for your hot function probably someday it will. It’s not always beneficial to vectorize loops with small trip count, sometimes it’s better to do full unrolling. Compiler will be many times better than you at figuring out such things. Also this optimization doesn’t make much sense when you have big arrays. Compiler already prepared the code for it (remember that there is autovectorized version that processes 128 bytes at a time - see in the beginning of the article). Another thing worth to mention is that in the fastest attempt there are at least 6 versions of the same loop (3 handwritten + 1 autovectorized + 1 unrolled + 1 fallback). This increases code size significantly! In this case enabling LTO doesn’t make much of a difference (results are mostly the same). However, if you replace the trip count arguments with constant values (4,8,16) and enable LTO (pass -flto flag), then compiler will propagate this constant into the function and scalar version will beat all the others! I profiled this case and noticed that compiler recognized my dirty trick with processing array by chunks and realized that in the nutshell there is no difference between those 3 function calls - they all do the same thing. Final note: I want to share a great talk from CppCon 2014 by Mike Acton: Data-Oriented Design and C++. Want performance - know your data. UPD: one more article related to the topic of multiversioning: Function multi-versioning in GCC 6. All posts from this series::  Vectorization intro.  Vectorization warmup.  Checking compiler vectorization report.  Vectorization width.  Multiversioning by data dependency.  Multiversioning by trip counts (this article).  Tips for writing vectorizable code. "
    }, {
    "id": 37,
    "url": "https://dendibakh.github.io/blog/2017/11/03/Multiversioning_by_DD",
    "title": "Vectorization part5. Multiversioning by data dependency.",
    "body": "2017/11/03 - Vectorization doesn’t always come for free. In this post we will see what penalties we have to pay with vectorization. Without further ado let me show you example of the code: void foo( unsigned short * a, unsigned short * b ){ for( int i = 0; i &lt; 128; i++ ) {  a[i] += b[i];  }}Now lets consider a kind of weird invocation of this function:  unsigned short x[] = {1, 1, 1, 1, . . . , 1}; // 129 elements unsigned short* a = x + 1; unsgined short* b = x; foo (a, b);In scalar version we will receive results: x = {1, 2, 3, 4, 5, . . . }. But in vector version we will first load some portion of a (starting from x + 1). Then we will load some portion of b (starting from x). Then we will add two vector registers together, resulting in x = (2, 2, 2, 2, . . . ). Oops! Something is wrong. Vectorized version of the loop works perfectly fine as long as input arrays do not alias (there is memory intersection). To protect from this problem compilers insert runtime checks for arrays aliasing. Lets see what clang 5. 0 generated for us (link to godbolt):  lea rax, [rsi + 256] # calculating the end of b (b + 128) cmp rax, rdi     # comparing the beginning of a and the end of b jbe . LBB0_4 lea rax, [rdi + 256] # calculating the end of a (a + 128) cmp rax, rsi     # comparing the beginning of b and the end of a jbe . LBB0_4 xor eax, eax &lt;scalar version&gt;. LBB0_4: &lt;vector version&gt;As you can see there is some runtime dispatching between scalar and vector version of the same loop. This is what is called Multiversioning.  Normally pointer aliasing is rather rare case, but we don’t know for sure, so we need to have a runtime check for that. If you are sure that your arrays will never alias you can use __restrict__ keyword to tell the compiler about it: (link to godbolt). As you can see runtime check was removed. Situation gets somewhat complex when there are multiple arrays in the function arguments. This significantly increases the number of runtime checks in the beginning of the function. Gcc even has heuristic for that: --param vect-max-version-for-alias-checks which is 10 by default. Another frequently used runtime check for the compiler is testing number of loop iterations. It should not be negative or lower than the vectorization width. All posts from this series::  Vectorization intro.  Vectorization warmup.  Checking compiler vectorization report.  Vectorization width.  Multiversioning by data dependency (this article).  Multiversioning by trip counts.  Tips for writing vectorizable code. "
    }, {
    "id": 38,
    "url": "https://dendibakh.github.io/blog/2017/11/02/Vectorization_width",
    "title": "Vectorization part4. Vectorization Width.",
    "body": "2017/11/02 - In my previous posts we have seen somewhat basic examples of loop vectorization. In this post we will go deeper in this fun stuff. In this post I will show multiple ways for the compiler to vectorize the same loop. Usually compiler knowns better if it is beneficial to do vectorization (vs. scalar). Or maybe scalar version with good unrolling factor will do a better job. Or maybe we can vectorize the loop and then do unrolling. Or maybe interleaving will help more (see below in this article for an example). OMG. So in base case you should let compiler do the job. Every decent compiler has internal cost model which it uses to make good decisions about vectorization/unrolling, etc. However, compiler is not always getting it right. When good decision making requires introspection (for example knowing how much loop iterations there will be) compiler can make not the best choice. Imagine code like this (as always link to godbolt): void add_arrays(unsigned* a, unsigned* b, unsigned n){ for (unsigned i = 0; i &lt; n; ++i)   a[i] += b[i];}Kind of an easy code. But, as you can see the trip count n (number of iterations) is unknown. This makes compiler guess what is the number of iterations. If it is just a constant passing as an argument to a function call - no problem, compiler will propagate that (if it won’t be a big chain of invocations). What if there are multiple calls with two different constants - slightly more complex, but still compiler knows everything it needs to know to make a good decision. But when the trip count is coming from some heavy computation or even from IO, compiler is just throwing up hands and do what it think will work good on the average. In this case clang 5. 0 decided to vectorize the function using AVX instructions (ymm registers with 256 bits capacity) and unroll it by a factor of 8. Output from a compiler otp report: Passed - vectorized loop (vectorization width: 8, interleaved count: 4)Warning: Don’t fall into a trap here, compiler explorer will show additional output, but this output is related to scalar version of the loop: Passed - unrolled loop by a factor of 8 with run-time trip countYou can see the same reports if you use -Rpass* option with loop-vectorize and loop-unroll parameters (see my previous post). Let’s check assembly to see if it matches the report: . LBB0_12: # =&gt;This Inner Loop Header: Depth=1 vmovdqu ymm0, ymmword ptr [rax - 96] vmovdqu ymm1, ymmword ptr [rcx - 64] vmovdqu ymm2, ymmword ptr [rcx - 32] vmovdqu ymm3, ymmword ptr [rcx] vpaddd ymm0, ymm0, ymmword ptr [rcx - 96] vpaddd ymm1, ymm1, ymmword ptr [rax - 64] vpaddd ymm2, ymm2, ymmword ptr [rax - 32] vpaddd ymm3, ymm3, ymmword ptr [rax] vmovdqu ymmword ptr [rcx - 96], ymm0 vmovdqu ymmword ptr [rcx - 64], ymm1 vmovdqu ymmword ptr [rcx - 32], ymm2 vmovdqu ymmword ptr [rcx], ymm3 sub rax, -128 sub rcx, -128 add r10, -32 jne . LBB0_12This is not super complicated piece of assembly, but it is interesting in a couple of ways. Basic observations:  We can spot how compiler do array indexing: rax - 96, rax - 64, etc. Natural way is to do forward indexing: rax + 0, rax + 32, etc. But okay, I actually don’t know what is the reason behind this.  Addition is done in a weird way: sub rax, -128. But it is done for having more compact code. -128 fits in one byte (two’s complement), but 128 needs two bytes. Thanks for @simonask at cpplang. slack. com.  r10 is just a counter, not used in offsets or computation. rax indexes b[] and rcx indexes a[]. Besides that we see that this loop is adding 32 unsigned integers on every iteration. Loop is unrolled by a factor of 4. Vectorization width of 8 is calculated like this: 256 (size of ymm register in bits) / 32 (size of unsigned in bits) = 8. So in this case it tells us how many elements fits in one vector register that was chosen by th e compiler. Not super fancy, but keep on reading, its not all. Vectorization width has another quite interesting property. Interleaving: Interleaving implicitly denotes the unrolled factor, which is 4 in the example above. Interleaving means that unrolled iterations are interleaved within a loop. In this example it first load 4 ymmwords (256 bits) from memory, starting all 4 iterations in parallel. Then it makes 4 additions again kinda in parallel. Then it does 4 write backs. Clang 5. 0 always do interleaving for vectorized version of this loop, however version with no interleaving would look like this:  Load a[0-7] Add b[0-7], a[0-7] Store b[0-7] Load a[8-15] Add b[8-15], a[8-15] Store b[8-15] etc. Interleaving in some cases makes better utilization of a CPU resources, however it adds more register pressure, because we are doing more work in parallel. Why we care about vectorization width?: Now, lets see how good compiler did for this code. We will fall into vectorized version of a loop if we have at least 32 loop iterations. If the trip count for this loop is always 16, but compiler does not know that (say, it comes from configuration file) then we will fall down to the scalar version. And if it is the hot place in our application, than this will cause us significant performance hit. This is actually a call why you should use library functions like memset, memcpy, and STL algorithms - because they are heavily optimized for such cases. If you know that your function will always has the same trip count, say 16, then you can specifically tune it with the method I will describe further. If you have multiple trip counts, say 8 and 16, you can tune it as well, but I will leave this for the future article, namely “Multiversioning by trip counts”. Vectorization width tuning: In clang one can control vectorization width with the following pragma: #pragma clang loop vectorize_width(4)This will implicitly tell llvm that vectorization of the loop is enabled and the width is 4. More on this you can read on llvm site. Here is the link to godbolt with this change. Essentially what has changed is that now llvm uses SSE registers instead of AVX to do the job, so processing 16 unsigned integers in one loop iteration (previously 32). This will enable using vectorized version for arrays of 16+ elements. If we set vectorization width greater than what ymm(AVX)/zmm(AVX-512) can handle, than it is a signal for llvm to unroll the loop. See example with vectorize_width(64): link to godbolt. Here llvm uses AVX registers and loop was unrolled by a factor of 8 (previously 4). If we set vectorization width smaller than what xmm (SSE) register can handle, than llvm will do meaningful work on some part of xmm register. See example with vectorize_width(2): link to godbolt. You can spot inlined comments from the compiler like: vmovq xmm0, qword ptr [rax - 24] # xmm0 = mem[0],zeroIt means that CPU only load qword(8 bytes) into the xmm0 register (half of it’s capacity), filling the rest of it with zeros. Quite smart in my opinion! GCC 8. 0 is not able to do that yet. For the same code gcc able to vectorize the loop with the minimum width of 4. See this thread in gcc mailing list. In gcc it can be controlled with #pragma omp simd with simdlen clause. More information here. As a final note, I want to say, that in gcc there is different name for width. It’s called vectorization factor (vf). And in Intel compiler (icc) it is called vector length (VL). All posts from this series::  Vectorization intro.  Vectorization warmup.  Checking compiler vectorization report.  Vectorization width (this article).  Multiversioning by data dependency.  Multiversioning by trip counts.  Tips for writing vectorizable code. "
    }, {
    "id": 39,
    "url": "https://dendibakh.github.io/blog/2017/10/30/Compiler-optimization-report",
    "title": "Vectorization part3. Compiler report.",
    "body": "2017/10/30 - This post will be short but it is quite important to know about compiler optimization reports because it can save you a lot of time. Sometimes you want to know if your loop was vectorized or not, unrolled or not. If it was unrolled, what is the unrol factor? Was your function inlined? There is a hard way - by looking at the assembly. This can be a really hard if the function is big, or it has many loops that were also vectorized, or if compiler created multiple versions of the same loop, OMG. There is more convienient way to know that - by checking compiler report. For example, for the following code (link to godbolt): void add_arrays(float* a, float* b, std::size_t n){  for (std::size_t i = 0; i &lt; n; ++i)    a[i] += b[i];}To emit opt report in clang you need to pass -Rpass* flags: $ clang -O3 -Rpass-analysis=loop-vectorize -Rpass=loop-vectorize -Rpass-missed=loop-vectorizea. cpp:5:5: remark: vectorized loop (vectorization width: 4, interleaved count: 2) [-Rpass=loop-vectorize]  for (std::size_t i = 0; i &lt; n; ++i)  ^Great, so at least now we know that our loop was vectorized with a vectorization width = 4 (see next posts what that mean) and vectorized loop iterations were interleaved with count = 2. You still may want to check assembly, as it might surprise you in some cases, but it gives a good starting point and quick way to check things. However, it requires a little bit of experience to understand what those parameters mean to fully leverage compiler opt reports. In compiler explorer there is a cool opt report viewer. You need just to hover your mouse over the line with the code and you will see all high-level optimizations that were performed on that loop. Sometimes, vectorization fails. For example: void add_arrays(float* a, float* b, std::size_t n){  float agg = 0. 0;  for (std::size_t i = 0; i &lt; n; ++i)  {    a[i] += b[i];    agg += b[i];    if (agg &gt; 100)      break;  }}Opt report: a. cpp:6:5: remark: loop not vectorized: value that could not be identified as reduction is used outside the loop [-Rpass-analysis=loop-vectorize]  for (std::size_t i = 0; i &lt; n; ++i)  ^a. cpp:6:5: remark: loop not vectorized: could not determine number of   loop iterations [-Rpass-analysis=loop-vectorize]a. cpp:6:5: remark: loop not vectorized [-Rpass-missed=loop-vectorize]Sometimes you will see reports about missed vectorization opportunities because it was not beneficial to vectorize the loop. For example, because there were not enough iterations. Vectorizer has some internal cost model, which compiler uses to make decision about vectorizing particular loop. Situation gets a little bit compilcated when you are using LTO. When you are building with LTO, clang does not produce the binary files, but bitcode (intermediate representation) which will be combined into executable on linking stage. So, the final decision about whether it’s beneficial to vectorize the loop or not, now may happen on the LTO stage. For example, compiler inlined the function call and now it knows all possible trip counts of the loop. So, when you pass -Rpass* along with -flto it won’t print you anything. To see opt reports in this case first you need to add debug information(-g) to the compilation of the file you are interesting in. Lack of debug info will cause no filenames and line numbers in the report. After that, you need to pass additional options to the linking stage:  Gold plugin - pass -Wl,-plugin-opt,-pass-remarks=loop-vectorize -pass-remarks-missed=. etc.  LLD linker - pass -Wl,-mllvm -Wl,-pass-remarks=loop-vectorize -Wl,-mllvm -Wl,-pass-remarks-missed=. etc. Other compilers: For gcc you need to pass -ftree-vectorize -ftree-vectorizer-verbose=X, where X is the verbose level. More about this here. I find the most usable opt reports from Intel Compiler (icc). It shows if the loop was multiversioned, it has filter by the line of the code, etc. Also the issue with LTO (like in clang) works with no additional steps from the user. It remembers that user requested opt report on compilation stage and it will generate output in the text file on the linking stage (in icc it is called IPO - Inter Procedural Optimization). More links for icc here and here. All posts from this series::  Vectorization intro.  Vectorization warmup.  Checking compiler vectorization report (this article).  Vectorization width.  Multiversioning by data dependency.  Multiversioning by trip counts.  Tips for writing vectorizable code. "
    }, {
    "id": 40,
    "url": "https://dendibakh.github.io/blog/2017/10/27/Vectorization_warmup",
    "title": "Vectorization part2. Warmup.",
    "body": "2017/10/27 - I want to do this warmup post, just to be sure that we are on the same page. It will also give you the taste of how the vector assembly looks like and how it works. I remember when I was learning vectorization stuff, I really missed the assembly that was generated by the compiler. I was like: “Show me assembly, I want to see how it’s done. ” Even if there was assembly I didn’t understand how this hairy scary lines of code do the job. I find the example in this post somewhat special. It might be not the most efficient assembly code, but in my opinion it nicely shows capabilities of the vectorization. So, in this post we will go through an assembly code line by line, visualizing the state of the registers after some portion of assembly code. Here is the example: godbolt. In general, compiler has quite hard time dealing with various control flow statements inside the loop when trying to vectorize it. Vectorization gets much harder when there is if statement inside the loop, and even infeasible when there is a break or throwing exception. However, in our example there is if statement and the loop was successfully vectorized by gcc. int foo( short a[16], unsigned short b[16], unsigned short bias[16] ){ int agg = 0; for( int i = 0; i &lt; 16; i++ ) {   if( a[i] &gt; 0 )    a[i] = (bias[i] + a[i]) * b[i] &gt;&gt; 16;   else    a[i] = - ((bias[i] - a[i]) * b[i] &gt;&gt; 16);   agg += a[i];  } return agg;}Loading: Let’s look at first 4 instructions. Description of all instructions can be found here.  vmovdqu ymm4,YMMWORD PTR [rdi] # a[] vmovdqu ymm2,YMMWORD PTR [rdx] # bias[] vmovdqu ymm5,YMMWORD PTR [rsi] # b[] vmovdqa ymm6,YMMWORD PTR [rip+0x5c6] # 400b00They load our input arrays into registers. By x86 calling conventions for 64 bit architecture our first three arguments are stored in rdi, rsi and rdx respectively. Here is what values are in registers after this step (I removed registers ymm10-ymm15 because they are not used in this example): Extracting: Because our computations can overflow the size of short, we need to use 32 bit integers for computation. That’s why we first “unpack” all the 16-bit values and make them 32-bit.  vpmovzxwd ymm1,xmm2 vextracti128 xmm8,ymm4,0x1 vextracti128 xmm2,ymm2,0x1vpmovzxwd ymm1, xmm2 instruction zero extend 8 16-bit integers in xmm2 to 8 32-bit integers and puts them into ymm1. vextracti128 xmm8,ymm4,0x1 extract 128 most significant bits (3rd parameter 0x1 tells to do so) of integer data from ymm4 and store results in xmm8.  Next stage is mostly similar to the previous one, so I will just skip it and only show the result. You can trace it by your own.  vpmovzxwd ymm9,xmm5 vpmovzxwd ymm2,xmm2 vpmovsxwd ymm0,xmm4 vpmovsxwd ymm8,xmm8 vextracti128 xmm5,ymm5,0x1 Computation: Up to this moment we prepared all data into separate parts. Next assembly blob is actually doing computation inside the loop.  vpaddd ymm3,ymm1,ymm0 vpmovzxwd ymm5,xmm5 vpsubd ymm0,ymm1,ymm0 vpsubd ymm1,ymm2,ymm8 vpmulld ymm0,ymm0,ymm9 vpmulld ymm1,ymm1,ymm5 vpaddd ymm7,ymm2,ymm8 vpmulld ymm3,ymm3,ymm9 vpmulld ymm7,ymm7,ymm5So, vpaddd, vpsubd and vpmuld do arithmetic operations on dwords and I believe you can easily extract what each instruction does from its name. I will mark temporary results with the signs of completed arithmetic operations. For example, bias[i] + a[i] as result of +, (bias[i] - a[i]) * b[i] as result of - *. Updated state of the registers looks like this: Finally, we do the right shifts.  vpsrad ymm0,ymm0,0x10 vpsrad ymm1,ymm1,0x10 vpand ymm1,ymm6,ymm1 vpsrad ymm3,ymm3,0x10 vpand ymm0,ymm6,ymm0By doing vpand ymm1,ymm6,ymm1 we just zeroing most significant half of the bits of each value in ymm1 (recall that ymm6 has values 0xFFFF in it). We need to do that because vpsrad is doing sign extension after shifting.  Packing: vpackusdw ymm0,ymm0,ymm1 vpsrad ymm7,ymm7,0x10 vpxor xmm1,xmm1,xmm1vpackusdw ymm0, ymm0, ymm1 converts 8 16-bit signed integers from ymm0 and 8 16-bit signed integers from ymm1 and store them into 16 16-bit unsigned integers in ymm0.  Handling if statement: vpcmpgtw ymm4,ymm4,ymm1 vpand ymm3,ymm6,ymm3 vpand ymm7,ymm6,ymm7 vpackusdw ymm3,ymm3,ymm7This is the most interesting part of this example. Instruction vpcmpgtw ymm4,ymm4,ymm1 compares 16-bit signed integers for greater than. Effectively this instruction does all 16 comparisons just in one instruction. Amazing!And for each pair of 16-bit ints it stores either 0xFFFF or 0 in the destination vector, depending on the result of the comparison.  Writing back: Note, that at this stage we have both branches of if statement computed (in ymm0 and ymm3) and we know all outcomes of the if conditions (in ymm4). So, we need to select proper values from correct branch results.  vpermq ymm0,ymm0,0xd8 vpermq ymm3,ymm3,0xd8 vpsubw ymm0,ymm1,ymm0 vpblendvb ymm0,ymm0,ymm3,ymm4 vpmovsxwd ymm1,xmm0 vmovdqu YMMWORD PTR [rdi],ymm0vperfmq ymm0,ymm3,0xd8 permute qwords in ymm3 using indexes in the third operand(0xd8) and store the result in ymm0. We need to permute values in ymm0 and ymm3 because of the nature of vpackusdw (see above). When vpackusdw combines two arrays together it does interleaving. When combining a[0. . 7] and a[8-15] this instruction makes the output of a[0. . 3] a[8-11] a[4-7] a[12-15]. So, to receive correct result we need to swap elements in the middle. And this is exactly what vpermq instruction with the mask 0xd8 is doing. vpsubw ymm0,ymm1,ymm0 simply negates all the values in ymm0, completing final steps of computation. vpblendvb ymm0,ymm0,ymm3,ymm4 selects values from ymm0 and ymm3 according to the control in ymm4 and stores them into ymm0. And finally, vmovdqu YMMWORD PTR [rdi],ymm0 stores values from ymm0 back to memory (in a[]). What is left is calculate the return value (agg), bit I will leave this exercise for the reader. Conclusions: I want to point out a couple of things we learned from this example.  Note, that the loop was fully vectorized, meaning that there are no backward jump. It is simple for compiler to do because the trip count (number of loop iterations) is known.  Still there is lots of scalar code. I will explain why we need it in the next posts. Stay tuned.  Surprisingly, gcc is not using horizontal add for calculating agg. Clang does, but it is only able to vectorize the loop when using GVN-hoisting (-mllvm -enable-gvn-hoist). link to godbolt.  There is not so much register pressure in this function. Registers ymm10-ymm15 are free for compiler to do something else. So, still there is some free space for the vectorizer to handle even more complex code. As a final note I want to put a links to the two great presentations from CppCon2017:  Matt Godbolt - “What Has My Compiler Done for Me Lately? Unbolting the Compiler’s Lid”.  Charles Bailey “Enough x86 Assembly to Be Dangerous”. All posts from this series::  Vectorization intro.  Vectorization warmup (this article).  Checking compiler vectorization report.  Vectorization width.  Multiversioning by data dependency.  Multiversioning by trip counts.  Tips for writing vectorizable code. "
    }, {
    "id": 41,
    "url": "https://dendibakh.github.io/blog/2017/10/24/Vectorization_part1",
    "title": "Vectorization part1. Intro.",
    "body": "2017/10/24 - Recently I was working closely with analyzing different vectorization cases. So I decided to write a series of articles dedicated to this topic. This is the first post in this series, so let me start with some introduction info. Vectorization is a form of SIMD which allows to crunch multiple values in one CPU instruction. I know it is bad introduction when just links to wiki thrown everywhere around, so let me show you simple example (godbolt): #include &lt;vector&gt;void foo( std::vector&lt;unsigned&gt;&amp; lhs, std::vector&lt;unsigned&gt;&amp; rhs ){  for( unsigned i = 0; i &lt; lhs. size(); i++ )  {      lhs[i] = ( rhs[i] + 1 ) &gt;&gt; 1;  }}Lets compile it with clang with options -O2 -march=core-avx2 -std=c++14 -fno-unroll-loops. I turned off loop unrolling to simplify the assembly and -march=core-avx2 tells compiler that generated code will be executed on a machine with avx2 support. Assembly generated contains: Scalar version mov edx, dword ptr [r9 + 4*rsi]     # loading rhs[i]add edx, 1               # rhs[i] + 1shr edx                 # (rhs[i] + 1) &gt;&gt; 1mov dword ptr [rax + 4*rsi], edx    # store result to lhsmov esi, ediadd edi, 1               # incrementing i by 1cmp rcx, rsija &lt;next iteration&gt;Vector version vmovdqu ymm1, ymmword ptr [r9 + 4*rdi] # loading 256 bits from rhsvpsubd ymm1, ymm1, ymm0         # ymm0 has all bits set, like +1vpsrld ymm1, ymm1, 1          # vector shift right. vmovdqu ymmword ptr [rax + 4*rdi], ymm1 # storing result add rdi, 8               # incrementing i by 8cmp rsi, rdijne &lt;next iteration&gt;So here you can see that vector version crunches 8 integers at a time (256 bits = 8 bytes). If you will analyze assembly carefull enough you will spot the runtime check which dispatch to those two versions. If there are not enough elements in the vector for choosing vector version, scalar version will be taken. Amount of instructions is smaller in vector version, although all vector instructions have bigger latency than scalar counterparts. Vector operations can give significant performance gains but they have quite many restrictions which we will cover later. Historically, Intel has 3 instruction sets for vectorization: MMX, SSE and AVX. Vector registers for those instruction sets are described here. In general, not only loops can be vectorized. There is also linear vectorizer (in llvm it is called SLP vectorizer) which is searching for similar independent scalar instructions and tries to combine them. To check vector capabilities of your CPU you can type lscpu. For my Intel Core i5-7300U filtered output is: Flags:         fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq ssse3 cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch rdseed clflushoptFor us the most interesting is that this CPU supports sse4_2 and avx instruction sets. That’s all for now. In later articles I’m planing to cover following topics:  Vectorization intro (this article).  Vectorization warmup.  Checking compiler vectorization report.  Vectorization width.  Multiversioning by data dependency.  Multiversioning by trip counts.  Tips for writing vectorizable code. "
    }, {
    "id": 42,
    "url": "https://dendibakh.github.io/blog/2016/11/25/Small_size_optimization",
    "title": "Small size optimization.",
    "body": "2016/11/25 - As Chandler Carruth said in his talk at CppCon 2016, a lot of people underestimate the benefit of Small Size Optimization(SSO). I decided to give a simple implementation of this idea. The problem in using std::vector is that as the number of elements grows memory allocations become more and more expensive. Not only that we need to find the memory for more elements, but we need also to move(copy) our elements to new location. Usually there are log(N) memory allocations, depending on the implementation of course. But let’s think for a moment… With high probability I can say that most of the time the number of elements in our containers do not exceed 100. Maybe the number is even less. Idea behind SSO is to allocate memory on the stack (which is just shifting the stack pointer against malloc syscall). And only if we exceed this preallocated buffer we will fallback to the heap storage. With this we essentially cover the majority of the scenarios. Of course we should keep in mind that this optimization consumes more memory. However, we can adjust the amount preallocated memory. Keep on reading… template &lt;unsigned N&gt;class SmallVector{ public:  SmallVector();  ~SmallVector();  SmallVector(const SmallVector&amp; rhs) = delete;  SmallVector&amp; operator=(const SmallVector&amp; rhs) = delete;  void push_back(int value);  int&amp; operator[](size_t index);  int&amp; at(size_t index); protected:  std::array&lt;int, N&gt; smallBuffer;  unsigned size;  unsigned capacity;  int* heapVector;};Usually the size of std::vector equals to the size of 3 pointers. In our case it looks pretty similar with additional plain std::array as this small buffer. We initially point heapVector to the beginning of our smallBuffer. template &lt;unsigned N&gt;SmallVector&lt;N&gt;::SmallVector() : size(0), capacity(N), heapVector(smallBuffer. data()){ for (auto&amp; e : smallBuffer)  e = int();}Implementation of operator[] is trivial: template &lt;unsigned N&gt;int&amp; SmallVector&lt;N&gt;::operator[](size_t index){ return heapVector[index];}The most interesting part of the code is inside push_back: template &lt;unsigned N&gt;void SmallVector&lt;N&gt;::push_back(int value){ if (size == capacity) {  int* newStorage = new int[capacity * 2];  capacity *= 2;  memcpy(newStorage, heapVector, size * sizeof(int));  if (heapVector != smallBuffer. data())   delete [] heapVector;  heapVector = newStorage; } heapVector[size++] = value;}So, as long as we do not exceed our storage allocated on the stack we will not even go into the heap. This can give us a huge win in performance critical part of the code. Complete code you can find on my github account. "
    }, {
    "id": 43,
    "url": "https://dendibakh.github.io/blog/2016/11/21/Sentinels",
    "title": "Sentinels.",
    "body": "2016/11/21 - Today I would like to show one interesting techique for optimizing your algorithms. This technique is called sentinels. Sentinel is a special thing that marks the end of the sequence. The natural example here example is '\0' terminator for a C-string. Let’s consider function that only searches for some value in the array: bool find_benchmark(int* arr, size_t size, int val){ for (size_t i = 0; i &lt; size; ++i) {  if (val == arr[i])   return true; } return false;} We check all elements for equality with out target.  At each iteration we check if we reach the end of our array. We can’t put away equality checks, but we can get rid of checking bound by using sentinel. The idea is to insert the number we are looking for at the end of the array. This will garanty that in worst case we will look through entire array, but we will always find the number we are looking for. If so, than we can get rid of checking out of bounds condition, making our loop naturally infinite. We can be sure that our infinite loop will finish, because we know there is a value we are looking for. // Assumption made: array has one empty slot for insertion our sentinel. bool sentinel_find_benchmark(int* vect, size_t size, int val){ vect[size] = val; size_t i = 0; for (;;++i) {  if (val == vect[i])  {   if (i == size)    return false;   else    return true;  } } return false;}This code is far from ideal, but it shows the idea behind the sentinels. In general there are much more concerns you should care about:  If the const array is passed, you are not allowed to change it, thus require a copy to be made.  If the non-const vector is passed, inserting a new element can cause reallocation -&gt; invalidating iterators.  Elements of the array can be non default-constructible, preventing for creation of a sentinel. I ran a benchmark test (search failure) with 1000 elements 1000000 times:  With no optimizations (-O0) sentinels version was 9% faster.  With -O3 sentinels version was 21% faster. To understand why this works lets look at the assembly. You can check all assembly output here: godbolt. org. Comparing effective loops of two algoritms we can see that one additional check eliminated: | Simple find            | Find with sentinel       ||:----------------------------------:|:-------------------------------:||Effective loop:           | Effective loop:         || mov   rax, QWORD PTR [rbp-8]  |                 || cmp   rax, QWORD PTR [rbp-32]  |                 || jnb   . L2            |                 || mov   rax, QWORD PTR [rbp-8]  | mov   rax, QWORD PTR [rbp-8] | | lea   rdx, [0+rax*4]      | lea   rdx, [0+rax*4]     || mov   rax, QWORD PTR [rbp-24]  | mov   rax, QWORD PTR [rbp-24]|| add   rax, rdx         | add   rax, rdx        || mov   eax, DWORD PTR [rax]   | mov   eax, DWORD PTR [rax]  || cmp   eax, DWORD PTR [rbp-36]  | cmp   eax, DWORD PTR [rbp-36]|| jne   . L3            | jne   . L2          |Complete set of functions as well as the benchmarking tests can be found here. Sentinels could be used even for speed up quicksort. See this great talk by Andrei Alexandrescu on ACCU 2016. "
    }, {
    "id": 44,
    "url": "https://dendibakh.github.io/blog/2016/11/20/Few-thougths_from_codedive",
    "title": "Code::Dive 2016 trip report.",
    "body": "2016/11/20 - It’s my second time I’ve beed on Code::Dive conference in Wroclaw. This year it was on 15-16 November. We had a really great speakers such as Chandler Carruth, Sean Parent and Mark Issacson. It was amazing opportunity to hear talks on compiler optimizations, clang tools, undefined behaviour, C++ history and future and best practices. All talks should be available on youtube shortly. Really many-many thanks to Nokia for such a great free conference!Looking forward to attend next year (probably as a speaker). Things I learned from this conference:    If you do not turn off exceptions, compiler will generate emergency buffer. This memory space is needed for example, when you’re throwing out_of_memory exception. Some amount of memory should be allocated somewhere, but you are already out of memory, so you need some preallocated storage for it. More on this topic here: Emergency buffer for exceptions.     Passing -fno-exceptions to the compiler will instruct it to turn every throw calls in STL into std::abort. Also compilers are able to detect lack of catch‘es in your program. In this case they will convert each throw call in your programm to terminate, because noone will catch it either way. More details on stackoverflow question.     Finally I got to know that dereferencing of nullptr is undefined behaviour. Because on some platforms (with direct memory mapping) dereferencing null pointer means accessing memory with offset 0x0. More information on stackoverflow question.     “No raw synchronization primitives” by Sean Parent. Coming soon on youtube and Code::Dive.     “Try to avoid inline keyword” by Chandler Carruth. Coming soon on youtube and Code::Dive.  "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form onsubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <p><input type="text" class="form-control" id="lunrsearch" name="q" maxlength="255" placeholder="Search via Lunr.js"></p>
</form>
<div id="lunrsearchresults">
    <ul></ul>
</div>
 </li>
		<li class=""><a href="https://dendibakh.github.io/notes" title="Posts">Blog</a></li>
		<li class=""><a href="https://dendibakh.github.io/about_me" title="About me">About me</a></li>
		<li class=""><a href="https://dendibakh.github.io/contact" title="Contact">Contact</a></li>
	</ul>
	<h3 class="text-muted"><a href="https://dendibakh.github.io/">Denis Bakhvalov</a></h3>
</div> <!-- /header -->

			
			<div class="notes">
		<div class="note single">
			<h1 class="title">Machine code layout optimizations.</h1>
			
			<h2 class="date"><info datetime="2019-03-27 00:00:00 -0700">
				27 Mar 2019 
			</info></h2>
			<!-- Post content -->
			<div class="notebody">
				<p><strong>Contents:</strong></p>
<ul id="markdown-toc">
  <li><a href="#what-is-machine-code-layout" id="markdown-toc-what-is-machine-code-layout">What is machine code layout?</a></li>
  <li><a href="#machine-code-layout-optimizations" id="markdown-toc-machine-code-layout-optimizations">Machine code layout optimizations</a>    <ul>
      <li><a href="#basic-block-placement" id="markdown-toc-basic-block-placement">Basic block placement</a></li>
      <li><a href="#basic-block-alignment" id="markdown-toc-basic-block-alignment">Basic block alignment</a></li>
      <li><a href="#function-splitting" id="markdown-toc-function-splitting">Function splitting</a></li>
      <li><a href="#function-grouping" id="markdown-toc-function-grouping">Function grouping</a></li>
    </ul>
  </li>
  <li><a href="#profile-guided-optimizations-pgo" id="markdown-toc-profile-guided-optimizations-pgo">Profile guided optimizations (PGO)</a></li>
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
</ul>

<p>I spent a large amount of time in 2018 working on optimizations that 
try to improve layout of machine code. I decided to share what it is and
 show some basic types of such transformations.</p>

<p>I think usually those improvements are underestimated and usually end
 up being omitted and forgotten. I agree that you might want to start 
with “fruits that hang lower” like loop unrolling and vectorization 
opportunities. But knowing that you might get extra 5-10% just from 
better laying out the machine code is still useful.</p>

<p>Before actually going to the core of the article I should say that 
CPU architects put a lot of efforts in hiding those kind of problems 
that we will talk about today. There are different structures in the CPU
 front-end that mitigate code layout inefficiencies, however there is 
still no free lunch there.</p>

<p>Everything that we will discuss in this article applies whenever you 
see a big amount of execution time wasted due to Front-End issues. See 
my previous article about <a href="https://dendibakh.github.io/blog/2019/02/09/Top-Down-performance-analysis-methodology">Top-Down performance analysis methodology</a> for examples.</p>

<p>Compilers? Is that what you think? Right, compilers are very smart 
nowadays and are getting smarter each day. They do the most part of the 
job of generating the best layout for your binary. In combination with 
profile guided optimization (see at the end of the article) they will do
 most of the things that we will talk about today. And I doubt you can 
do it better than PGO, however there are still some limitations. Keep on
 reading and you will know.</p>

<p><strong>If you just want to refresh knowledge in your head, you can jump straight to summary.</strong></p>

<h2 id="what-is-machine-code-layout">What is machine code layout?</h2>

<p>When compiler translates your source code into zeros and ones 
(machine code) it should generate serial byte sequence. For example, it 
should convert this C code:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&lt;=</span> <span class="n">b</span><span class="p">)</span>
  <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</code></pre></div></div>
<p>Into something like:</p>
<pre><code class="language-asm">; a is in rax
; b is in rdx
; c is in rcx
cmp rax, rdx
ja .label
mov rcx, 1
.label:
</code></pre>

<p>Assembly instructions will be encoded with some amount of bytes and 
will be laid out consequently in memory. This is what is called machine 
code layout.</p>

<p>Next I will show some typical optimizations of code layout in the order of biggest impact it can make<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<h2 id="machine-code-layout-optimizations">Machine code layout optimizations</h2>

<h3 id="basic-block-placement">Basic block placement</h3>

<p>If you’re unfamiliar with what is <a href="https://en.wikipedia.org/wiki/Basic_block">Basic block</a>, it is a sequence of instructions with single entry and single exit:</p>

<p><img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/Basic-block.jpg" alt="" class="center-image-width-20"></p>

<p>Compilers like to operate on a basic block level, because it is 
guaranteed that every instruction in the basic block will be executed 
exactly once. Thus for some problems we can treat all instructions in 
the basic block as one instruction. This greatly reduces the problem of 
CFG (control flow graph) analysis and transformations.</p>

<p>All right, enough theory. Why one sequence of blocks might be better than the other? Here is the answer. For the code like this:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// hot path
</span><span class="k">if</span> <span class="p">(</span><span class="n">cond</span><span class="p">)</span>
  <span class="n">coldFunc</span><span class="p">();</span>
<span class="c1">// hot path again
</span></code></pre></div></div>
<p>Here are two different physical layouts we may come up with:</p>

<p><img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/DefaultLayout.jpg" alt="" class="center-image-width-20-no-block"> <img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/Arrow.jpg" alt="" class="center-image-width-10-no-block"> <img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/BetterLayout.jpg" alt="" class="center-image-width-20-no-block"></p>

<p>What we did on the right was just invert the condition from <code class="highlighter-rouge">if (cond)</code> into <code class="highlighter-rouge">if (!cond)</code>. Arrow suggests that the one on the right is better than the one on the left. But why? <strong>Main
 reason is because we maintain fall through between hot pieces of the 
code. Not taken branches are fundamentally cheaper that taken. 
Additionally second case better utilizes L1 I-cache and uop-cache (DSB)</strong>. See one of my previous <a href="https://dendibakh.github.io/blog/2018/07/09/Improving-performance-by-better-code-locality">posts</a> for further details.</p>

<p>You can make a hint to compiler using <code class="highlighter-rouge">__builtin_expect</code> construct<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// hot path
</span><span class="k">if</span> <span class="p">(</span><span class="n">__builtin_expect</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="c1">// NOT likely to be taken
</span>  <span class="n">coldFunc</span><span class="p">();</span>
<span class="c1">// hot path again
</span></code></pre></div></div>

<p>When you dump assembly you will see the layout that looks like on the right.</p>

<p>In LLVM this functionality is implemented in <a href="http://llvm.org/doxygen/MachineBlockPlacement_8cpp_source.html">lib/CodeGen/MachineBlockPlacement.cpp</a> based on <a href="http://llvm.org/doxygen/BranchProbabilityInfo_8cpp_source.html">lib/Analysis/BranchProbabilityInfo.h</a> and <a href="http://llvm.org/doxygen/BlockFrequencyInfoImpl_8cpp_source.html">lib/Analysis/BlockFrequencyInfoImpl.cpp</a>.
 It is very educational to browse through the code and see what 
heuristics are implemented there. There are also hidden gems implemented
 in <code class="highlighter-rouge">MachineBlockPlacement.cpp</code> 
like, for example, rotating blocks of the loop. It’s not obvious at a 
glance why it is done and there is no explanation in the comments 
either. I learned why we need it <del>the hard way</del> after disabling it and look at the result. I might write a separate article on that topic.</p>

<p>Facebook in the mid 2018 open-sourced their great peace of work called <a href="https://code.fb.com/data-infrastructure/accelerate-large-scale-applications-with-bolt/">BOLT</a> (<a href="https://github.com/facebookincubator/BOLT">github</a>). This tool works on already compiled binary. It uses profile information to reorder basic blocks within the function<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>.
 I think it shouldn’t be too hard to integrate it in the build system 
and enjoy the optimized code layout! The only thing you need to worry 
about is to have representative and meaningful workload for collecting 
profiling information, but that a topic for PGO which we will touch 
later.</p>

<h3 id="basic-block-alignment">Basic block alignment</h3>

<p>I already wrote a complete article on this topic some time ago: <a href="https://dendibakh.github.io/blog/2018/01/18/Code_alignment_issues">Code alignment issues</a>.
 This is purely microarchitectural optimization which is usually applied
 to loops. Figure below is the best brief explanation of the matter:</p>

<p><img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/Defaultalignment.png" alt="" class="center-image-width-60"> 
<img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/NarrowArrow.jpg" alt="" class="center-image"> 
<img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/Betteralignment.png" alt="" class="center-image-width-60"></p>

<p>Idea here is that <strong>shift the hot code (in yellow) down using NOPs (in blue) so that the whole loop will reside in one cache line</strong>. On the picture below cache line start from <code class="highlighter-rouge">c0</code> and ends at <code class="highlighter-rouge">ff</code>. This transformation usually improves I-cache and DSB utilization.</p>

<p>In LLVM it is implemented in the same file as basic block placement algorithms: <a href="http://llvm.org/doxygen/MachineBlockPlacement_8cpp_source.html">lib/CodeGen/MachineBlockPlacement.cpp</a>, look at <code class="highlighter-rouge">MachineBlockPlacement::alignBlocks()</code>.
 This topic was so popular that I wrote also article that describes 
compiler different options in LLVM to manually control alignment of 
basic blocks: <a href="https://dendibakh.github.io/blog/2018/01/25/Code_alignment_options_in_llvm">Code alignment options in llvm</a>.</p>

<p>For experimental purposes it is also possible to emit assembly listing and then insert <code class="highlighter-rouge">NOP</code> instructions or <code class="highlighter-rouge">ALIGN</code><sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup> assembler directives:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>; will place the .loop at the beginning of 256 byte boundary
ALIGN 256
.loop
  dec rdi
  jnz rdi
</code></pre></div></div>

<h3 id="function-splitting">Function splitting</h3>

<p>The idea of function splitting is to separate hot from cold code. 
This usually improves the memory locality of the hot code. Example:</p>
<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">foo</span><span class="p">(</span><span class="kt">bool</span> <span class="n">cond1</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">cond2</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// hot path
</span>  <span class="k">if</span> <span class="p">(</span><span class="n">cond1</span><span class="p">)</span>
    <span class="c1">// cold code 1
</span>  <span class="c1">//hot code
</span>  <span class="k">if</span> <span class="p">(</span><span class="n">cond2</span><span class="p">)</span>
    <span class="c1">// cold code 2
</span><span class="p">}</span>
</code></pre></div></div>
<p>We might want to cut cold part of the function into it’s own new function and put a call to it instead. Something like this:</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">foo</span><span class="p">(</span><span class="kt">bool</span> <span class="n">cond1</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">cond2</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// hot path
</span>  <span class="k">if</span> <span class="p">(</span><span class="n">cond1</span><span class="p">)</span>
    <span class="n">cold1</span><span class="p">();</span> 
  <span class="c1">//hot code
</span>  <span class="k">if</span> <span class="p">(</span><span class="n">cond2</span><span class="p">)</span>
    <span class="n">cold2</span><span class="p">();</span> 
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">cold1</span><span class="p">()</span> <span class="n">__attribute__</span><span class="p">((</span><span class="n">noinline</span><span class="p">))</span> <span class="p">{</span> <span class="c1">// cold code 1 }
</span><span class="kt">void</span> <span class="n">cold2</span><span class="p">()</span> <span class="n">__attribute__</span><span class="p">((</span><span class="n">noinline</span><span class="p">))</span> <span class="p">{</span> <span class="c1">// cold code 2 }
</span></code></pre></div></div>

<p>This is how it looks in the physical layout:</p>

<p><img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/FunctionSplitting.jpg" alt="" class="center-image-width-20-no-block"> <img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/Arrow.jpg" alt="" class="center-image-width-10-no-block"> <img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/FunctionSplitted.jpg" alt="" class="center-image-width-40-no-block"></p>

<p>Because we just left the call instruction inside the hot path it’s likely that <strong>next
 hot instruction will reside in the same cache line. This improves 
utilization of CPU Front-End data structures like I-cache and DSB-cache</strong>.</p>

<p>This transformation contains another important idea which is disable 
inlining of cold functions. You see how we forbid inlining of <code class="highlighter-rouge">cold1</code> and <code class="highlighter-rouge">cold2</code>
 functions  above? The same applies when you see a lot of cold code that
 appeared after inlining some function. You don’t need to split it into 
new function, just disable inlining of this function.</p>

<p>Typically we also would like to put cold code into subsection of .text or even into a separate section.</p>

<p>This optimization is beneficial for relatively big functions with 
complex CFG where there are big pieces of cold code inside hot path, for
 example, switch statement inside the loop.</p>

<p>There is implementation of this functionality in LLVM that resides in <a href="http://llvm.org/doxygen/HotColdSplitting_8cpp_source.html">lib/Transforms/IPO/HotColdSplitting.cpp</a>. Last time<sup id="fnref:5"><a href="#fn:5" class="footnote">5</a></sup>
 I analyzed it wasn’t able to do much with really complicated CFGs, but 
the work is under development, so I hope it will be better soon.</p>

<h3 id="function-grouping">Function grouping</h3>

<p>Usually we want to place hot functions together such that they touch each other in the same cache line.</p>

<p><img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/FunctionGrouping.jpg" alt="" class="center-image-width-30-no-block"> <img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/Arrow.jpg" alt="" class="center-image-width-10-no-block"> <img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/FunctionGrouped.jpg" alt="" class="center-image-width-30-no-block"></p>

<p>In the figure above you can see how we grouped <code class="highlighter-rouge">foo</code>, <code class="highlighter-rouge">bar</code> and <code class="highlighter-rouge">zoo</code> in such a way that now their code fits in only 3 cache lines. Additionally when we call <code class="highlighter-rouge">zoo</code> from <code class="highlighter-rouge">foo</code>, beginning of <code class="highlighter-rouge">zoo</code>
 is already in the I-cache, since we fetched that cache line already. 
Similar to previous optimizations here we try to improve utilization of 
I-cache and DSB-cache.</p>

<p>This is rather the job for the linker, because it has a power of 
placing functions in executable. In gold linker it can be done using <a href="https://manpages.debian.org/unstable/binutils/x86_64-linux-gnu-ld.gold.1.en.html">–section-ordering-file</a> option.</p>

<p><strong>This optimization works best when there are many small hot functions.</strong></p>

<p>There is also very cool tool for doing this automatically: <a href="https://github.com/facebook/hhvm/tree/master/hphp/tools/hfsort">hfsort</a>.
 It uses linux perf for getting profile information, then it does it’s 
ordering magic and gives the text file with optimized function order 
which you can then pass to the linker. Here is the <a href="https://research.fb.com/wp-content/uploads/2017/01/cgo2017-hfsort-final1.pdf">whitepaper</a> if you want to read more about the underlying algorithms.</p>

<h2 id="profile-guided-optimizations-pgo">Profile guided optimizations (PGO)</h2>

<p>It is usually the best option to use PGO if you can come up with a 
typical scenario for your application. Why that’s important? Well, I 
will explain shortly.</p>

<p>Compiling a program and generating assembly is all about heuristics. 
You will be surprised how much uncertainty there is inside every good 
optimizing compiler, like LLVM. For a lot of decisions compiler makes it
 tries to guess the best solution based on some typical cases. For 
example, should I inline this function? But what if it is called a lot 
of times? In this case I probably should do it, but how do I know that 
beforehand?</p>

<p>Here is when profiling information becomes handy. <strong>Given profiling information compiler doesn’t need to question what should be the decision</strong>. And often times you will see in compiler implementation the pattern like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if (profiling information available)
  make decision based on profiling data
else
  make decision based on heuristics
</code></pre></div></div>

<p>Keep in mind that compiler “blindly” uses the profile data you 
provided. What do I mean by that is compiler assumes that all the 
workloads will behave the same, so it optimizes your app just for that 
single workload. <strong>So be careful with choosing the workload to profile, because you can make things even worse.</strong></p>

<p>I must say that it shouldn’t be exactly single workload since you can merge multiple profile data into single file.</p>

<p>I’ve seen real workloads that were improved up to 15% from profile 
guided optimizations. PGO does not only improve code placement, but also
 improve register allocation, because with PGO compiler can put all the 
hot variables into registers, etc. The guide for using PGO in clang is 
described <a href="https://clang.llvm.org/docs/UsersManual.html#profiling-with-instrumentation">here</a>.</p>

<h2 id="summary">Summary</h2>

<table class="table table-bordered table-striped">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th style="text-align: center">How transformed?</th>
      <th style="text-align: center">Why helps?</th>
      <th style="text-align: center">Works best for</th>
      <th style="text-align: center">Done by</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Basic block placement</strong></td>
      <td style="text-align: center">maintain fall through hot code</td>
      <td style="text-align: center">not taken branches are cheaper that taken; better caches utilization</td>
      <td style="text-align: center">any code, especially with a lot of branches</td>
      <td style="text-align: center">compiler</td>
    </tr>
    <tr>
      <td><strong>Basic block alignment</strong></td>
      <td style="text-align: center">shift the hot code using NOPS</td>
      <td style="text-align: center">better caches utilization</td>
      <td style="text-align: center">hot loops</td>
      <td style="text-align: center">compiler</td>
    </tr>
    <tr>
      <td><strong>Function splitting</strong></td>
      <td style="text-align: center">split cold blocks of code and place them in separate functions</td>
      <td style="text-align: center">better caches utilization</td>
      <td style="text-align: center">functions with complex CFG when there are big blocks of cold code between   hot parts</td>
      <td style="text-align: center">compiler</td>
    </tr>
    <tr>
      <td><strong>Function grouping</strong></td>
      <td style="text-align: center">group hot functions together</td>
      <td style="text-align: center">better caches utilization</td>
      <td style="text-align: center">many small hot functions</td>
      <td style="text-align: center">linker</td>
    </tr>
  </tbody>
</table>

<hr>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>This is solely based on my experience and typical gains I saw.&nbsp;<a href="#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>You can read more about builtin-expect here: https://llvm.org/docs/BranchWeightMetadata.html#builtin-expect.&nbsp;<a href="#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>It also able to do function splitting and grouping.&nbsp;<a href="#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p>This example use MASM. Otherwise you will see <code class="highlighter-rouge">.align</code> directive.&nbsp;<a href="#fnref:4" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:5">
      <p>I did it on December 2018.&nbsp;<a href="#fnref:5" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

			</div>
			<!-- Post info -->
			<div class="noteinfo">
			<!-- include some categories and tags maybe? -->
			</div>

			<hr>

			<!-- Post comments -->
			<div class="notecomments">
				<div id="disqus_thread"><iframe id="dsq-app9253" name="dsq-app9253" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 1px !important; min-width: 100% !important; border: medium none !important; overflow: hidden !important; height: 877px !important;" src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/a.html" horizontalscrolling="no" verticalscrolling="no" width="100%" frameborder="0"></iframe><iframe id="indicator-north" name="indicator-north" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 1037px !important; border: medium none !important; overflow: hidden !important; top: 0px !important; min-width: 1037px !important; max-width: 1037px !important; position: fixed !important; z-index: 2147483646 !important; height: 18px !important; min-height: 18px !important; max-height: 18px !important; display: none !important;" frameborder="0"></iframe><iframe id="indicator-south" name="indicator-south" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 1037px !important; border: medium none !important; overflow: hidden !important; bottom: 0px !important; min-width: 1037px !important; max-width: 1037px !important; position: fixed !important; z-index: 2147483646 !important; height: 18px !important; min-height: 18px !important; max-height: 18px !important; display: none !important;" frameborder="0"></iframe></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'dendibakh'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
	 
			</div>
                        <div class="share-page">
    Share this on →
    <a href="https://twitter.com/intent/tweet?text=Machine%20code%20layout%20optimizations.%20&amp;url=https://dendibakh.github.io/blog/2019/03/27/Machine-code-layout-optimizatoins" rel="nofollow" target="_blank" title="Share on Twitter">Twitter</a>
    <a href="https://facebook.com/sharer.php?u=https://dendibakh.github.io//blog/2019/03/27/Machine-code-layout-optimizatoins" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
</div>


		</div><!-- / .note .single -->
</div><!-- /.notes -->

			
			<!-- Begin Mailchimp Signup Form -->
<link href="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/slim-10_7.css" rel="stylesheet" type="text/css">
<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }
	/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
	#mc_embed_signup input.email {font-family:"Open Sans","Helvetica Neue",Arial,Helvetica,Verdana,sans-serif; font-size: 15px; display:block; padding:0 0.4em; margin:0 4% 10px 0; min-height:32px; width:20%; min-width:130px; -webkit-border-radius: 3px; -moz-border-radius: 3px; border-radius: 3px;}
	#mc_embed_signup input.button {display:block; width:20%; margin:0 0 10px 0; min-width:90px;}

</style>
<div id="mc_embed_signup">
<form action="https://github.us20.list-manage.com/subscribe/post?u=a8d7caa7874c7a96f1301bc15&amp;id=346341f01e" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate="">
    <div id="mc_embed_signup_scroll">
	<label for="mce-EMAIL">Subscribe to get more updates fom me:</label>
	<input type="email" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required="">
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_a8d7caa7874c7a96f1301bc15_346341f01e" tabindex="-1"></div>
    <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->


			<div>
<p>If you like this blog, support me on <a href="https://www.patreon.com/dendibakh">Patreon</a>.</p>
</div>


			<div class="footer">
	<div class="row">

	<div class="col-sm-8">
	<p><small>© Denis Bakhvalov 2019</small></p>
	</div>

        <div class="col-sm-1">
             <p class="pull-right"><small>
                <a href="https://dendibakh.github.io/feed.xml"><img src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/feed-icon.png" alt="subscribe"></a></small>
             </p>
       	</div>

	<div class="col-sm-3">
	<p class="pull-right"><small><a href="https://dendibakh.github.io/about" title="About this site">About this site</a></small></p>
	</div>

        </div>
</div> <!-- /footer -->

		
		</div> <!-- /container -->
	
	
		<!-- javascript at the end of the doc so page loads faster -->
<script src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/jquery.js"></script>
<script src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/bootstrap.js"></script>
<script src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/library.js"></script>
<script src="Machine%20code%20layout%20optimizations.%20%20%20Denis%20Bakhvalov%20%20%20C++%20compiler%20dev_files/run_prettify.html"></script>

	
	


<iframe style="display: none;"></iframe></body></html>